{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Prototype: 5-Channel 32x32 Network Payload Classification\n",
    "\n",
    "**Objective:** Evaluate Vision Transformer performance on 5-channel 32x32 image representations of network payloads.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook adapts the ViT prototype for **5-channel 32x32 images** from the downloaded parquet dataset, enabling direct comparison with other encoding strategies. The 5-channel encoding provides rich multi-dimensional representation of payload bytes.\n",
    "\n",
    "## Key Configuration\n",
    "\n",
    "- **Image Format:** 5-channel 32x32 images (5 √ó 32 √ó 32 = 5,120 dimensional)\n",
    "- **Dataset Source:** `/home/ubuntu/analyst/pcap-dataset-samples/parquet/5channel_32x32/`\n",
    "- **Classes:** 5 attack types (Benign_Final, DDoS-HTTP_Flood, DDoS-SYN_Flood, DictionaryBruteForce, DoS-TCP_Flood)\n",
    "- **Architecture:** ViT with 16√ó16 patches, adapted for multi-channel input\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "- **Format:** Apache Parquet files with train/val/test splits\n",
    "- **Channels:** 5 channels representing different aspects of payload data\n",
    "- **Size:** 32√ó32 pixel resolution per channel\n",
    "- **Classes:** Balanced representation across 5 attack categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Environment Setup and Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration for 5-channel ViT - MODIFIED for 6-class training (excluding 3 classes for few-shot)\n",
    "CONFIG = {\n",
    "    'image_size': 32,          # 32x32 images\n",
    "    'channels': 5,             # 5-channel input\n",
    "    'patch_size': 16,          # 16x16 patches\n",
    "    'embed_dim': 192,          # Embedding dimension  \n",
    "    'num_heads': 3,            # Attention heads\n",
    "    'num_layers': 6,           # Transformer layers\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-4,\n",
    "    'epochs': 30,\n",
    "    'num_classes': 6           # REDUCED from 9 to 6 (excluding DDoS-HTTP_Flood, DoS-UDP_Flood, Recon-PortScan)\n",
    "}\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"‚úì Environment setup complete\")\n",
    "print(f\"‚úì Device: {device}\")\n",
    "print(f\"‚úì Target: {CONFIG['channels']}-channel {CONFIG['image_size']}x{CONFIG['image_size']} ‚Üí ViT classification\")\n",
    "print(f\"‚úì Training classes: {CONFIG['num_classes']} (3 classes held out for few-shot testing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 5-channel 32x32 dataset from parquet files\n",
    "# MODIFIED: Excluding DDoS-HTTP_Flood, DoS-UDP_Flood, Recon-PortScan for few-shot testing\n",
    "data_path = '/home/ubuntu/analyst/notebooks/ViT-experiment/pcap-dataset-samples/parquet/5channel_32x32/'\n",
    "print(f\"Loading 5-channel 32x32 dataset from: {data_path}\")\n",
    "\n",
    "# Classes to exclude from training (hold out for few-shot testing)\n",
    "HELD_OUT_CLASSES = ['DDoS-HTTP_Flood', 'DoS-UDP_Flood', 'Recon-PortScan']\n",
    "print(f\"Holding out classes for few-shot testing: {HELD_OUT_CLASSES}\")\n",
    "\n",
    "def load_parquet_data(base_path, exclude_classes=None):\n",
    "    \"\"\"Load all parquet files from train/val/test splits, optionally excluding classes\"\"\"\n",
    "    all_image_data = []\n",
    "    all_labels = []\n",
    "    held_out_data = []\n",
    "    held_out_labels = []\n",
    "    splits = ['train', 'val', 'test']\n",
    "    \n",
    "    if exclude_classes is None:\n",
    "        exclude_classes = []\n",
    "    \n",
    "    # Get all class directories - SORT for consistency\n",
    "    class_dirs = sorted([d for d in glob.glob(f\"{base_path}*/\") if not any(s in d for s in splits)])\n",
    "    class_names = [d.split('/')[-2] for d in class_dirs]\n",
    "    \n",
    "    training_classes = [cls for cls in class_names if cls not in exclude_classes]\n",
    "    print(f\"Training classes: {training_classes}\")\n",
    "    print(f\"Held-out classes: {[cls for cls in class_names if cls in exclude_classes]}\")\n",
    "    \n",
    "    for class_dir in class_dirs:\n",
    "        class_name = class_dir.split('/')[-2]\n",
    "        \n",
    "        # Determine if this class should be held out\n",
    "        is_held_out = class_name in exclude_classes\n",
    "        \n",
    "        print(f\"Loading {class_name}{'(HELD OUT)' if is_held_out else ''}...\")\n",
    "        \n",
    "        for split in splits:\n",
    "            split_path = f\"{class_dir}{split}/\"\n",
    "            parquet_files = sorted(glob.glob(f\"{split_path}*.parquet\"))  # SORT for consistency\n",
    "            \n",
    "            for file_path in parquet_files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(file_path)\n",
    "                    \n",
    "                    # Extract image data from the 'image_data' column\n",
    "                    if 'image_data' in df.columns:\n",
    "                        # Convert list-based image data to numpy arrays\n",
    "                        for idx, row in df.iterrows():\n",
    "                            image_data = np.array(row['image_data'], dtype=np.float32)\n",
    "                            \n",
    "                            # Add to appropriate dataset\n",
    "                            if is_held_out:\n",
    "                                held_out_data.append(image_data)\n",
    "                                held_out_labels.append(class_name)\n",
    "                            else:\n",
    "                                all_image_data.append(image_data)\n",
    "                                all_labels.append(class_name)\n",
    "                        \n",
    "                        print(f\"   Loaded {len(df)} samples from {file_path.split('/')[-1]} {'-> HELD OUT' if is_held_out else ''}\")\n",
    "                    else:\n",
    "                        print(f\"   Warning: No 'image_data' column found in {file_path.split('/')[-1]}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   Error loading {file_path}: {e}\")\n",
    "    \n",
    "    if not all_image_data:\n",
    "        raise ValueError(\"No training image data was loaded successfully!\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train = np.array(all_image_data, dtype=np.float32)\n",
    "    y_train = np.array(all_labels)\n",
    "    \n",
    "    X_held_out = np.array(held_out_data, dtype=np.float32) if held_out_data else None\n",
    "    y_held_out = np.array(held_out_labels) if held_out_labels else None\n",
    "    \n",
    "    print(f\"\\n‚úì Training samples loaded: {len(X_train)}\")\n",
    "    print(f\"‚úì Training image data shape: {X_train.shape}\")\n",
    "    print(f\"‚úì Training unique labels: {np.unique(y_train)}\")\n",
    "    \n",
    "    if X_held_out is not None:\n",
    "        print(f\"‚úì Held-out samples loaded: {len(X_held_out)}\")\n",
    "        print(f\"‚úì Held-out image data shape: {X_held_out.shape}\")\n",
    "        print(f\"‚úì Held-out unique labels: {np.unique(y_held_out)}\")\n",
    "    \n",
    "    return X_train, y_train, X_held_out, y_held_out\n",
    "\n",
    "# Load the dataset with held-out classes\n",
    "X, y, X_held_out, y_held_out = load_parquet_data(data_path, exclude_classes=HELD_OUT_CLASSES)\n",
    "\n",
    "print(f\"\\n‚úì Data preparation:\")\n",
    "print(f\"   Training X shape: {X.shape}\")\n",
    "print(f\"   Training y shape: {y.shape}\")\n",
    "print(f\"   Expected features: {CONFIG['channels']} √ó {CONFIG['image_size']}¬≤ = {CONFIG['channels'] * CONFIG['image_size']**2}\")\n",
    "\n",
    "if X_held_out is not None:\n",
    "    print(f\"   Held-out X shape: {X_held_out.shape}\")\n",
    "    print(f\"   Held-out y shape: {y_held_out.shape}\")\n",
    "\n",
    "# Verify the data matches expected dimensions\n",
    "expected_features = CONFIG['channels'] * CONFIG['image_size'] ** 2\n",
    "if X.shape[1] == expected_features:\n",
    "    print(f\"‚úì Feature count matches expected: {X.shape[1]} == {expected_features}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Feature count mismatch: got {X.shape[1]}, expected {expected_features}\")\n",
    "\n",
    "# Encode labels for training classes only\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "actual_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"\\n‚úì Training label encoding:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(y == label)\n",
    "    print(f\"   {i}: {label} ({count:,} samples)\")\n",
    "\n",
    "# Store held-out class information for later few-shot testing\n",
    "if y_held_out is not None:\n",
    "    print(f\"\\n‚úì Held-out classes for few-shot testing:\")\n",
    "    for label in np.unique(y_held_out):\n",
    "        count = np.sum(y_held_out == label)\n",
    "        print(f\"   {label} ({count:,} samples)\")\n",
    "\n",
    "# Update CONFIG to reflect actual number of training classes\n",
    "if actual_classes != CONFIG['num_classes']:\n",
    "    print(f\"\\n‚ö†Ô∏è  Adjusting from {CONFIG['num_classes']} to {actual_classes} classes based on training data\")\n",
    "    CONFIG['num_classes'] = actual_classes\n",
    "\n",
    "# Verify data range and quality\n",
    "print(f\"\\n‚úì Training data quality:\")\n",
    "print(f\"   Value range: [{X.min():.3f}, {X.max():.3f}]\")\n",
    "print(f\"   Missing values: {np.isnan(X).sum()}\")\n",
    "print(f\"   Non-zero features per sample (avg): {np.mean(np.count_nonzero(X, axis=1)):.1f}\")\n",
    "\n",
    "# Save held-out data for later few-shot evaluation\n",
    "if X_held_out is not None:\n",
    "    np.save('held_out_X.npy', X_held_out)\n",
    "    np.save('held_out_y.npy', y_held_out)\n",
    "    print(f\"\\n‚úì Held-out data saved for few-shot testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Reshaping and Visualization\n",
    "# Reshape data from flat features to 5-channel images\n",
    "print(\"Reshaping training data to 5-channel images...\")\n",
    "\n",
    "# Expected: (N, 5120) ‚Üí (N, 5, 32, 32)\n",
    "expected_features = CONFIG['channels'] * CONFIG['image_size'] ** 2\n",
    "if X.shape[1] == expected_features:\n",
    "    # Reshape to (N, channels, height, width)\n",
    "    X_images = X.reshape(-1, CONFIG['channels'], CONFIG['image_size'], CONFIG['image_size'])\n",
    "    print(f\"‚úì Reshaped to: {X_images.shape}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Feature count mismatch: got {X.shape[1]}, expected {expected_features}\")\n",
    "    # Try to adapt - take first N features if more, or pad if fewer\n",
    "    if X.shape[1] > expected_features:\n",
    "        X_trimmed = X[:, :expected_features]\n",
    "        X_images = X_trimmed.reshape(-1, CONFIG['channels'], CONFIG['image_size'], CONFIG['image_size'])\n",
    "        print(f\"‚úì Trimmed and reshaped to: {X_images.shape}\")\n",
    "    else:\n",
    "        # Pad with zeros\n",
    "        padding = np.zeros((X.shape[0], expected_features - X.shape[1]))\n",
    "        X_padded = np.concatenate([X, padding], axis=1)\n",
    "        X_images = X_padded.reshape(-1, CONFIG['channels'], CONFIG['image_size'], CONFIG['image_size'])\n",
    "        print(f\"‚úì Padded and reshaped to: {X_images.shape}\")\n",
    "\n",
    "# Normalize images to [0, 1] if needed\n",
    "if X_images.max() > 1.0:\n",
    "    X_images = X_images / 255.0\n",
    "    print(f\"‚úì Normalized to [0, 1]: [{X_images.min():.3f}, {X_images.max():.3f}]\")\n",
    "\n",
    "# Visualize sample images for each TRAINING class only\n",
    "num_classes = len(label_encoder.classes_)\n",
    "fig, axes = plt.subplots(num_classes, CONFIG['channels'], figsize=(20, 4*num_classes))\n",
    "if num_classes == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "print(f\"\\nVisualizing sample images for {num_classes} TRAINING classes...\")\n",
    "print(f\"(Held-out classes: {HELD_OUT_CLASSES} will be used for few-shot testing)\")\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    class_name = label_encoder.classes_[class_idx]\n",
    "    \n",
    "    # Find first sample of this class\n",
    "    class_mask = y_encoded == class_idx\n",
    "    sample_indices = np.where(class_mask)[0]\n",
    "    \n",
    "    if len(sample_indices) > 0:\n",
    "        sample_idx = sample_indices[0]\n",
    "        sample_image = X_images[sample_idx]\n",
    "        \n",
    "        # Show each channel\n",
    "        for channel in range(CONFIG['channels']):\n",
    "            ax = axes[class_idx, channel] if num_classes > 1 else axes[channel]\n",
    "            im = ax.imshow(sample_image[channel], cmap='gray', vmin=0, vmax=1)\n",
    "            if channel == 0:\n",
    "                ax.set_ylabel(f'{class_name}', fontsize=12)\n",
    "            ax.set_title(f'Channel {channel+1}', fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "plt.suptitle('5-Channel 32√ó32 Network Payload Images by Training Class\\n(SqlInjection, Recon-PortScan, DictionaryBruteForce held out)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze channel statistics for training data\n",
    "print(f\"\\nüìä Channel-wise Analysis (Training Data Only):\")\n",
    "for channel in range(CONFIG['channels']):\n",
    "    channel_data = X_images[:, channel, :, :]\n",
    "    mean_intensity = np.mean(channel_data)\n",
    "    std_intensity = np.std(channel_data)\n",
    "    nonzero_ratio = np.mean(channel_data > 0)\n",
    "    print(f\"   Channel {channel+1}: mean={mean_intensity:.3f}, std={std_intensity:.3f}, nonzero={nonzero_ratio:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úì Data visualization and analysis complete!\")\n",
    "print(f\"‚úì Ready for ViT model training with {CONFIG['channels']}-channel input on {CONFIG['num_classes']} classes\")\n",
    "print(f\"‚úì Few-shot testing data prepared for: {HELD_OUT_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Multi-Channel Vision Transformer Model:\n",
      "‚úì Input: 5-channel 32√ó32 images\n",
      "‚úì Patches per image: 4\n",
      "‚úì Total parameters: 2,918,409\n",
      "‚úì Trainable parameters: 2,918,409\n",
      "‚úì Model size: 11.1 MB\n",
      "‚úì Forward pass test: torch.Size([2, 5, 32, 32]) ‚Üí torch.Size([2, 9])\n",
      "\n",
      "‚úì Multi-channel ViT architecture ready!\n",
      "‚úì Configured for 9 classes: ['Benign_Final' 'DDoS-HTTP_Flood' 'DDoS-SYN_Flood' 'DictionaryBruteForce'\n",
      " 'DoS-TCP_Flood' 'DoS-UDP_Flood' 'Mirai-udpplain' 'Recon-PortScan'\n",
      " 'SqlInjection']\n"
     ]
    }
   ],
   "source": [
    "# Section 3: Multi-Channel Vision Transformer Architecture\n",
    "class MultiChannelPatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert multi-channel images to patch embeddings\"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Convolutional layer to extract patches from multi-channel input\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_channels, img_size, img_size)\n",
    "        x = self.projection(x)  # (batch_size, embed_dim, H', W')\n",
    "        x = x.flatten(2)        # (batch_size, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)   # (batch_size, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class MultiChannelVisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer for Multi-Channel Network Payload Classification\"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim, num_heads, num_layers, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-channel patch embedding\n",
    "        self.patch_embedding = MultiChannelPatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embedding.num_patches\n",
    "        \n",
    "        # Learnable position embeddings\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Class token (for classification)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Convert to patches and embed\n",
    "        x = self.patch_embedding(x)  # (batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (batch_size, num_patches + 1, embed_dim)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Classification from class token\n",
    "        cls_output = x[:, 0]  # Take the class token\n",
    "        cls_output = self.norm(cls_output)\n",
    "        output = self.head(cls_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize the Multi-Channel Vision Transformer\n",
    "model = MultiChannelVisionTransformer(\n",
    "    img_size=CONFIG['image_size'],\n",
    "    patch_size=CONFIG['patch_size'],\n",
    "    in_channels=CONFIG['channels'],\n",
    "    embed_dim=CONFIG['embed_dim'],\n",
    "    num_heads=CONFIG['num_heads'],\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    num_classes=CONFIG['num_classes']\n",
    ").to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"ü§ñ Multi-Channel Vision Transformer Model:\")\n",
    "print(f\"‚úì Input: {CONFIG['channels']}-channel {CONFIG['image_size']}√ó{CONFIG['image_size']} images\")\n",
    "print(f\"‚úì Patches per image: {(CONFIG['image_size']//CONFIG['patch_size'])**2}\")\n",
    "print(f\"‚úì Total parameters: {total_params:,}\")\n",
    "print(f\"‚úì Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"‚úì Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Test forward pass\n",
    "sample_input = torch.randn(2, CONFIG['channels'], CONFIG['image_size'], CONFIG['image_size']).to(device)\n",
    "with torch.no_grad():\n",
    "    sample_output = model(sample_input)\n",
    "    print(f\"‚úì Forward pass test: {sample_input.shape} ‚Üí {sample_output.shape}\")\n",
    "\n",
    "print(f\"\\n‚úì Multi-channel ViT architecture ready!\")\n",
    "print(f\"‚úì Configured for {CONFIG['num_classes']} classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Training Pipeline\n",
    "print(\"Preparing training pipeline for 6-class model...\")\n",
    "\n",
    "# Convert to PyTorch tensors (training data only)\n",
    "X_tensor = torch.FloatTensor(X_images)\n",
    "y_tensor = torch.LongTensor(y_encoded)\n",
    "\n",
    "print(f\"‚úì Tensor conversion: X={X_tensor.shape}, y={y_tensor.shape}\")\n",
    "print(f\"‚úì Training on {CONFIG['num_classes']} classes: {label_encoder.classes_}\")\n",
    "print(f\"‚úì Held-out classes for few-shot testing: {HELD_OUT_CLASSES}\")\n",
    "\n",
    "# Stratified train/validation/test split for training classes only\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.3, stratify=y_tensor, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data splits (6 training classes only):\")\n",
    "print(f\"   Train: {X_train.shape[0]} samples\")\n",
    "print(f\"   Val:   {X_val.shape[0]} samples\")\n",
    "print(f\"   Test:  {X_test.shape[0]} samples\")\n",
    "\n",
    "# Calculate class weights for balanced training\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train.numpy()\n",
    ")\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "print(f\"‚úì Class weights for 6 classes: {class_weights}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"‚úì Data loaders: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test batches\")\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
    "\n",
    "print(f\"‚úì Training setup complete for 6-class model\")\n",
    "\n",
    "# Training functions\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nüöÄ Starting 6-Class ViT training for {CONFIG['epochs']} epochs...\")\n",
    "print(f\"üìù Training classes: {list(label_encoder.classes_)}\")\n",
    "print(f\"üîÑ Held-out classes: {HELD_OUT_CLASSES}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1:2d}/{CONFIG['epochs']}: \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_6class_vit_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úì 6-Class training completed!\")\n",
    "print(f\"‚úì Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "ax1.set_title('6-Class ViT: Training and Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(train_accuracies, label='Train Acc', color='blue')\n",
    "ax2.plot(val_accuracies, label='Val Acc', color='red')\n",
    "ax2.set_title('6-Class ViT: Training and Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì 6-Class ViT training complete!\")\n",
    "print(f\"‚úì Model ready for evaluation on 6 training classes\")\n",
    "print(f\"‚úì Held-out data ready for few-shot testing: {HELD_OUT_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Evaluation & Few-Shot Testing\n",
    "print(\"üîç 6-Class ViT Evaluation and Few-Shot Testing\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_6class_vit_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Comprehensive evaluation function\n",
    "def evaluate_model(model, dataloader, device, class_names=None):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = test_loss / len(dataloader)\n",
    "    \n",
    "    return accuracy, avg_loss, np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "# 1. Evaluate on 6 training classes\n",
    "print(\"üìä Part 1: Evaluation on 6 Training Classes\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_accuracy, test_loss, predictions, targets = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(f\"üéØ 6-Class ViT Test Results:\")\n",
    "print(f\"‚úì Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"‚úì Test Loss: {test_loss:.4f}\")\n",
    "print(f\"‚úì Samples evaluated: {len(targets):,}\")\n",
    "\n",
    "# Detailed classification report for training classes\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(targets, predictions, target_names=class_names, output_dict=True, zero_division=0)\n",
    "\n",
    "print(f\"\\nüìä Per-Class Performance (Training Classes):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    if str(i) in report:\n",
    "        precision = report[str(i)]['precision']\n",
    "        recall = report[str(i)]['recall']\n",
    "        f1 = report[str(i)]['f1-score']\n",
    "        support = report[str(i)]['support']\n",
    "        print(f\"{class_name:15s}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, Support={support:,}\")\n",
    "\n",
    "print(f\"\\nüìà Overall Metrics (Training Classes):\")\n",
    "print(f\"‚úì Macro Avg: Precision={report['macro avg']['precision']:.3f}, Recall={report['macro avg']['recall']:.3f}, F1={report['macro avg']['f1-score']:.3f}\")\n",
    "print(f\"‚úì Weighted Avg: Precision={report['weighted avg']['precision']:.3f}, Recall={report['weighted avg']['recall']:.3f}, F1={report['weighted avg']['f1-score']:.3f}\")\n",
    "\n",
    "# Confusion Matrix for training classes\n",
    "cm = confusion_matrix(targets, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('6-Class ViT: Confusion Matrix (Training Classes)')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Few-Shot Testing on Held-Out Classes\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä Part 2: Few-Shot Testing on Held-Out Classes\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if X_held_out is not None and len(X_held_out) > 0:\n",
    "    # Prepare held-out data\n",
    "    print(f\"üî¨ Few-shot testing on: {HELD_OUT_CLASSES}\")\n",
    "    \n",
    "    # Reshape held-out data\n",
    "    X_held_out_images = X_held_out.reshape(-1, CONFIG['channels'], CONFIG['image_size'], CONFIG['image_size'])\n",
    "    \n",
    "    # Normalize if needed\n",
    "    if X_held_out_images.max() > 1.0:\n",
    "        X_held_out_images = X_held_out_images / 255.0\n",
    "    \n",
    "    # For few-shot testing, we'll use feature extraction from the pre-trained model\n",
    "    # and evaluate how well the learned features can distinguish held-out classes\n",
    "    \n",
    "    def extract_features(model, data_tensor, device, batch_size=32):\n",
    "        \"\"\"Extract features from the model's penultimate layer\"\"\"\n",
    "        model.eval()\n",
    "        features = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(data_tensor), batch_size):\n",
    "                batch = data_tensor[i:i+batch_size].to(device)\n",
    "                \n",
    "                # Forward pass through the model (excluding final classification layer)\n",
    "                x = model.patch_embedding(batch)\n",
    "                \n",
    "                # Add class token\n",
    "                cls_tokens = model.cls_token.expand(batch.size(0), -1, -1)\n",
    "                x = torch.cat((cls_tokens, x), dim=1)\n",
    "                \n",
    "                # Add positional embeddings\n",
    "                x = x + model.pos_embedding\n",
    "                x = model.dropout(x)\n",
    "                \n",
    "                # Pass through transformer\n",
    "                x = model.transformer(x)\n",
    "                \n",
    "                # Extract class token features (before final classification)\n",
    "                cls_output = x[:, 0]\n",
    "                cls_output = model.norm(cls_output)\n",
    "                \n",
    "                features.append(cls_output.cpu())\n",
    "        \n",
    "        return torch.cat(features, dim=0)\n",
    "    \n",
    "    # Extract features for held-out classes\n",
    "    print(\"üîß Extracting features from held-out data...\")\n",
    "    held_out_tensor = torch.FloatTensor(X_held_out_images)\n",
    "    held_out_features = extract_features(model, held_out_tensor, device)\n",
    "    \n",
    "    print(f\"‚úì Extracted features shape: {held_out_features.shape}\")\n",
    "    \n",
    "    # Analyze feature separability for held-out classes\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "    \n",
    "    # PCA visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    features_2d = pca.fit_transform(held_out_features.numpy())\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot by actual class\n",
    "    unique_held_out_classes = np.unique(y_held_out)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_held_out_classes)))\n",
    "    \n",
    "    for i, class_name in enumerate(unique_held_out_classes):\n",
    "        mask = y_held_out == class_name\n",
    "        plt.scatter(features_2d[mask, 0], features_2d[mask, 1], \n",
    "                   c=[colors[i]], label=class_name, alpha=0.7, s=20)\n",
    "    \n",
    "    plt.title('PCA Visualization of Held-Out Classes (Feature Space)')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Clustering analysis\n",
    "    print(f\"\\nüîç Few-Shot Clustering Analysis:\")\n",
    "    n_clusters = len(unique_held_out_classes)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(held_out_features.numpy())\n",
    "    \n",
    "    # Create label encoder for held-out classes\n",
    "    held_out_label_encoder = LabelEncoder()\n",
    "    y_held_out_encoded = held_out_label_encoder.fit_transform(y_held_out)\n",
    "    \n",
    "    # Calculate clustering metrics\n",
    "    ari_score = adjusted_rand_score(y_held_out_encoded, cluster_labels)\n",
    "    sil_score = silhouette_score(held_out_features.numpy(), cluster_labels)\n",
    "    \n",
    "    print(f\"‚úì Adjusted Rand Index: {ari_score:.3f}\")\n",
    "    print(f\"‚úì Silhouette Score: {sil_score:.3f}\")\n",
    "    \n",
    "    # Per-class analysis for held-out classes\n",
    "    print(f\"\\nüìä Held-Out Class Statistics:\")\n",
    "    for class_name in unique_held_out_classes:\n",
    "        count = np.sum(y_held_out == class_name)\n",
    "        print(f\"   {class_name}: {count:,} samples\")\n",
    "    \n",
    "    print(f\"\\nüí° Few-Shot Testing Results:\")\n",
    "    print(f\"‚úì Held-out classes: {len(unique_held_out_classes)}\")\n",
    "    print(f\"‚úì Total held-out samples: {len(y_held_out):,}\")\n",
    "    print(f\"‚úì Feature separability (ARI): {ari_score:.3f}\")\n",
    "    print(f\"‚úì Cluster quality (Silhouette): {sil_score:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No held-out data available for few-shot testing\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"üí° Final Results Summary:\")\n",
    "print(f\"‚úì 6-Class model accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"‚úì Training classes: {list(label_encoder.classes_)}\")\n",
    "print(f\"‚úì Held-out classes: {HELD_OUT_CLASSES}\")\n",
    "print(f\"‚úì Model complexity: {total_params:,} parameters\")\n",
    "print(f\"‚úì Input format: {CONFIG['channels']}-channel {CONFIG['image_size']}√ó{CONFIG['image_size']} images\")\n",
    "\n",
    "if X_held_out is not None:\n",
    "    print(f\"‚úì Few-shot separability: {ari_score:.3f} ARI, {sil_score:.3f} Silhouette\")\n",
    "\n",
    "# Save results\n",
    "results_6class = {\n",
    "    'model_type': '6class_32x32_few_shot',\n",
    "    'training_classes': list(label_encoder.classes_),\n",
    "    'held_out_classes': HELD_OUT_CLASSES,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_loss': test_loss,\n",
    "    'num_training_classes': CONFIG['num_classes'],\n",
    "    'num_parameters': total_params,\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'classification_report': report\n",
    "}\n",
    "\n",
    "if X_held_out is not None:\n",
    "    results_6class['few_shot_metrics'] = {\n",
    "        'adjusted_rand_index': ari_score,\n",
    "        'silhouette_score': sil_score,\n",
    "        'held_out_samples': len(y_held_out)\n",
    "    }\n",
    "\n",
    "import json\n",
    "with open('results_6class_few_shot.json', 'w') as f:\n",
    "    json.dump(results_6class, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n‚úì Results saved to results_6class_few_shot.json\")\n",
    "print(f\"‚úì 6-Class ViT evaluation and few-shot testing complete!\")\n",
    "print(f\"‚úì Model ready for few-shot learning experiments on held-out classes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
