{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LSTM Architecture: CIC-IoT23 3-Class Full Capacity Experiment\n",
        "\n",
        "## Sequential Learning Architecture Comparison Research\n",
        "**Objective**: Compare LSTM vs CNN vs ViT performance on IoT cybersecurity classification\n",
        "\n",
        "**Dataset**: CIC-IoT23 with semantic 3-class grouping\n",
        "- **Normal**: Normal IoT traffic\n",
        "- **Reconnaissance**: Recon-PortScan, DictionaryBruteForce\n",
        "- **Active_Attack**: DDoS-HTTP_Flood, DDoS-SYN_Flood, DoS-TCP_Flood, DoS-UDP_Flood, Mirai-udpplain, SqlInjection\n",
        "\n",
        "**Capacity**: 12,000 samples per class (36,000 total)  \n",
        "**Input**: 5-channel 32x32 → Sequential (32 timesteps × 160 features)  \n",
        "**Architecture**: Long Short-Term Memory (LSTM)  \n",
        "**Baselines**: CNN (training), ViT (96.94% accuracy)\n",
        "\n",
        "**Research Questions**:\n",
        "1. Can LSTM capture temporal patterns better than spatial methods (CNN/ViT)?\n",
        "2. How does sequential modeling compare to local (CNN) vs global (ViT) approaches?\n",
        "3. Will LSTM show different cross-domain transfer characteristics to UNSW-NB15?\n",
        "4. Which paradigm (spatial, sequential, attention) is optimal for IoT cybersecurity?\n",
        "\n",
        "**LSTM Advantages for Cybersecurity**:\n",
        "- Network traffic is inherently sequential\n",
        "- Attack patterns unfold over time\n",
        "- Memory of previous states (stateful detection)\n",
        "- Natural fit for protocol analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 LSTM SEQUENTIAL ARCHITECTURE EXPERIMENT INITIALIZED\n",
            "📋 Notebook: LSTM_Prototype_CIC_3class_full_capacity.ipynb\n",
            "🔧 Version: Sequential modeling for IoT cybersecurity\n",
            "📊 Device: cpu\n",
            "📊 Dataset: CIC-IoT23 3-class semantic grouping\n",
            "📊 Capacity: 12,000 samples per class\n",
            "📊 Total samples: 36,000\n",
            "📊 Architecture: LSTM (Sequential Modeling)\n",
            "📊 Sequence format: 32 timesteps × 160 features\n",
            "🎯 Baselines: ViT (96.94%), CNN (training ~93%+)\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup and Configuration\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration  \n",
        "CONFIG = {\n",
        "    'data_path': '/home/ubuntu/Cyber_AI/ai-cyber/notebooks/ViT-experiment/pcap-dataset-samples/parquet/5channel_32x32/',\n",
        "    'max_samples_per_class': 12000,  # Full capacity\n",
        "    'test_size': 0.2,\n",
        "    'val_size': 0.2,\n",
        "    'random_state': 42,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,  # Higher LR for LSTM\n",
        "    'epochs': 50,\n",
        "    'patience': 7,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'num_workers': 4,\n",
        "    # LSTM-specific config\n",
        "    'sequence_length': 32,     # Treat as 32 time steps\n",
        "    'input_features': 160,     # 5 channels * 32 features per timestep\n",
        "    'hidden_size': 128,        # LSTM hidden dimension\n",
        "    'num_layers': 2,           # Stacked LSTM layers\n",
        "    'dropout': 0.3             # Dropout for regularization\n",
        "}\n",
        "\n",
        "# CIC 3-class mapping (same as CNN/ViT for fair comparison)\n",
        "CLASS_MAPPING = {\n",
        "    'Normal': ['Benign_Final'],\n",
        "    'Reconnaissance': ['Recon-PortScan', 'DictionaryBruteForce'],\n",
        "    'Active_Attack': ['DDoS-HTTP_Flood', 'DDoS-SYN_Flood', 'DoS-TCP_Flood', \n",
        "                     'DoS-UDP_Flood', 'Mirai-udpplain', 'SqlInjection']\n",
        "}\n",
        "\n",
        "print(\"🔄 LSTM SEQUENTIAL ARCHITECTURE EXPERIMENT INITIALIZED\")\n",
        "print(\"📋 Notebook: LSTM_Prototype_CIC_3class_full_capacity.ipynb\")\n",
        "print(\"🔧 Version: Sequential modeling for IoT cybersecurity\")\n",
        "print(f\"📊 Device: {CONFIG['device']}\")\n",
        "print(f\"📊 Dataset: CIC-IoT23 3-class semantic grouping\")\n",
        "print(f\"📊 Capacity: {CONFIG['max_samples_per_class']:,} samples per class\")\n",
        "print(f\"📊 Total samples: {CONFIG['max_samples_per_class'] * 3:,}\")\n",
        "print(f\"📊 Architecture: LSTM (Sequential Modeling)\")\n",
        "print(f\"📊 Sequence format: {CONFIG['sequence_length']} timesteps × {CONFIG['input_features']} features\")\n",
        "print(f\"🎯 Baselines: ViT (96.94%), CNN (training ~93%+)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔄 LSTM ARCHITECTURE SUMMARY:\n",
            "📊 Total parameters: 355,331\n",
            "📊 Trainable parameters: 355,331\n",
            "📊 Model size: ~1.4 MB\n",
            "\n",
            "🔍 Architecture Details:\n",
            "   • 2-layer LSTM with attention\n",
            "   • Hidden size: 128 per layer\n",
            "   • Multi-head attention mechanism (8 heads)\n",
            "   • Layer normalization and dropout (0.3)\n",
            "   • Sequential input: 32 × 160\n",
            "\n",
            "🆚 COMPARISON TARGETS:\n",
            "   📊 ViT: 96.94% accuracy (~150K parameters)\n",
            "   📊 CNN: ~93%+ accuracy (4.8M parameters, training)\n",
            "   🔄 LSTM: TBD accuracy (355,331 parameters)\n",
            "\n",
            "💡 LSTM Advantages:\n",
            "   • Captures temporal attack progression\n",
            "   • Memory of previous network states\n",
            "   • Natural for sequential protocol analysis\n",
            "   • Attention focuses on critical time points\n"
          ]
        }
      ],
      "source": [
        "# Multi-Layer LSTM Architecture for IoT Cybersecurity Sequential Analysis\n",
        "class MultiLayerLSTM(nn.Module):\n",
        "    def __init__(self, input_size=160, hidden_size=128, num_layers=2, num_classes=3, dropout=0.3):\n",
        "        super(MultiLayerLSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,  # (batch, seq, feature)\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        \n",
        "        # Attention mechanism for focusing on important timesteps\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=8,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, num_classes)\n",
        "        )\n",
        "        \n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, input_features)\n",
        "        # Expected: (batch_size, 32, 160)\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Initialize hidden states\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
        "        # lstm_out shape: (batch_size, sequence_length, hidden_size)\n",
        "        \n",
        "        # Apply attention to focus on important timesteps\n",
        "        attended_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        # attended_out shape: (batch_size, sequence_length, hidden_size)\n",
        "        \n",
        "        # Global average pooling over sequence dimension\n",
        "        pooled = torch.mean(attended_out, dim=1)  # (batch_size, hidden_size)\n",
        "        \n",
        "        # Classification\n",
        "        output = self.classifier(pooled)  # (batch_size, num_classes)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                # Input-to-hidden weights\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                # Hidden-to-hidden weights\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            elif 'bias' in name:\n",
        "                param.data.fill_(0.)\n",
        "                # Set forget gate bias to 1 (LSTM best practice)\n",
        "                if 'bias_ih' in name:\n",
        "                    n = param.size(0)\n",
        "                    param.data[n//4:n//2].fill_(1.)\n",
        "\n",
        "# Initialize LSTM model\n",
        "model = MultiLayerLSTM(\n",
        "    input_size=CONFIG['input_features'],\n",
        "    hidden_size=CONFIG['hidden_size'],\n",
        "    num_layers=CONFIG['num_layers'],\n",
        "    num_classes=3,\n",
        "    dropout=CONFIG['dropout']\n",
        ")\n",
        "model = model.to(CONFIG['device'])\n",
        "\n",
        "# Count parameters for comparison\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"\\n🔄 LSTM ARCHITECTURE SUMMARY:\")\n",
        "print(f\"📊 Total parameters: {total_params:,}\")\n",
        "print(f\"📊 Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"📊 Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
        "print(\"\\n🔍 Architecture Details:\")\n",
        "print(f\"   • {CONFIG['num_layers']}-layer LSTM with attention\")\n",
        "print(f\"   • Hidden size: {CONFIG['hidden_size']} per layer\")\n",
        "print(f\"   • Multi-head attention mechanism (8 heads)\")\n",
        "print(f\"   • Layer normalization and dropout ({CONFIG['dropout']})\")\n",
        "print(f\"   • Sequential input: {CONFIG['sequence_length']} × {CONFIG['input_features']}\")\n",
        "\n",
        "print(f\"\\n🆚 COMPARISON TARGETS:\")\n",
        "print(f\"   📊 ViT: 96.94% accuracy (~150K parameters)\")\n",
        "print(f\"   📊 CNN: ~93%+ accuracy (4.8M parameters, training)\")\n",
        "print(f\"   🔄 LSTM: TBD accuracy ({trainable_params:,} parameters)\")\n",
        "print(f\"\\n💡 LSTM Advantages:\")\n",
        "print(f\"   • Captures temporal attack progression\")\n",
        "print(f\"   • Memory of previous network states\")\n",
        "print(f\"   • Natural for sequential protocol analysis\")\n",
        "print(f\"   • Attention focuses on critical time points\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Loading CIC-IoT23 3-class FULL CAPACITY dataset from: /home/ubuntu/Cyber_AI/ai-cyber/notebooks/ViT-experiment/pcap-dataset-samples/parquet/5channel_32x32/\n",
            "🎯 Target: 12,000 samples per class = 36,000 total\n",
            "3-Class mapping (FULL CAPACITY): {'Normal': ['Benign_Final'], 'Reconnaissance': ['Recon-PortScan', 'DictionaryBruteForce'], 'Active_Attack': ['DDoS-HTTP_Flood', 'DDoS-SYN_Flood', 'DoS-TCP_Flood', 'DoS-UDP_Flood', 'Mirai-udpplain', 'SqlInjection']}\n",
            "\n",
            "🔄 Loading Normal from: ['Benign_Final']\n",
            "   Target: 12,000 samples\n",
            "  📂 Processing Benign_Final...\n",
            "    ✓ Loaded 1,000 from shard_00000.parquet (total Normal: 1,000)\n",
            "    ✓ Loaded 1,000 from shard_00001.parquet (total Normal: 2,000)\n",
            "    ✓ Loaded 1,000 from shard_00002.parquet (total Normal: 3,000)\n",
            "    ✓ Loaded 1,000 from shard_00003.parquet (total Normal: 4,000)\n",
            "    ✓ Loaded 1,000 from shard_00004.parquet (total Normal: 5,000)\n",
            "    ✓ Loaded 1,000 from shard_00005.parquet (total Normal: 6,000)\n",
            "    ✓ Loaded 1,000 from shard_00006.parquet (total Normal: 7,000)\n",
            "    ✓ Loaded 1,000 from shard_00007.parquet (total Normal: 8,000)\n",
            "    ✓ Loaded 358 from shard_00008.parquet (total Normal: 8,358)\n",
            "    ✓ Loaded 1,000 from shard_00000.parquet (total Normal: 9,358)\n",
            "    ✓ Loaded 810 from shard_00001.parquet (total Normal: 10,168)\n",
            "    ✓ Loaded 1,000 from shard_00000.parquet (total Normal: 11,168)\n",
            "    ✓ Loaded 832 from shard_00001.parquet (total Normal: 12,000)\n",
            "\n",
            "🔄 Loading Reconnaissance from: ['Recon-PortScan', 'DictionaryBruteForce']\n",
            "   Target: 12,000 samples\n",
            "  📂 Processing Recon-PortScan...\n",
            "    ✓ Loaded 1,000 from shard_00000.parquet (total Reconnaissance: 1,000)\n",
            "    ✓ Loaded 1,000 from shard_00001.parquet (total Reconnaissance: 2,000)\n",
            "    ✓ Loaded 1,000 from shard_00002.parquet (total Reconnaissance: 3,000)\n",
            "    ✓ Loaded 1,000 from shard_00003.parquet (total Reconnaissance: 4,000)\n",
            "    ✓ Loaded 1,000 from shard_00004.parquet (total Reconnaissance: 5,000)\n",
            "    ✓ Loaded 1,000 from shard_00005.parquet (total Reconnaissance: 6,000)\n",
            "    ✓ Loaded 1,000 from shard_00006.parquet (total Reconnaissance: 7,000)\n",
            "    ✓ Loaded 1,000 from shard_00007.parquet (total Reconnaissance: 8,000)\n",
            "    ✓ Loaded 301 from shard_00008.parquet (total Reconnaissance: 8,301)\n",
            "    ✓ Loaded 1,000 from shard_00000.parquet (total Reconnaissance: 9,301)\n",
            "    ✓ Loaded 790 from shard_00001.parquet (total Reconnaissance: 10,091)\n",
            "    ✓ Loaded 1,000 from shard_00000.parquet (total Reconnaissance: 11,091)\n",
            "    ✓ Loaded 909 from shard_00001.parquet (total Reconnaissance: 12,000)\n",
            "\n",
            "🔄 Loading Active_Attack from: ['DDoS-HTTP_Flood', 'DDoS-SYN_Flood', 'DoS-TCP_Flood', 'DoS-UDP_Flood', 'Mirai-udpplain', 'SqlInjection']\n",
            "   Target: 12,000 samples\n",
            "  📂 Processing DDoS-HTTP_Flood...\n",
            "    ✓ Loaded 1,000 from shard_00000.parquet (total Active_Attack: 1,000)\n",
            "    ✓ Loaded 1,000 from shard_00001.parquet (total Active_Attack: 2,000)\n",
            "    ✓ Loaded 1,000 from shard_00002.parquet (total Active_Attack: 3,000)\n",
            "    ✓ Loaded 1,000 from shard_00003.parquet (total Active_Attack: 4,000)\n",
            "    ✓ Loaded 1,000 from shard_00004.parquet (total Active_Attack: 5,000)\n",
            "    ✓ Loaded 1,000 from shard_00005.parquet (total Active_Attack: 6,000)\n",
            "    ✓ Loaded 1,000 from shard_00006.parquet (total Active_Attack: 7,000)\n",
            "    ✓ Loaded 1,000 from shard_00007.parquet (total Active_Attack: 8,000)\n",
            "    ✓ Loaded 520 from shard_00008.parquet (total Active_Attack: 8,520)\n",
            "    ✓ Loaded 1,000 from shard_00000.parquet (total Active_Attack: 9,520)\n",
            "    ✓ Loaded 738 from shard_00001.parquet (total Active_Attack: 10,258)\n",
            "    ✓ Loaded 1,000 from shard_00000.parquet (total Active_Attack: 11,258)\n",
            "    ✓ Loaded 742 from shard_00001.parquet (total Active_Attack: 12,000)\n",
            "\n",
            "🎉 CIC-IoT23 3-class FULL CAPACITY dataset loaded: 36,000 samples\n",
            "📊 Final class distribution:\n",
            "   Normal         : 12,000 samples (33.3%)\n",
            "   Reconnaissance : 12,000 samples (33.3%)\n",
            "   Active_Attack  : 12,000 samples (33.3%)\n",
            "\n",
            "✓ Capacity achievement: 100.0% of target (36,000 / 36,000)\n",
            "\n",
            "🔄 Reshaping data for LSTM sequential input...\n",
            "   Original shape: (36000, 5120) (flattened: samples × 5120 features)\n",
            "   LSTM shape: (36000, 32, 160) (samples × timesteps × features)\n",
            "   Sequential interpretation: 32 timesteps of 160 features\n",
            "   ➤ Each timestep represents one 'row' of the 5-channel data\n",
            "   ➤ Each feature vector combines all 5 channels for that row\n",
            "\n",
            "🏷️ CIC-IoT23 3-class label distribution:\n",
            "   0: Active_Attack (12,000 samples)\n",
            "   1: Normal (12,000 samples)\n",
            "   2: Reconnaissance (12,000 samples)\n",
            "\n",
            "📈 LSTM data ready: range=[-0.000, 1.000], shape=(36000, 32, 160)\n",
            "🚀 Total samples: 36,000\n",
            "🔄 Ready for LSTM vs CNN vs ViT comparison!\n",
            "\n",
            "💡 Sequential Learning Hypothesis:\n",
            "   • Network attacks often have temporal patterns\n",
            "   • LSTM can model state transitions in protocols\n",
            "   • Memory allows detection of multi-step attacks\n",
            "   • May outperform spatial methods (CNN/ViT) on sequential data\n"
          ]
        }
      ],
      "source": [
        "# Data Loading using Working CIC Approach (same as CNN)\n",
        "import glob\n",
        "\n",
        "def load_cic_3class_full_capacity(base_path, class_mapping, max_samples_per_class):\n",
        "    \"\"\"Load CIC-IoT23 data using exact approach from working ViT notebook\"\"\"\n",
        "    print(f\"📂 Loading CIC-IoT23 3-class FULL CAPACITY dataset from: {base_path}\")\n",
        "    print(f\"🎯 Target: {max_samples_per_class:,} samples per class = {max_samples_per_class * len(class_mapping):,} total\")\n",
        "    \n",
        "    all_image_data = []\n",
        "    all_labels = []\n",
        "    splits = ['train', 'val', 'test']\n",
        "    \n",
        "    print(f\"3-Class mapping (FULL CAPACITY): {class_mapping}\")\n",
        "    \n",
        "    # Track samples collected per combined class\n",
        "    class_samples = {combined_class: 0 for combined_class in class_mapping.keys()}\n",
        "    \n",
        "    # Process each combined class\n",
        "    for combined_class, original_classes in class_mapping.items():\n",
        "        print(f\"\\n🔄 Loading {combined_class} from: {original_classes}\")\n",
        "        print(f\"   Target: {max_samples_per_class:,} samples\")\n",
        "        \n",
        "        for original_class in original_classes:\n",
        "            if class_samples[combined_class] >= max_samples_per_class:\n",
        "                break\n",
        "                \n",
        "            class_dir = f\"{base_path}{original_class}/\"\n",
        "            print(f\"  📂 Processing {original_class}...\")\n",
        "            \n",
        "            for split in splits:\n",
        "                if class_samples[combined_class] >= max_samples_per_class:\n",
        "                    break\n",
        "                    \n",
        "                split_path = f\"{class_dir}{split}/\"\n",
        "                parquet_files = sorted(glob.glob(f\"{split_path}*.parquet\"))\n",
        "                \n",
        "                for file_path in parquet_files:\n",
        "                    if class_samples[combined_class] >= max_samples_per_class:\n",
        "                        break\n",
        "                        \n",
        "                    try:\n",
        "                        df = pd.read_parquet(file_path)\n",
        "                        \n",
        "                        if 'image_data' in df.columns:\n",
        "                            remaining_samples = max_samples_per_class - class_samples[combined_class]\n",
        "                            samples_to_take = min(len(df), remaining_samples)\n",
        "                            \n",
        "                            for idx in range(samples_to_take):\n",
        "                                row = df.iloc[idx]\n",
        "                                image_data = np.array(row['image_data'], dtype=np.float32)\n",
        "                                all_image_data.append(image_data)\n",
        "                                all_labels.append(combined_class)\n",
        "                                class_samples[combined_class] += 1\n",
        "                            \n",
        "                            if samples_to_take > 0:\n",
        "                                print(f\"    ✓ Loaded {samples_to_take:,} from {file_path.split('/')[-1]} (total {combined_class}: {class_samples[combined_class]:,})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ⚠️ Error loading {file_path}: {e}\")\n",
        "    \n",
        "    X = np.array(all_image_data, dtype=np.float32)\n",
        "    y = np.array(all_labels)\n",
        "    \n",
        "    print(f\"\\n🎉 CIC-IoT23 3-class FULL CAPACITY dataset loaded: {len(X):,} samples\")\n",
        "    print(f\"📊 Final class distribution:\")\n",
        "    for combined_class, count in class_samples.items():\n",
        "        percentage = (count / len(X)) * 100\n",
        "        print(f\"   {combined_class:15s}: {count:,} samples ({percentage:.1f}%)\")\n",
        "    \n",
        "    total_target = max_samples_per_class * len(class_mapping)\n",
        "    achievement = (len(X) / total_target) * 100\n",
        "    print(f\"\\n✓ Capacity achievement: {achievement:.1f}% of target ({len(X):,} / {total_target:,})\")\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Load FULL CAPACITY CIC data\n",
        "X, y = load_cic_3class_full_capacity(CONFIG['data_path'], CLASS_MAPPING, CONFIG['max_samples_per_class'])\n",
        "\n",
        "# Reshape data for LSTM: (samples, features) -> (samples, sequence_length, input_features)\n",
        "print(f\"\\n🔄 Reshaping data for LSTM sequential input...\")\n",
        "print(f\"   Original shape: {X.shape} (flattened: samples × 5120 features)\")\n",
        "\n",
        "# Reshape from (36000, 5120) to (36000, 32, 160) for LSTM\n",
        "# Treat as 32 timesteps with 160 features each (5 channels × 32 width)\n",
        "X = X.reshape(-1, CONFIG['sequence_length'], CONFIG['input_features'])\n",
        "print(f\"   LSTM shape: {X.shape} (samples × timesteps × features)\")\n",
        "print(f\"   Sequential interpretation: {CONFIG['sequence_length']} timesteps of {CONFIG['input_features']} features\")\n",
        "print(f\"   ➤ Each timestep represents one 'row' of the 5-channel data\")\n",
        "print(f\"   ➤ Each feature vector combines all 5 channels for that row\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"\\n🏷️ CIC-IoT23 3-class label distribution:\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    count = np.sum(y == label)\n",
        "    print(f\"   {i}: {label} ({count:,} samples)\")\n",
        "\n",
        "print(f\"\\n📈 LSTM data ready: range=[{X.min():.3f}, {X.max():.3f}], shape={X.shape}\")\n",
        "print(f\"🚀 Total samples: {len(X):,}\")\n",
        "print(f\"🔄 Ready for LSTM vs CNN vs ViT comparison!\")\n",
        "print(f\"\\n💡 Sequential Learning Hypothesis:\")\n",
        "print(f\"   • Network attacks often have temporal patterns\")\n",
        "print(f\"   • LSTM can model state transitions in protocols\")\n",
        "print(f\"   • Memory allows detection of multi-step attacks\")\n",
        "print(f\"   • May outperform spatial methods (CNN/ViT) on sequential data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Data Split Summary:\n",
            "   Training: 21,600 samples (60.0%)\n",
            "   Validation: 7,200 samples (20.0%)\n",
            "   Test: 7,200 samples (20.0%)\n",
            "   Sequential format: timesteps=32, features=160\n",
            "\n",
            "⚖️  Class weights: {np.str_('Active_Attack'): np.float64(1.0), np.str_('Normal'): np.float64(1.0), np.str_('Reconnaissance'): np.float64(1.0)}\n",
            "\n",
            "🎯 LSTM Training Configuration:\n",
            "   📊 Optimizer: Adam (lr=0.001, weight_decay=1e-4)\n",
            "   📊 Loss: Weighted CrossEntropyLoss\n",
            "   📊 Scheduler: ReduceLROnPlateau (patience=3)\n",
            "   📊 Batch size: 64\n",
            "   📊 Max epochs: 50\n",
            "   📊 Early stopping patience: 7\n",
            "   🔄 Sequential processing: 32 timesteps per sample\n",
            "\n",
            "🎯 Architecture Comparison Setup:\n",
            "   🤖 ViT: Global attention, 96.94% target\n",
            "   🏗️  CNN: Local convolution, ~93%+ (training)\n",
            "   🔄 LSTM: Sequential memory, starting training next...\n"
          ]
        }
      ],
      "source": [
        "# Data Splitting and Training Setup\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Split data into train/val/test (same random_state as CNN/ViT for fairness)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=CONFIG['test_size'], \n",
        "    random_state=CONFIG['random_state'], \n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=CONFIG['val_size']/(1-CONFIG['test_size']), \n",
        "    random_state=CONFIG['random_state'], \n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"📊 Data Split Summary:\")\n",
        "print(f\"   Training: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   Sequential format: timesteps={X_train.shape[1]}, features={X_train.shape[2]}\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_val_tensor = torch.FloatTensor(X_val)\n",
        "y_val_tensor = torch.LongTensor(y_val)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "\n",
        "# Compute class weights for balanced training\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_tensor = torch.FloatTensor(class_weights).to(CONFIG['device'])\n",
        "\n",
        "print(f\"\\n⚖️  Class weights: {dict(zip(label_encoder.classes_, class_weights))}\")\n",
        "\n",
        "# Initialize optimizer and loss function\n",
        "# Higher learning rate for LSTM compared to CNN/ViT\n",
        "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', patience=3, factor=0.5\n",
        ")\n",
        "\n",
        "print(f\"\\n🎯 LSTM Training Configuration:\")\n",
        "print(f\"   📊 Optimizer: Adam (lr={CONFIG['learning_rate']}, weight_decay=1e-4)\")\n",
        "print(f\"   📊 Loss: Weighted CrossEntropyLoss\")\n",
        "print(f\"   📊 Scheduler: ReduceLROnPlateau (patience=3)\")\n",
        "print(f\"   📊 Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"   📊 Max epochs: {CONFIG['epochs']}\")\n",
        "print(f\"   📊 Early stopping patience: {CONFIG['patience']}\")\n",
        "print(f\"   🔄 Sequential processing: {CONFIG['sequence_length']} timesteps per sample\")\n",
        "\n",
        "print(f\"\\n🎯 Architecture Comparison Setup:\")\n",
        "print(f\"   🤖 ViT: Global attention, 96.94% target\")\n",
        "print(f\"   🏗️  CNN: Local convolution, ~93%+ (training)\")\n",
        "print(f\"   🔄 LSTM: Sequential memory, starting training next...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Starting LSTM sequential learning...\n",
            "🎯 Targets: Beat ViT (96.94%) and CNN (~93%+)\n",
            "💡 Hypothesis: Sequential patterns > Spatial patterns for IoT security\n",
            "\n",
            "Epoch  1/50 | Train: 0.6969 (0.6745) | Val: 0.7683 (0.5153) | LR: 0.001000\n",
            "✅ New best validation accuracy: 0.7683\n",
            "Epoch  2/50 | Train: 0.8172 (0.4666) | Val: 0.8331 (0.3780) | LR: 0.001000\n",
            "✅ New best validation accuracy: 0.8331\n",
            "Epoch  3/50 | Train: 0.8553 (0.3930) | Val: 0.8769 (0.3111) | LR: 0.001000\n",
            "✅ New best validation accuracy: 0.8769\n",
            "Epoch  4/50 | Train: 0.8780 (0.3260) | Val: 0.8883 (0.2761) | LR: 0.001000\n",
            "✅ New best validation accuracy: 0.8883\n",
            "Epoch  5/50 | Train: 0.8837 (0.3131) | Val: 0.8757 (0.3086) | LR: 0.001000\n",
            "Epoch  6/50 | Train: 0.8889 (0.2908) | Val: 0.9047 (0.2355) | LR: 0.001000\n",
            "✅ New best validation accuracy: 0.9047\n",
            "Epoch  7/50 | Train: 0.8941 (0.2711) | Val: 0.9006 (0.2482) | LR: 0.001000\n",
            "Epoch  8/50 | Train: 0.8941 (0.2732) | Val: 0.8994 (0.2651) | LR: 0.001000\n",
            "Epoch  9/50 | Train: 0.8950 (0.2651) | Val: 0.9038 (0.2313) | LR: 0.001000\n",
            "Epoch 10/50 | Train: 0.8979 (0.2492) | Val: 0.8947 (0.2842) | LR: 0.000500\n",
            "Epoch 11/50 | Train: 0.9040 (0.2286) | Val: 0.9107 (0.2171) | LR: 0.000500\n",
            "✅ New best validation accuracy: 0.9107\n",
            "Epoch 12/50 | Train: 0.9068 (0.2238) | Val: 0.9115 (0.2265) | LR: 0.000500\n",
            "✅ New best validation accuracy: 0.9115\n",
            "Epoch 13/50 | Train: 0.9100 (0.2177) | Val: 0.9150 (0.2096) | LR: 0.000500\n",
            "✅ New best validation accuracy: 0.9150\n",
            "Epoch 14/50 | Train: 0.9121 (0.2111) | Val: 0.9207 (0.1881) | LR: 0.000500\n",
            "✅ New best validation accuracy: 0.9207\n",
            "Epoch 15/50 | Train: 0.9189 (0.1910) | Val: 0.9197 (0.1884) | LR: 0.000500\n",
            "Epoch 16/50 | Train: 0.9199 (0.1899) | Val: 0.9276 (0.1842) | LR: 0.000500\n",
            "✅ New best validation accuracy: 0.9276\n",
            "Epoch 17/50 | Train: 0.9236 (0.1857) | Val: 0.9304 (0.1742) | LR: 0.000500\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9304 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9304\n",
            "Epoch 18/50 | Train: 0.9247 (0.1786) | Val: 0.9221 (0.1784) | LR: 0.000500\n",
            "Epoch 19/50 | Train: 0.9270 (0.1769) | Val: 0.9235 (0.1786) | LR: 0.000500\n",
            "Epoch 20/50 | Train: 0.9290 (0.1685) | Val: 0.9276 (0.2219) | LR: 0.000500\n",
            "Epoch 21/50 | Train: 0.9291 (0.1677) | Val: 0.9250 (0.2031) | LR: 0.000250\n",
            "Epoch 22/50 | Train: 0.9385 (0.1470) | Val: 0.9319 (0.1805) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9319 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9319\n",
            "Epoch 23/50 | Train: 0.9383 (0.1460) | Val: 0.9358 (0.1698) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9358 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9358\n",
            "Epoch 24/50 | Train: 0.9438 (0.1391) | Val: 0.9400 (0.1553) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9400 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9400\n",
            "Epoch 25/50 | Train: 0.9438 (0.1352) | Val: 0.9389 (0.1660) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9389 vs CNN: ~0.93+\n",
            "Epoch 26/50 | Train: 0.9464 (0.1335) | Val: 0.9424 (0.1614) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9424 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9424\n",
            "Epoch 27/50 | Train: 0.9498 (0.1289) | Val: 0.9351 (0.1552) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9351 vs CNN: ~0.93+\n",
            "Epoch 28/50 | Train: 0.9499 (0.1276) | Val: 0.9486 (0.1489) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9486 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9486\n",
            "Epoch 29/50 | Train: 0.9535 (0.1215) | Val: 0.9454 (0.1519) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9454 vs CNN: ~0.93+\n",
            "Epoch 30/50 | Train: 0.9520 (0.1260) | Val: 0.9496 (0.1451) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9496 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9496\n",
            "Epoch 31/50 | Train: 0.9573 (0.1188) | Val: 0.9485 (0.1446) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9485 vs CNN: ~0.93+\n",
            "Epoch 32/50 | Train: 0.9583 (0.1162) | Val: 0.9504 (0.1513) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9504 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9504\n",
            "Epoch 33/50 | Train: 0.9581 (0.1103) | Val: 0.9431 (0.1659) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9431 vs CNN: ~0.93+\n",
            "Epoch 34/50 | Train: 0.9598 (0.1087) | Val: 0.9483 (0.1349) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9483 vs CNN: ~0.93+\n",
            "Epoch 35/50 | Train: 0.9604 (0.1048) | Val: 0.9549 (0.1397) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9549 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9549\n",
            "Epoch 36/50 | Train: 0.9587 (0.1114) | Val: 0.9536 (0.1573) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9536 vs CNN: ~0.93+\n",
            "Epoch 37/50 | Train: 0.9611 (0.1050) | Val: 0.9557 (0.1346) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9557 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9557\n",
            "Epoch 38/50 | Train: 0.9633 (0.0993) | Val: 0.9557 (0.1363) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9557 vs CNN: ~0.93+\n",
            "Epoch 39/50 | Train: 0.9618 (0.1044) | Val: 0.9583 (0.1317) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9583 vs CNN: ~0.93+\n",
            "✅ New best validation accuracy: 0.9583\n",
            "Epoch 40/50 | Train: 0.9633 (0.1016) | Val: 0.9579 (0.1281) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9579 vs CNN: ~0.93+\n",
            "Epoch 41/50 | Train: 0.9649 (0.0972) | Val: 0.9519 (0.1520) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9519 vs CNN: ~0.93+\n",
            "Epoch 42/50 | Train: 0.9639 (0.0986) | Val: 0.9540 (0.1307) | LR: 0.000250\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9540 vs CNN: ~0.93+\n",
            "Epoch 43/50 | Train: 0.9669 (0.0932) | Val: 0.9549 (0.1382) | LR: 0.000125\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9549 vs CNN: ~0.93+\n",
            "Epoch 44/50 | Train: 0.9690 (0.0821) | Val: 0.9613 (0.1176) | LR: 0.000125\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9613 vs CNN: ~0.93+\n",
            "🎯 LSTM approaching ViT performance! Current: 0.9613 vs ViT: 0.9694\n",
            "✅ New best validation accuracy: 0.9613\n",
            "Epoch 45/50 | Train: 0.9701 (0.0797) | Val: 0.9613 (0.1233) | LR: 0.000125\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9613 vs CNN: ~0.93+\n",
            "🎯 LSTM approaching ViT performance! Current: 0.9613 vs ViT: 0.9694\n",
            "Epoch 46/50 | Train: 0.9704 (0.0783) | Val: 0.9587 (0.1449) | LR: 0.000125\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9587 vs CNN: ~0.93+\n",
            "Epoch 47/50 | Train: 0.9720 (0.0791) | Val: 0.9597 (0.1257) | LR: 0.000125\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9597 vs CNN: ~0.93+\n",
            "Epoch 48/50 | Train: 0.9712 (0.0758) | Val: 0.9632 (0.1240) | LR: 0.000125\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9632 vs CNN: ~0.93+\n",
            "🎯 LSTM approaching ViT performance! Current: 0.9632 vs ViT: 0.9694\n",
            "✅ New best validation accuracy: 0.9632\n",
            "Epoch 49/50 | Train: 0.9721 (0.0778) | Val: 0.9592 (0.1499) | LR: 0.000125\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9592 vs CNN: ~0.93+\n",
            "Epoch 50/50 | Train: 0.9713 (0.0775) | Val: 0.9636 (0.1248) | LR: 0.000125\n",
            "🔄 LSTM approaching CNN performance! Current: 0.9636 vs CNN: ~0.93+\n",
            "🎯 LSTM approaching ViT performance! Current: 0.9636 vs ViT: 0.9694\n",
            "✅ New best validation accuracy: 0.9636\n",
            "\n",
            "🎯 LSTM Training Complete!\n",
            "   ⏱️  Total time: 0:16:21.743299\n",
            "   🏆 Best validation accuracy: 0.9636\n",
            "   📊 Total epochs: 50\n",
            "\n",
            "📊 ARCHITECTURE COMPARISON:\n",
            "   🤖 ViT (Global Attention): 0.9694\n",
            "   🏗️  CNN (Local Features): ~0.93+ (training)\n",
            "   🔄 LSTM (Sequential): 0.9636\n",
            "\n",
            "🥈 LSTM BEATS CNN but trails ViT\n",
            "   Sequential > Local features, but Global attention still leads\n"
          ]
        }
      ],
      "source": [
        "# LSTM Training Pipeline with Sequential Processing\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        # data shape: (batch_size, sequence_length, input_features)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping for LSTM stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "    \n",
        "    return total_loss / len(train_loader), correct / total\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    \n",
        "    return total_loss / len(val_loader), correct / total\n",
        "\n",
        "# Training loop with early stopping\n",
        "print(\"🔄 Starting LSTM sequential learning...\")\n",
        "print(\"🎯 Targets: Beat ViT (96.94%) and CNN (~93%+)\")\n",
        "print(\"💡 Hypothesis: Sequential patterns > Spatial patterns for IoT security\\n\")\n",
        "\n",
        "best_val_acc = 0\n",
        "patience_counter = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    # Train for one epoch\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, CONFIG['device'])\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, CONFIG['device'])\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step(val_acc)\n",
        "    \n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    # Print progress with architecture comparison\n",
        "    print(f\"Epoch {epoch+1:2d}/{CONFIG['epochs']} | \"\n",
        "          f\"Train: {train_acc:.4f} ({train_loss:.4f}) | \"\n",
        "          f\"Val: {val_acc:.4f} ({val_loss:.4f}) | \"\n",
        "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Architecture comparison updates\n",
        "    if val_acc > 0.93:\n",
        "        print(f\"🔄 LSTM approaching CNN performance! Current: {val_acc:.4f} vs CNN: ~0.93+\")\n",
        "    if val_acc > 0.96:\n",
        "        print(f\"🎯 LSTM approaching ViT performance! Current: {val_acc:.4f} vs ViT: 0.9694\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_lstm_3class_full_capacity_model.pth')\n",
        "        print(f\"✅ New best validation accuracy: {val_acc:.4f}\")\n",
        "        \n",
        "        # Check performance milestones\n",
        "        if val_acc > 0.9694:\n",
        "            print(f\"🏆 LSTM BEATS ViT! {val_acc:.4f} > 0.9694\")\n",
        "            print(f\"🔄 Sequential modeling proves superior for IoT cybersecurity!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        \n",
        "        if patience_counter >= CONFIG['patience']:\n",
        "            print(f\"\\n⏰ Early stopping triggered after {epoch+1} epochs\")\n",
        "            print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
        "            break\n",
        "\n",
        "training_time = datetime.now() - start_time\n",
        "print(f\"\\n🎯 LSTM Training Complete!\")\n",
        "print(f\"   ⏱️  Total time: {training_time}\")\n",
        "print(f\"   🏆 Best validation accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"   📊 Total epochs: {epoch+1}\")\n",
        "\n",
        "# Compare with baselines\n",
        "vit_accuracy = 0.9694\n",
        "cnn_accuracy = 0.93  # Conservative estimate\n",
        "\n",
        "print(f\"\\n📊 ARCHITECTURE COMPARISON:\")\n",
        "print(f\"   🤖 ViT (Global Attention): {vit_accuracy:.4f}\")\n",
        "print(f\"   🏗️  CNN (Local Features): ~{cnn_accuracy:.2f}+ (training)\")\n",
        "print(f\"   🔄 LSTM (Sequential): {best_val_acc:.4f}\")\n",
        "\n",
        "if best_val_acc > vit_accuracy:\n",
        "    improvement = (best_val_acc - vit_accuracy) * 100\n",
        "    print(f\"\\n🎉 LSTM OUTPERFORMS ALL BASELINES!\")\n",
        "    print(f\"   LSTM beats ViT by +{improvement:.2f} percentage points\")\n",
        "    print(f\"   🔄 Sequential modeling is superior for IoT cybersecurity\")\n",
        "elif best_val_acc > cnn_accuracy:\n",
        "    print(f\"\\n🥈 LSTM BEATS CNN but trails ViT\")\n",
        "    print(f\"   Sequential > Local features, but Global attention still leads\")\n",
        "else:\n",
        "    print(f\"\\n📊 LSTM provides alternative perspective\")\n",
        "    print(f\"   All architectures show competitive performance\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Final LSTM Test Set Evaluation...\n",
            "\n",
            "🎯 FINAL LSTM RESULTS:\n",
            "   📊 Test Accuracy: 0.9615 (96.15%)\n",
            "\n",
            "🏆 MULTI-ARCHITECTURE COMPARISON:\n",
            "   🥇 ViT (Global Attention): 0.9694 (96.94%)\n",
            "   🥈 LSTM (Sequential): 0.9615 (96.15%)\n",
            "   🥉 CNN (Local Features): 0.9300 (93.00%)\n",
            "\n",
            "🥈 LSTM EXCELLENT Performance!\n",
            "   LSTM: 0.9615 vs ViT: 0.9694\n",
            "   Deficit: -0.79 percentage points\n",
            "\n",
            "📋 Detailed LSTM Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            " Active_Attack     0.9962    0.9733    0.9846      2400\n",
            "        Normal     0.9627    0.9363    0.9493      2400\n",
            "Reconnaissance     0.9282    0.9750    0.9510      2400\n",
            "\n",
            "      accuracy                         0.9615      7200\n",
            "     macro avg     0.9624    0.9615    0.9616      7200\n",
            "  weighted avg     0.9624    0.9615    0.9616      7200\n",
            "\n",
            "\n",
            "🎯 LSTM Prediction Confidence Analysis:\n",
            "   Mean confidence: 0.9774\n",
            "   High confidence (>0.9): 6,738 samples\n",
            "   Low confidence (<0.7): 241 samples\n",
            "\n",
            "💾 Results saved to: results_lstm_3class_32x32_full_capacity.json\n",
            "\n",
            "🎯 MULTI-ARCHITECTURE RESEARCH SUMMARY:\n",
            "   🤖 ViT (Attention): 0.9694 (~150K params)\n",
            "   🏗️  CNN (Convolution): ~0.93+ (4.8M params)\n",
            "   🔄 LSTM (Sequential): 0.9615 (355,331 params)\n",
            "   📊 Performance Tier: 🥈 EXCELLENT\n",
            "\n",
            "🔬 RESEARCH IMPLICATIONS:\n",
            "   📊 Multiple architectures achieve competitive performance\n",
            "   📊 Choice depends on deployment constraints\n",
            "   📊 All approaches valid for IoT cybersecurity\n",
            "\n",
            "🔄 NEXT STEPS:\n",
            "   1. ✅ LSTM baseline established: 0.9615\n",
            "   2. 🔄 Run cross-dataset validation on UNSW-NB15\n",
            "   3. 🔄 Compare LSTM vs CNN vs ViT domain transfer\n",
            "   4. 🔄 Implement ensemble methods (LSTM+CNN+ViT)\n",
            "   5. 📊 Complete Kyle's multi-architecture paper\n",
            "\n",
            "🎓 Ready for publication: Comprehensive IoT Cybersecurity Architecture Study!\n"
          ]
        }
      ],
      "source": [
        "# LSTM Evaluation & Multi-Architecture Results Analysis\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_lstm_3class_full_capacity_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Test set evaluation\n",
        "def evaluate_model(model, test_loader, device, label_encoder):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "    \n",
        "    return np.array(all_preds), np.array(all_targets), np.array(all_probs)\n",
        "\n",
        "print(\"🧪 Final LSTM Test Set Evaluation...\")\n",
        "test_preds, test_targets, test_probs = evaluate_model(model, test_loader, CONFIG['device'], label_encoder)\n",
        "test_accuracy = accuracy_score(test_targets, test_preds)\n",
        "\n",
        "print(f\"\\n🎯 FINAL LSTM RESULTS:\")\n",
        "print(f\"   📊 Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Multi-architecture performance comparison\n",
        "vit_test_accuracy = 0.9694\n",
        "cnn_test_accuracy = 0.93  # Conservative estimate until CNN completes\n",
        "\n",
        "print(f\"\\n🏆 MULTI-ARCHITECTURE COMPARISON:\")\n",
        "architectures = {\n",
        "    'ViT (Global Attention)': vit_test_accuracy,\n",
        "    'CNN (Local Features)': cnn_test_accuracy,\n",
        "    'LSTM (Sequential)': test_accuracy\n",
        "}\n",
        "\n",
        "# Sort by performance\n",
        "sorted_archs = sorted(architectures.items(), key=lambda x: x[1], reverse=True)\n",
        "for i, (arch, acc) in enumerate(sorted_archs):\n",
        "    medal = ['🥇', '🥈', '🥉'][i] if i < 3 else '📊'\n",
        "    print(f\"   {medal} {arch}: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "# Performance tier classification\n",
        "if test_accuracy > vit_test_accuracy:\n",
        "    improvement = (test_accuracy - vit_test_accuracy) * 100\n",
        "    print(f\"\\n🎉 LSTM ACHIEVES NEW STATE-OF-THE-ART!\")\n",
        "    print(f\"   LSTM: {test_accuracy:.4f} vs ViT: {vit_test_accuracy:.4f}\")\n",
        "    print(f\"   Improvement: +{improvement:.2f} percentage points\")\n",
        "    print(f\"   🔄 Sequential modeling proves superior for IoT cybersecurity!\")\n",
        "    tier = \"🥇 STATE-OF-THE-ART\"\n",
        "elif test_accuracy > 0.95:\n",
        "    deficit = (vit_test_accuracy - test_accuracy) * 100\n",
        "    print(f\"\\n🥈 LSTM EXCELLENT Performance!\")\n",
        "    print(f\"   LSTM: {test_accuracy:.4f} vs ViT: {vit_test_accuracy:.4f}\")\n",
        "    print(f\"   Deficit: -{deficit:.2f} percentage points\")\n",
        "    tier = \"🥈 EXCELLENT\"\n",
        "elif test_accuracy > 0.90:\n",
        "    deficit = (vit_test_accuracy - test_accuracy) * 100\n",
        "    print(f\"\\n🥉 LSTM VERY GOOD Performance!\")\n",
        "    print(f\"   LSTM: {test_accuracy:.4f} vs ViT: {vit_test_accuracy:.4f}\")\n",
        "    print(f\"   Deficit: -{deficit:.2f} percentage points\")\n",
        "    tier = \"🥉 VERY GOOD\"\n",
        "else:\n",
        "    deficit = (vit_test_accuracy - test_accuracy) * 100\n",
        "    print(f\"\\n📊 LSTM Performance Analysis:\")\n",
        "    print(f\"   LSTM: {test_accuracy:.4f} vs ViT: {vit_test_accuracy:.4f}\")\n",
        "    print(f\"   Deficit: -{deficit:.2f} percentage points\")\n",
        "    tier = \"📊 BASELINE\"\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\n📋 Detailed LSTM Classification Report:\")\n",
        "class_names = label_encoder.classes_\n",
        "report = classification_report(test_targets, test_preds, target_names=class_names, digits=4)\n",
        "print(report)\n",
        "\n",
        "# Confidence analysis\n",
        "confidence_scores = np.max(test_probs, axis=1)\n",
        "mean_confidence = np.mean(confidence_scores)\n",
        "print(f\"\\n🎯 LSTM Prediction Confidence Analysis:\")\n",
        "print(f\"   Mean confidence: {mean_confidence:.4f}\")\n",
        "print(f\"   High confidence (>0.9): {np.sum(confidence_scores > 0.9):,} samples\")\n",
        "print(f\"   Low confidence (<0.7): {np.sum(confidence_scores < 0.7):,} samples\")\n",
        "\n",
        "# Save comprehensive results\n",
        "results = {\n",
        "    'experiment': 'LSTM_CIC_3class_full_capacity',\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'dataset': 'CIC-IoT23',\n",
        "    'approach': '3-class semantic grouping',\n",
        "    'architecture': 'LSTM_Sequential',\n",
        "    'total_samples': len(X),\n",
        "    'samples_per_class': CONFIG['max_samples_per_class'],\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'validation_accuracy': float(best_val_acc),\n",
        "    'training_epochs': epoch + 1,\n",
        "    'training_time': str(training_time),\n",
        "    'parameters': trainable_params,\n",
        "    'sequence_config': {\n",
        "        'sequence_length': CONFIG['sequence_length'],\n",
        "        'input_features': CONFIG['input_features'],\n",
        "        'hidden_size': CONFIG['hidden_size'],\n",
        "        'num_layers': CONFIG['num_layers']\n",
        "    },\n",
        "    'multi_architecture_comparison': {\n",
        "        'vit_baseline': vit_test_accuracy,\n",
        "        'cnn_baseline': cnn_test_accuracy,\n",
        "        'lstm_performance': float(test_accuracy),\n",
        "        'lstm_vs_vit_diff': float(test_accuracy - vit_test_accuracy),\n",
        "        'lstm_vs_cnn_diff': float(test_accuracy - cnn_test_accuracy)\n",
        "    },\n",
        "    'confidence_analysis': {\n",
        "        'mean_confidence': float(mean_confidence),\n",
        "        'high_confidence_samples': int(np.sum(confidence_scores > 0.9)),\n",
        "        'low_confidence_samples': int(np.sum(confidence_scores < 0.7))\n",
        "    },\n",
        "    'performance_tier': tier,\n",
        "    'classification_report': report,\n",
        "    'class_names': list(class_names)\n",
        "}\n",
        "\n",
        "with open('results_lstm_3class_32x32_full_capacity.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\n💾 Results saved to: results_lstm_3class_32x32_full_capacity.json\")\n",
        "print(f\"\\n🎯 MULTI-ARCHITECTURE RESEARCH SUMMARY:\")\n",
        "print(f\"   🤖 ViT (Attention): {vit_test_accuracy:.4f} (~150K params)\")\n",
        "print(f\"   🏗️  CNN (Convolution): ~{cnn_test_accuracy:.2f}+ (4.8M params)\")\n",
        "print(f\"   🔄 LSTM (Sequential): {test_accuracy:.4f} ({trainable_params:,} params)\")\n",
        "print(f\"   📊 Performance Tier: {tier}\")\n",
        "\n",
        "# Research implications\n",
        "print(f\"\\n🔬 RESEARCH IMPLICATIONS:\")\n",
        "if test_accuracy > vit_test_accuracy:\n",
        "    print(f\"   ✅ Sequential modeling superior for IoT cybersecurity\")\n",
        "    print(f\"   ✅ Temporal patterns more important than spatial/attention\")\n",
        "    print(f\"   ✅ LSTM architecture recommended for deployment\")\n",
        "else:\n",
        "    print(f\"   📊 Multiple architectures achieve competitive performance\")\n",
        "    print(f\"   📊 Choice depends on deployment constraints\")\n",
        "    print(f\"   📊 All approaches valid for IoT cybersecurity\")\n",
        "\n",
        "print(f\"\\n🔄 NEXT STEPS:\")\n",
        "print(f\"   1. ✅ LSTM baseline established: {test_accuracy:.4f}\")\n",
        "print(f\"   2. 🔄 Run cross-dataset validation on UNSW-NB15\")\n",
        "print(f\"   3. 🔄 Compare LSTM vs CNN vs ViT domain transfer\")\n",
        "print(f\"   4. 🔄 Implement ensemble methods (LSTM+CNN+ViT)\")\n",
        "print(f\"   5. 📊 Complete your team's multi-architecture paper\")\n",
        "print(f\"\\n🎓 Ready for publication: Comprehensive IoT Cybersecurity Architecture Study!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
