{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-6ktiuoUvoJ"
      },
      "source": [
        "# Create Multi-Format Dataset from PCAP Files (Organized by Label)\n",
        "\n",
        "**Objective:** Process PCAP files from GCS to extract packet payloads and create a balanced dataset with 12k samples per label in Parquet and PNG formats.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook:\n",
        "1. Reads PCAP files from `gs://ai-cyber/datasets/cic-iot23/pcap/`\n",
        "2. Extracts packet payloads (first 1500 bytes)\n",
        "3. Uses folder names as labels (34 attack types)\n",
        "4. Creates multiple image encoding formats\n",
        "5. **Saves data ORGANIZED BY LABEL** for easy selective downloading\n",
        "6. Outputs in Parquet (for ML) and PNG (for visualization) formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uQ6LTSWUvoM",
        "outputId": "bb485635-a425-4efe-cf9a-53d67f87dabd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Environment configured\n",
            "✓ Will process PCAP files from: gs://ai-cyber/datasets/cic-iot23/pcap/\n",
            "✓ Output: gs://ai-cyber/datasets/pcap-organized-by-label/\n",
            "✓ Target samples per class: 12,000\n",
            "✓ Data will be ORGANIZED BY LABEL for easy access!\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import struct\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.cloud import storage\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import io\n",
        "import time\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "from PIL import Image\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'bucket_name': 'ai-cyber',\n",
        "    'input_prefix': 'datasets/cic-iot23/pcap/',\n",
        "    'output_prefix': 'datasets/pcap-organized-by-label/',  # Changed to reflect better organization\n",
        "    'samples_per_class': 12000,  # Changed to 12k as per user requirement\n",
        "    'payload_bytes': 1500,  # First 1500 bytes of packet\n",
        "    'test_size': 0.15,\n",
        "    'val_size': 0.15,\n",
        "    'random_seed': 42,\n",
        "    'packets_per_pcap': 100000,  # Process in chunks\n",
        "    'shard_size': 1000,  # Samples per shard\n",
        "    'max_workers': 4,\n",
        "    'save_sample_pngs': 100,  # Save first N PNGs per class\n",
        "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "}\n",
        "\n",
        "# Image format configurations\n",
        "IMAGE_FORMATS = {\n",
        "    'grayscale_32x32': {'shape': (32, 32), 'channels': 1, 'method': 'sequential'},\n",
        "    'grayscale_39x39': {'shape': (39, 39), 'channels': 1, 'method': 'sequential'},\n",
        "    'grayscale_64x64': {'shape': (64, 64), 'channels': 1, 'method': 'sequential'},\n",
        "    'rgb_hilbert_32x32': {'shape': (32, 32), 'channels': 3, 'method': 'hilbert'},\n",
        "    'rgb_spiral_32x32': {'shape': (32, 32), 'channels': 3, 'method': 'spiral'},\n",
        "    '5channel_32x32': {'shape': (32, 32), 'channels': 5, 'method': 'multiview'}\n",
        "}\n",
        "\n",
        "np.random.seed(CONFIG['random_seed'])\n",
        "\n",
        "print(\"✓ Environment configured\")\n",
        "print(f\"✓ Will process PCAP files from: gs://{CONFIG['bucket_name']}/{CONFIG['input_prefix']}\")\n",
        "print(f\"✓ Output: gs://{CONFIG['bucket_name']}/{CONFIG['output_prefix']}\")\n",
        "print(f\"✓ Target samples per class: {CONFIG['samples_per_class']:,}\")\n",
        "print(f\"✓ Data will be ORGANIZED BY LABEL for easy access!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHoyEoseUvoN"
      },
      "source": [
        "## PCAP Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f6QgqhpUvoN",
        "outputId": "55199c52-ac33-4730-f584-36102d7bec3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ PCAP processing functions ready\n"
          ]
        }
      ],
      "source": [
        "def read_pcap_packets(pcap_data, max_packets=None):\n",
        "    \"\"\"Extract packets from PCAP data\"\"\"\n",
        "    packets = []\n",
        "\n",
        "    # PCAP global header is 24 bytes\n",
        "    if len(pcap_data) < 24:\n",
        "        return packets\n",
        "\n",
        "    # Skip global header\n",
        "    offset = 24\n",
        "    packet_count = 0\n",
        "\n",
        "    while offset < len(pcap_data):\n",
        "        # Check if we have enough data for packet header (16 bytes)\n",
        "        if offset + 16 > len(pcap_data):\n",
        "            break\n",
        "\n",
        "        # Read packet header\n",
        "        ts_sec, ts_usec, incl_len, orig_len = struct.unpack('IIII', pcap_data[offset:offset+16])\n",
        "        offset += 16\n",
        "\n",
        "        # Check if we have the packet data\n",
        "        if offset + incl_len > len(pcap_data):\n",
        "            break\n",
        "\n",
        "        # Extract packet data\n",
        "        packet_data = pcap_data[offset:offset+incl_len]\n",
        "        offset += incl_len\n",
        "\n",
        "        # Extract payload (skip Ethernet header - 14 bytes, IP header - 20 bytes min)\n",
        "        # This is simplified - in reality we'd parse headers properly\n",
        "        if len(packet_data) > 34:  # At least Ethernet + minimal IP header\n",
        "            # For simplicity, we'll use the entire packet as \"payload\"\n",
        "            # In production, you'd properly parse headers\n",
        "            payload = packet_data[14:]  # Skip Ethernet header\n",
        "\n",
        "            # Ensure we have at least some data\n",
        "            if len(payload) > 20:  # More than just IP header\n",
        "                packets.append({\n",
        "                    'timestamp': ts_sec + ts_usec/1000000,\n",
        "                    'payload': payload[:CONFIG['payload_bytes']],  # First 1500 bytes\n",
        "                    'length': len(payload)\n",
        "                })\n",
        "\n",
        "        packet_count += 1\n",
        "        if max_packets and packet_count >= max_packets:\n",
        "            break\n",
        "\n",
        "    return packets\n",
        "\n",
        "def extract_label_from_path(gcs_path):\n",
        "    \"\"\"Extract label from GCS path (folder name)\"\"\"\n",
        "    # Example: datasets/cic-iot23/pcap/DDoS-HTTP_Flood/file.pcap\n",
        "    parts = gcs_path.split('/')\n",
        "    if len(parts) >= 2:\n",
        "        return parts[-2]  # Folder name is the label\n",
        "    return 'unknown'\n",
        "\n",
        "print(\"✓ PCAP processing functions ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgMEzTE1UvoO"
      },
      "source": [
        "## Image Encoding Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVq43fxEUvoO",
        "outputId": "ed357306-15d5-453a-d4ed-bfaaf5cec3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Image encoding functions ready\n"
          ]
        }
      ],
      "source": [
        "def hilbert_curve_positions(n):\n",
        "    \"\"\"Generate Hilbert curve positions for n×n grid\"\"\"\n",
        "    def hilbert(x, y, xi, xj, yi, yj, n):\n",
        "        if n <= 0:\n",
        "            yield x + (xi + yi) // 2, y + (xj + yj) // 2\n",
        "        else:\n",
        "            for i in hilbert(x, y, yi//2, yj//2, xi//2, xj//2, n-1):\n",
        "                yield i\n",
        "            for i in hilbert(x + xi//2, y + xj//2, xi//2, xj//2, yi//2, yj//2, n-1):\n",
        "                yield i\n",
        "            for i in hilbert(x + xi//2 + yi//2, y + xj//2 + yj//2, xi//2, xj//2, yi//2, yj//2, n-1):\n",
        "                yield i\n",
        "            for i in hilbert(x + xi//2 + yi, y + xj//2 + yj, -yi//2, -yj//2, -xi//2, -xj//2, n-1):\n",
        "                yield i\n",
        "\n",
        "    return list(hilbert(0, 0, n, 0, 0, n, int(np.log2(n))))\n",
        "\n",
        "def spiral_positions(n):\n",
        "    \"\"\"Generate spiral positions for n×n grid\"\"\"\n",
        "    positions = []\n",
        "    x, y = n // 2, n // 2\n",
        "    dx, dy = 0, -1\n",
        "\n",
        "    for _ in range(n * n):\n",
        "        if 0 <= x < n and 0 <= y < n:\n",
        "            positions.append((x, y))\n",
        "\n",
        "        if x == y or (x < 0 and x == -y) or (x > 0 and x == 1 - y):\n",
        "            dx, dy = -dy, dx\n",
        "        x, y = x + dx, y + dy\n",
        "\n",
        "    return positions\n",
        "\n",
        "def encode_payload_multiformat(payload_bytes, format_config):\n",
        "    \"\"\"Encode payload bytes into various image formats\"\"\"\n",
        "    # Convert payload to numpy array of uint8\n",
        "    if isinstance(payload_bytes, (bytes, bytearray)):\n",
        "        payload_bytes = np.frombuffer(payload_bytes, dtype=np.uint8)\n",
        "    else:\n",
        "        payload_bytes = np.array(payload_bytes, dtype=np.uint8)\n",
        "\n",
        "    height, width = format_config['shape']\n",
        "    channels = format_config['channels']\n",
        "    method = format_config['method']\n",
        "\n",
        "    # Ensure payload is correct length\n",
        "    target_pixels = height * width\n",
        "    if len(payload_bytes) < target_pixels:\n",
        "        payload_bytes = np.pad(payload_bytes, (0, target_pixels - len(payload_bytes)), 'constant')\n",
        "    else:\n",
        "        payload_bytes = payload_bytes[:target_pixels]\n",
        "\n",
        "    if method == 'sequential':\n",
        "        # Simple sequential reshape\n",
        "        image = payload_bytes.reshape(height, width)\n",
        "        if channels == 1:\n",
        "            return image.astype(np.float32) / 255.0\n",
        "        else:\n",
        "            # Create RGB by repeating grayscale\n",
        "            image_norm = image.astype(np.float32) / 255.0\n",
        "            return np.stack([image_norm] * channels, axis=-1)\n",
        "\n",
        "    elif method == 'hilbert':\n",
        "        # Hilbert curve mapping\n",
        "        positions = hilbert_curve_positions(min(height, width))\n",
        "        image = np.zeros((height, width, 3), dtype=np.float32)\n",
        "\n",
        "        for i, (x, y) in enumerate(positions[:len(payload_bytes)]):\n",
        "            if x < height and y < width:\n",
        "                val = payload_bytes[i] / 255.0\n",
        "                image[x, y] = [val, val * 0.7, val * 0.5]\n",
        "\n",
        "        return image\n",
        "\n",
        "    elif method == 'spiral':\n",
        "        # Spiral mapping\n",
        "        positions = spiral_positions(min(height, width))\n",
        "        image = np.zeros((height, width, 3), dtype=np.float32)\n",
        "\n",
        "        for i, (x, y) in enumerate(positions[:len(payload_bytes)]):\n",
        "            if 0 <= x < height and 0 <= y < width:\n",
        "                val = payload_bytes[i] / 255.0\n",
        "                image[x, y] = [val * 0.5, val, val * 0.7]\n",
        "\n",
        "        return image\n",
        "\n",
        "    elif method == 'multiview':\n",
        "        # 5-channel representation\n",
        "        image = np.zeros((height, width, 5), dtype=np.float32)\n",
        "\n",
        "        # Channel 1: Raw bytes\n",
        "        image[:, :, 0] = payload_bytes.reshape(height, width) / 255.0\n",
        "\n",
        "        # Channel 2: Header emphasis (first 64 bytes)\n",
        "        header_channel = np.zeros(target_pixels)\n",
        "        header_channel[:64] = payload_bytes[:64] / 255.0\n",
        "        image[:, :, 1] = header_channel.reshape(height, width)\n",
        "\n",
        "        # Channel 3: Byte frequency\n",
        "        byte_freq = np.bincount(payload_bytes.astype(int), minlength=256)\n",
        "        freq_map = byte_freq[payload_bytes] / (np.max(byte_freq) + 1e-10)\n",
        "        image[:, :, 2] = freq_map.reshape(height, width)\n",
        "\n",
        "        # Channel 4: Local entropy\n",
        "        entropy_map = np.zeros(target_pixels)\n",
        "        window = 16\n",
        "        for i in range(0, len(payload_bytes) - window, window):\n",
        "            window_bytes = payload_bytes[i:i+window]\n",
        "            # Simplified entropy calculation\n",
        "            unique, counts = np.unique(window_bytes, return_counts=True)\n",
        "            probs = counts / window\n",
        "            entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
        "            entropy_map[i:i+window] = entropy / 8  # Normalize by max entropy\n",
        "        image[:, :, 3] = entropy_map.reshape(height, width)\n",
        "\n",
        "        # Channel 5: Gradient magnitude\n",
        "        grad = np.abs(np.diff(payload_bytes.astype(float)))\n",
        "        grad_padded = np.pad(grad, (0, 1), 'edge')\n",
        "        image[:, :, 4] = grad_padded.reshape(height, width) / 255.0\n",
        "\n",
        "        return image\n",
        "\n",
        "print(\"✓ Image encoding functions ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpuFjkNvUvoP"
      },
      "source": [
        "## Storage Functions (Parquet and PNG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCIANmKNUvoP",
        "outputId": "0ec6ba67-60a0-4c7c-8147-06bc41d65efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Multi-format writer ready - NOW ORGANIZED BY LABEL (Parquet + PNG only)!\n"
          ]
        }
      ],
      "source": [
        "# Parquet helper functions\n",
        "class MultiFormatDataWriter:\n",
        "    \"\"\"Writes data in Parquet and PNG formats - ORGANIZED BY LABEL\"\"\"\n",
        "\n",
        "    def __init__(self, bucket, base_path, shard_size):\n",
        "        self.bucket = bucket\n",
        "        self.base_path = base_path\n",
        "        self.shard_size = shard_size\n",
        "        # Organize by label/format/split\n",
        "        self.current_shard = defaultdict(list)\n",
        "        self.shard_counts = defaultdict(int)\n",
        "        self.png_counts = defaultdict(int)\n",
        "        self.manifest = {\n",
        "            'parquet': defaultdict(lambda: defaultdict(list)),\n",
        "            'png': defaultdict(list)\n",
        "        }\n",
        "\n",
        "    def add_sample(self, sample, split, format_name):\n",
        "        \"\"\"Add a sample and write to all formats\"\"\"\n",
        "        # Include label in the key\n",
        "        label = sample['label']\n",
        "        key = f\"{label}/{format_name}/{split}\"\n",
        "        self.current_shard[key].append(sample)\n",
        "\n",
        "        # Write PNG immediately (for first N samples per class)\n",
        "        label_key = f\"{format_name}/{split}/{label}\"\n",
        "        if self.png_counts[label_key] < CONFIG['save_sample_pngs']:\n",
        "            self._write_png(sample, split, format_name)\n",
        "            self.png_counts[label_key] += 1\n",
        "\n",
        "        # Write shard if full\n",
        "        if len(self.current_shard[key]) >= self.shard_size:\n",
        "            self._write_shard(key)\n",
        "\n",
        "    def _write_shard(self, key):\n",
        "        \"\"\"Write a shard in Parquet format\"\"\"\n",
        "        if not self.current_shard[key]:\n",
        "            return\n",
        "\n",
        "        # Parse label from key\n",
        "        label, format_name, split = key.split('/')\n",
        "        shard_num = self.shard_counts[key]\n",
        "        samples = self.current_shard[key]\n",
        "\n",
        "        # Write Parquet shard - ORGANIZED BY LABEL\n",
        "        parquet_path = f\"{self.base_path}parquet/{format_name}/{label}/{split}/shard_{shard_num:05d}.parquet\"\n",
        "\n",
        "        # Prepare data for Parquet\n",
        "        data = {\n",
        "            'sample_id': [],\n",
        "            'label': [],\n",
        "            'image_format': [],\n",
        "            'image_data': [],\n",
        "            'height': [],\n",
        "            'width': [],\n",
        "            'channels': [],\n",
        "            'payload_bytes': []\n",
        "        }\n",
        "\n",
        "        for sample in samples:\n",
        "            image = sample['image']\n",
        "            data['sample_id'].append(sample['sample_id'])\n",
        "            data['label'].append(sample['label'])\n",
        "            data['image_format'].append(format_name)\n",
        "            data['image_data'].append(image.flatten().tolist())\n",
        "            data['height'].append(image.shape[0])\n",
        "            data['width'].append(image.shape[1])\n",
        "            data['channels'].append(image.shape[2] if len(image.shape) > 2 else 1)\n",
        "            data['payload_bytes'].append(sample['payload_bytes'].tolist())\n",
        "\n",
        "        # Create table and save\n",
        "        table = pa.table(data)\n",
        "        buffer = io.BytesIO()\n",
        "        pq.write_table(table, buffer)\n",
        "        buffer.seek(0)\n",
        "\n",
        "        blob = self.bucket.blob(parquet_path)\n",
        "        blob.upload_from_file(buffer)\n",
        "\n",
        "        self.manifest['parquet'][label][f\"{format_name}/{split}\"].append({\n",
        "            'shard_num': shard_num,\n",
        "            'path': parquet_path,\n",
        "            'num_samples': len(samples)\n",
        "        })\n",
        "\n",
        "        # Clear shard and increment counter\n",
        "        self.current_shard[key] = []\n",
        "        self.shard_counts[key] += 1\n",
        "\n",
        "        print(f\"   ✓ Wrote shard {shard_num} for {label}/{format_name}/{split}\")\n",
        "\n",
        "    def _write_png(self, sample, split, format_name):\n",
        "        \"\"\"Write individual PNG file\"\"\"\n",
        "        image = sample['image']\n",
        "\n",
        "        # Normalize to 0-255 range\n",
        "        if image.dtype == np.float32 or image.dtype == np.float64:\n",
        "            image_uint8 = (image * 255).astype(np.uint8)\n",
        "        else:\n",
        "            image_uint8 = image.astype(np.uint8)\n",
        "\n",
        "        # Create PIL image\n",
        "        if len(image_uint8.shape) == 2:\n",
        "            pil_image = Image.fromarray(image_uint8, mode='L')\n",
        "        elif image_uint8.shape[2] == 3:\n",
        "            pil_image = Image.fromarray(image_uint8, mode='RGB')\n",
        "        else:\n",
        "            # For 5-channel, save first 3 as RGB\n",
        "            pil_image = Image.fromarray(image_uint8[:, :, :3], mode='RGB')\n",
        "\n",
        "        # Save to buffer\n",
        "        buffer = io.BytesIO()\n",
        "        pil_image.save(buffer, format='PNG')\n",
        "        buffer.seek(0)\n",
        "\n",
        "        # Upload to GCS - ORGANIZED BY LABEL\n",
        "        path = f\"{self.base_path}png/{format_name}/{label}/{split}/{sample['sample_id']}.png\"\n",
        "        blob = self.bucket.blob(path)\n",
        "        blob.upload_from_file(buffer, content_type='image/png')\n",
        "\n",
        "    def finalize(self):\n",
        "        \"\"\"Write remaining shards and save manifests\"\"\"\n",
        "        # Write remaining shards\n",
        "        for key in list(self.current_shard.keys()):\n",
        "            if self.current_shard[key]:\n",
        "                self._write_shard(key)\n",
        "\n",
        "        # Save combined manifest\n",
        "        manifest_data = {\n",
        "            'timestamp': CONFIG['timestamp'],\n",
        "            'shard_size': self.shard_size,\n",
        "            'formats': {\n",
        "                'parquet': dict(self.manifest['parquet']),\n",
        "                'png': dict(self.png_counts)\n",
        "            },\n",
        "            'total_shards': dict(self.shard_counts),\n",
        "            'image_formats': IMAGE_FORMATS,\n",
        "            'labels': list(self.manifest['parquet'].keys())  # List all labels processed\n",
        "        }\n",
        "\n",
        "        manifest_blob = self.bucket.blob(f\"{self.base_path}manifest.json\")\n",
        "        manifest_blob.upload_from_string(\n",
        "            json.dumps(manifest_data, indent=2),\n",
        "            content_type='application/json'\n",
        "        )\n",
        "\n",
        "        return manifest_data\n",
        "\n",
        "print(\"✓ Multi-format writer ready - NOW ORGANIZED BY LABEL (Parquet + PNG only)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwbOL8MGUvoQ"
      },
      "source": [
        "## Process PCAP Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoR6UHN2UvoQ",
        "outputId": "d58ebd79-67e1-4e5f-c4ec-2ab87a204eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Discovering PCAP files...\n",
            "\n",
            "📊 Found 77 PCAP files across 9 labels\n",
            "Labels: ['Benign_Final', 'DDoS-HTTP_Flood', 'DDoS-SYN_Flood', 'DictionaryBruteForce', 'DoS-TCP_Flood', 'DoS-UDP_Flood', 'Mirai-udpplain', 'Recon-PortScan', 'SqlInjection']\n",
            "\n",
            "📁 Files per label:\n",
            "   Benign_Final: 4 files, 6673.7 MB total\n",
            "   DDoS-HTTP_Flood: 1 files, 582.6 MB total\n",
            "   DDoS-SYN_Flood: 16 files, 30307.2 MB total\n",
            "   DictionaryBruteForce: 1 files, 37.3 MB total\n",
            "   DoS-TCP_Flood: 11 files, 20143.2 MB total\n",
            "   DoS-UDP_Flood: 17 files, 32592.4 MB total\n",
            "   Mirai-udpplain: 25 files, 47676.0 MB total\n",
            "   Recon-PortScan: 1 files, 191.6 MB total\n",
            "   SqlInjection: 1 files, 9.4 MB total\n"
          ]
        }
      ],
      "source": [
        "# Initialize GCS\n",
        "client = storage.Client()\n",
        "bucket = client.bucket(CONFIG['bucket_name'])\n",
        "\n",
        "# List all PCAP files\n",
        "print(\"🔍 Discovering PCAP files...\")\n",
        "pcap_files = []\n",
        "labels = set()\n",
        "\n",
        "# List all blobs in the PCAP directory\n",
        "all_blobs = list(bucket.list_blobs(prefix=CONFIG['input_prefix']))\n",
        "\n",
        "# Extract PCAP files and labels from the blob paths\n",
        "for blob in all_blobs:\n",
        "    if blob.name.endswith('.pcap'):\n",
        "        # Extract label from path\n",
        "        path_parts = blob.name.split('/')\n",
        "        if len(path_parts) >= 2:\n",
        "            label = path_parts[-2]  # Folder name before the filename\n",
        "            labels.add(label)\n",
        "\n",
        "            pcap_files.append({\n",
        "                'path': blob.name,\n",
        "                'label': label,\n",
        "                'size_mb': blob.size / (1024 * 1024)\n",
        "            })\n",
        "\n",
        "print(f\"\\n📊 Found {len(pcap_files)} PCAP files across {len(labels)} labels\")\n",
        "print(f\"Labels: {sorted(labels)}\")\n",
        "\n",
        "# Group files by label\n",
        "files_by_label = defaultdict(list)\n",
        "for file_info in pcap_files:\n",
        "    files_by_label[file_info['label']].append(file_info)\n",
        "\n",
        "print(\"\\n📁 Files per label:\")\n",
        "for label in sorted(files_by_label.keys()):\n",
        "    total_size = sum(f['size_mb'] for f in files_by_label[label])\n",
        "    print(f\"   {label}: {len(files_by_label[label])} files, {total_size:.1f} MB total\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh-x9gJSUvoQ",
        "outputId": "fd9d7e85-1146-4e29-ce90-35265e6499d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Processing PCAP files and creating dataset...\n",
            "Target: 12000 samples per label\n",
            "\n",
            "\n",
            "📦 Processing label: Benign_Final\n",
            "   Reading BenignTraffic.pcap (1953.1 MB)...\n",
            "   Extracted 100000 packets\n",
            "   Progress: 1,000 total samples (4 samples/sec)\n",
            "   ✓ Wrote shard 0 for Benign_Final/grayscale_32x32/train\n",
            "   ✓ Wrote shard 0 for Benign_Final/grayscale_39x39/train\n",
            "   ✓ Wrote shard 0 for Benign_Final/grayscale_64x64/train\n",
            "   ✓ Wrote shard 0 for Benign_Final/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 0 for Benign_Final/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 0 for Benign_Final/5channel_32x32/train\n",
            "   Progress: 2,000 total samples (7 samples/sec)\n",
            "   ✓ Wrote shard 1 for Benign_Final/grayscale_32x32/train\n",
            "   ✓ Wrote shard 1 for Benign_Final/grayscale_39x39/train\n",
            "   ✓ Wrote shard 1 for Benign_Final/grayscale_64x64/train\n",
            "   ✓ Wrote shard 1 for Benign_Final/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 1 for Benign_Final/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 1 for Benign_Final/5channel_32x32/train\n",
            "   Progress: 3,000 total samples (10 samples/sec)\n",
            "   Progress: 4,000 total samples (13 samples/sec)\n",
            "   ✓ Wrote shard 2 for Benign_Final/grayscale_32x32/train\n",
            "   ✓ Wrote shard 2 for Benign_Final/grayscale_39x39/train\n",
            "   ✓ Wrote shard 2 for Benign_Final/grayscale_64x64/train\n",
            "   ✓ Wrote shard 2 for Benign_Final/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 2 for Benign_Final/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 2 for Benign_Final/5channel_32x32/train\n",
            "   Progress: 5,000 total samples (16 samples/sec)\n",
            "   ✓ Wrote shard 3 for Benign_Final/grayscale_32x32/train\n",
            "   ✓ Wrote shard 3 for Benign_Final/grayscale_39x39/train\n",
            "   ✓ Wrote shard 3 for Benign_Final/grayscale_64x64/train\n",
            "   ✓ Wrote shard 3 for Benign_Final/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 3 for Benign_Final/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 3 for Benign_Final/5channel_32x32/train\n",
            "   Progress: 6,000 total samples (18 samples/sec)\n",
            "   ✓ Wrote shard 0 for Benign_Final/grayscale_32x32/test\n",
            "   ✓ Wrote shard 0 for Benign_Final/grayscale_39x39/test\n",
            "   ✓ Wrote shard 0 for Benign_Final/grayscale_64x64/test\n",
            "   ✓ Wrote shard 0 for Benign_Final/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 0 for Benign_Final/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 0 for Benign_Final/5channel_32x32/test\n",
            "   ✓ Wrote shard 0 for Benign_Final/grayscale_32x32/val\n",
            "   ✓ Wrote shard 0 for Benign_Final/grayscale_39x39/val\n",
            "   ✓ Wrote shard 0 for Benign_Final/grayscale_64x64/val\n",
            "   ✓ Wrote shard 0 for Benign_Final/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 0 for Benign_Final/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 0 for Benign_Final/5channel_32x32/val\n",
            "   Progress: 7,000 total samples (21 samples/sec)\n",
            "   ✓ Wrote shard 4 for Benign_Final/grayscale_32x32/train\n",
            "   ✓ Wrote shard 4 for Benign_Final/grayscale_39x39/train\n",
            "   ✓ Wrote shard 4 for Benign_Final/grayscale_64x64/train\n",
            "   ✓ Wrote shard 4 for Benign_Final/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 4 for Benign_Final/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 4 for Benign_Final/5channel_32x32/train\n",
            "   Progress: 8,000 total samples (23 samples/sec)\n",
            "   ✓ Wrote shard 5 for Benign_Final/grayscale_32x32/train\n",
            "   ✓ Wrote shard 5 for Benign_Final/grayscale_39x39/train\n",
            "   ✓ Wrote shard 5 for Benign_Final/grayscale_64x64/train\n",
            "   ✓ Wrote shard 5 for Benign_Final/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 5 for Benign_Final/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 5 for Benign_Final/5channel_32x32/train\n",
            "   Progress: 9,000 total samples (25 samples/sec)\n",
            "   Progress: 10,000 total samples (27 samples/sec)\n",
            "   ✓ Wrote shard 6 for Benign_Final/grayscale_32x32/train\n",
            "   ✓ Wrote shard 6 for Benign_Final/grayscale_39x39/train\n",
            "   ✓ Wrote shard 6 for Benign_Final/grayscale_64x64/train\n",
            "   ✓ Wrote shard 6 for Benign_Final/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 6 for Benign_Final/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 6 for Benign_Final/5channel_32x32/train\n",
            "   Progress: 11,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 7 for Benign_Final/grayscale_32x32/train\n",
            "   ✓ Wrote shard 7 for Benign_Final/grayscale_39x39/train\n",
            "   ✓ Wrote shard 7 for Benign_Final/grayscale_64x64/train\n",
            "   ✓ Wrote shard 7 for Benign_Final/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 7 for Benign_Final/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 7 for Benign_Final/5channel_32x32/train\n",
            "   Progress: 12,000 total samples (31 samples/sec)\n",
            "   ✓ Collected 12000 samples for Benign_Final\n",
            "\n",
            "📦 Processing label: DDoS-HTTP_Flood\n",
            "   Reading DDoS-HTTP_Flood-.pcap (582.6 MB)...\n",
            "   Extracted 100000 packets\n",
            "   Progress: 13,000 total samples (20 samples/sec)\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/5channel_32x32/train\n",
            "   Progress: 14,000 total samples (21 samples/sec)\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/5channel_32x32/train\n",
            "   Progress: 15,000 total samples (22 samples/sec)\n",
            "   Progress: 16,000 total samples (23 samples/sec)\n",
            "   ✓ Wrote shard 2 for DDoS-HTTP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 2 for DDoS-HTTP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 2 for DDoS-HTTP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 2 for DDoS-HTTP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 2 for DDoS-HTTP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 2 for DDoS-HTTP_Flood/5channel_32x32/train\n",
            "   Progress: 17,000 total samples (24 samples/sec)\n",
            "   ✓ Wrote shard 3 for DDoS-HTTP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 3 for DDoS-HTTP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 3 for DDoS-HTTP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 3 for DDoS-HTTP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 3 for DDoS-HTTP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 3 for DDoS-HTTP_Flood/5channel_32x32/train\n",
            "   Progress: 18,000 total samples (26 samples/sec)\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/grayscale_32x32/val\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/grayscale_39x39/val\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/grayscale_64x64/val\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/5channel_32x32/val\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/grayscale_32x32/test\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/grayscale_39x39/test\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/grayscale_64x64/test\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 0 for DDoS-HTTP_Flood/5channel_32x32/test\n",
            "   Progress: 19,000 total samples (26 samples/sec)\n",
            "   ✓ Wrote shard 4 for DDoS-HTTP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 4 for DDoS-HTTP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 4 for DDoS-HTTP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 4 for DDoS-HTTP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 4 for DDoS-HTTP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 4 for DDoS-HTTP_Flood/5channel_32x32/train\n",
            "   Progress: 20,000 total samples (27 samples/sec)\n",
            "   ✓ Wrote shard 5 for DDoS-HTTP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 5 for DDoS-HTTP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 5 for DDoS-HTTP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 5 for DDoS-HTTP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 5 for DDoS-HTTP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 5 for DDoS-HTTP_Flood/5channel_32x32/train\n",
            "   Progress: 21,000 total samples (28 samples/sec)\n",
            "   ✓ Wrote shard 6 for DDoS-HTTP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 6 for DDoS-HTTP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 6 for DDoS-HTTP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 6 for DDoS-HTTP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 6 for DDoS-HTTP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 6 for DDoS-HTTP_Flood/5channel_32x32/train\n",
            "   Progress: 22,000 total samples (29 samples/sec)\n",
            "   Progress: 23,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 7 for DDoS-HTTP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 7 for DDoS-HTTP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 7 for DDoS-HTTP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 7 for DDoS-HTTP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 7 for DDoS-HTTP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 7 for DDoS-HTTP_Flood/5channel_32x32/train\n",
            "   Progress: 24,000 total samples (31 samples/sec)\n",
            "   ✓ Collected 12000 samples for DDoS-HTTP_Flood\n",
            "\n",
            "📦 Processing label: DDoS-SYN_Flood\n",
            "   Reading DDoS-SYN_Flood.pcap (1953.1 MB)...\n",
            "   Extracted 100000 packets\n",
            "   Progress: 25,000 total samples (24 samples/sec)\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/5channel_32x32/train\n",
            "   Progress: 26,000 total samples (25 samples/sec)\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/5channel_32x32/train\n",
            "   Progress: 27,000 total samples (26 samples/sec)\n",
            "   Progress: 28,000 total samples (26 samples/sec)\n",
            "   ✓ Wrote shard 2 for DDoS-SYN_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 2 for DDoS-SYN_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 2 for DDoS-SYN_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 2 for DDoS-SYN_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 2 for DDoS-SYN_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 2 for DDoS-SYN_Flood/5channel_32x32/train\n",
            "   Progress: 29,000 total samples (27 samples/sec)\n",
            "   ✓ Wrote shard 3 for DDoS-SYN_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 3 for DDoS-SYN_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 3 for DDoS-SYN_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 3 for DDoS-SYN_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 3 for DDoS-SYN_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 3 for DDoS-SYN_Flood/5channel_32x32/train\n",
            "   Progress: 30,000 total samples (28 samples/sec)\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/grayscale_32x32/test\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/grayscale_39x39/test\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/grayscale_64x64/test\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/5channel_32x32/test\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/grayscale_32x32/val\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/grayscale_39x39/val\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/grayscale_64x64/val\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 0 for DDoS-SYN_Flood/5channel_32x32/val\n",
            "   Progress: 31,000 total samples (28 samples/sec)\n",
            "   ✓ Wrote shard 4 for DDoS-SYN_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 4 for DDoS-SYN_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 4 for DDoS-SYN_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 4 for DDoS-SYN_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 4 for DDoS-SYN_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 4 for DDoS-SYN_Flood/5channel_32x32/train\n",
            "   Progress: 32,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 5 for DDoS-SYN_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 5 for DDoS-SYN_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 5 for DDoS-SYN_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 5 for DDoS-SYN_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 5 for DDoS-SYN_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 5 for DDoS-SYN_Flood/5channel_32x32/train\n",
            "   Progress: 33,000 total samples (29 samples/sec)\n",
            "   Progress: 34,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 6 for DDoS-SYN_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 6 for DDoS-SYN_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 6 for DDoS-SYN_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 6 for DDoS-SYN_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 6 for DDoS-SYN_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 6 for DDoS-SYN_Flood/5channel_32x32/train\n",
            "   Progress: 35,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 7 for DDoS-SYN_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 7 for DDoS-SYN_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 7 for DDoS-SYN_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 7 for DDoS-SYN_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 7 for DDoS-SYN_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 7 for DDoS-SYN_Flood/5channel_32x32/train\n",
            "   Progress: 36,000 total samples (31 samples/sec)\n",
            "   ✓ Collected 12000 samples for DDoS-SYN_Flood\n",
            "\n",
            "📦 Processing label: DictionaryBruteForce\n",
            "   Reading DictionaryBruteForce.pcap (37.3 MB)...\n",
            "   Extracted 100000 packets\n",
            "   Progress: 37,000 total samples (26 samples/sec)\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/grayscale_32x32/train\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/grayscale_39x39/train\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/grayscale_64x64/train\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/5channel_32x32/train\n",
            "   Progress: 38,000 total samples (27 samples/sec)\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/grayscale_32x32/train\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/grayscale_39x39/train\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/grayscale_64x64/train\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/5channel_32x32/train\n",
            "   Progress: 39,000 total samples (27 samples/sec)\n",
            "   Progress: 40,000 total samples (28 samples/sec)\n",
            "   ✓ Wrote shard 2 for DictionaryBruteForce/grayscale_32x32/train\n",
            "   ✓ Wrote shard 2 for DictionaryBruteForce/grayscale_39x39/train\n",
            "   ✓ Wrote shard 2 for DictionaryBruteForce/grayscale_64x64/train\n",
            "   ✓ Wrote shard 2 for DictionaryBruteForce/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 2 for DictionaryBruteForce/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 2 for DictionaryBruteForce/5channel_32x32/train\n",
            "   Progress: 41,000 total samples (28 samples/sec)\n",
            "   ✓ Wrote shard 3 for DictionaryBruteForce/grayscale_32x32/train\n",
            "   ✓ Wrote shard 3 for DictionaryBruteForce/grayscale_39x39/train\n",
            "   ✓ Wrote shard 3 for DictionaryBruteForce/grayscale_64x64/train\n",
            "   ✓ Wrote shard 3 for DictionaryBruteForce/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 3 for DictionaryBruteForce/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 3 for DictionaryBruteForce/5channel_32x32/train\n",
            "   Progress: 42,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/grayscale_32x32/test\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/grayscale_39x39/test\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/grayscale_64x64/test\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/5channel_32x32/test\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/grayscale_32x32/val\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/grayscale_39x39/val\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/grayscale_64x64/val\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 0 for DictionaryBruteForce/5channel_32x32/val\n",
            "   Progress: 43,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 4 for DictionaryBruteForce/grayscale_32x32/train\n",
            "   ✓ Wrote shard 4 for DictionaryBruteForce/grayscale_39x39/train\n",
            "   ✓ Wrote shard 4 for DictionaryBruteForce/grayscale_64x64/train\n",
            "   ✓ Wrote shard 4 for DictionaryBruteForce/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 4 for DictionaryBruteForce/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 4 for DictionaryBruteForce/5channel_32x32/train\n",
            "   Progress: 44,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 5 for DictionaryBruteForce/grayscale_32x32/train\n",
            "   ✓ Wrote shard 5 for DictionaryBruteForce/grayscale_39x39/train\n",
            "   ✓ Wrote shard 5 for DictionaryBruteForce/grayscale_64x64/train\n",
            "   ✓ Wrote shard 5 for DictionaryBruteForce/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 5 for DictionaryBruteForce/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 5 for DictionaryBruteForce/5channel_32x32/train\n",
            "   Progress: 45,000 total samples (30 samples/sec)\n",
            "   Progress: 46,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 6 for DictionaryBruteForce/grayscale_32x32/train\n",
            "   ✓ Wrote shard 6 for DictionaryBruteForce/grayscale_39x39/train\n",
            "   ✓ Wrote shard 6 for DictionaryBruteForce/grayscale_64x64/train\n",
            "   ✓ Wrote shard 6 for DictionaryBruteForce/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 6 for DictionaryBruteForce/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 6 for DictionaryBruteForce/5channel_32x32/train\n",
            "   Progress: 47,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 7 for DictionaryBruteForce/grayscale_32x32/train\n",
            "   ✓ Wrote shard 7 for DictionaryBruteForce/grayscale_39x39/train\n",
            "   ✓ Wrote shard 7 for DictionaryBruteForce/grayscale_64x64/train\n",
            "   ✓ Wrote shard 7 for DictionaryBruteForce/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 7 for DictionaryBruteForce/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 7 for DictionaryBruteForce/5channel_32x32/train\n",
            "   Progress: 48,000 total samples (32 samples/sec)\n",
            "   ✓ Collected 12000 samples for DictionaryBruteForce\n",
            "\n",
            "📦 Processing label: DoS-TCP_Flood\n",
            "   Reading DoS-TCP_Flood.pcap (1953.1 MB)...\n",
            "   Extracted 100000 packets\n",
            "   Progress: 49,000 total samples (27 samples/sec)\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/5channel_32x32/train\n",
            "   Progress: 50,000 total samples (28 samples/sec)\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/5channel_32x32/train\n",
            "   Progress: 51,000 total samples (28 samples/sec)\n",
            "   Progress: 52,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 2 for DoS-TCP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 2 for DoS-TCP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 2 for DoS-TCP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 2 for DoS-TCP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 2 for DoS-TCP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 2 for DoS-TCP_Flood/5channel_32x32/train\n",
            "   Progress: 53,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 3 for DoS-TCP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 3 for DoS-TCP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 3 for DoS-TCP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 3 for DoS-TCP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 3 for DoS-TCP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 3 for DoS-TCP_Flood/5channel_32x32/train\n",
            "   Progress: 54,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/grayscale_32x32/val\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/grayscale_39x39/val\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/grayscale_64x64/val\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/5channel_32x32/val\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/grayscale_32x32/test\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/grayscale_39x39/test\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/grayscale_64x64/test\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 0 for DoS-TCP_Flood/5channel_32x32/test\n",
            "   Progress: 55,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 4 for DoS-TCP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 4 for DoS-TCP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 4 for DoS-TCP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 4 for DoS-TCP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 4 for DoS-TCP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 4 for DoS-TCP_Flood/5channel_32x32/train\n",
            "   Progress: 56,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 5 for DoS-TCP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 5 for DoS-TCP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 5 for DoS-TCP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 5 for DoS-TCP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 5 for DoS-TCP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 5 for DoS-TCP_Flood/5channel_32x32/train\n",
            "   Progress: 57,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 6 for DoS-TCP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 6 for DoS-TCP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 6 for DoS-TCP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 6 for DoS-TCP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 6 for DoS-TCP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 6 for DoS-TCP_Flood/5channel_32x32/train\n",
            "   Progress: 58,000 total samples (31 samples/sec)\n",
            "   Progress: 59,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 7 for DoS-TCP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 7 for DoS-TCP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 7 for DoS-TCP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 7 for DoS-TCP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 7 for DoS-TCP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 7 for DoS-TCP_Flood/5channel_32x32/train\n",
            "   Progress: 60,000 total samples (32 samples/sec)\n",
            "   ✓ Collected 12000 samples for DoS-TCP_Flood\n",
            "\n",
            "📦 Processing label: DoS-UDP_Flood\n",
            "   Reading DoS-UDP_Flood.pcap (1953.1 MB)...\n",
            "   Extracted 100000 packets\n",
            "   Progress: 61,000 total samples (28 samples/sec)\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/5channel_32x32/train\n",
            "   Progress: 62,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/5channel_32x32/train\n",
            "   Progress: 63,000 total samples (29 samples/sec)\n",
            "   Progress: 64,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 2 for DoS-UDP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 2 for DoS-UDP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 2 for DoS-UDP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 2 for DoS-UDP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 2 for DoS-UDP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 2 for DoS-UDP_Flood/5channel_32x32/train\n",
            "   Progress: 65,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 3 for DoS-UDP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 3 for DoS-UDP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 3 for DoS-UDP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 3 for DoS-UDP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 3 for DoS-UDP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 3 for DoS-UDP_Flood/5channel_32x32/train\n",
            "   Progress: 66,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/grayscale_32x32/val\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/grayscale_39x39/val\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/grayscale_64x64/val\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/5channel_32x32/val\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/grayscale_32x32/test\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/grayscale_39x39/test\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/grayscale_64x64/test\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 0 for DoS-UDP_Flood/5channel_32x32/test\n",
            "   Progress: 67,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 4 for DoS-UDP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 4 for DoS-UDP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 4 for DoS-UDP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 4 for DoS-UDP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 4 for DoS-UDP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 4 for DoS-UDP_Flood/5channel_32x32/train\n",
            "   Progress: 68,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 5 for DoS-UDP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 5 for DoS-UDP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 5 for DoS-UDP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 5 for DoS-UDP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 5 for DoS-UDP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 5 for DoS-UDP_Flood/5channel_32x32/train\n",
            "   Progress: 69,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 6 for DoS-UDP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 6 for DoS-UDP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 6 for DoS-UDP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 6 for DoS-UDP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 6 for DoS-UDP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 6 for DoS-UDP_Flood/5channel_32x32/train\n",
            "   Progress: 70,000 total samples (31 samples/sec)\n",
            "   Progress: 71,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 7 for DoS-UDP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 7 for DoS-UDP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 7 for DoS-UDP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 7 for DoS-UDP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 7 for DoS-UDP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 7 for DoS-UDP_Flood/5channel_32x32/train\n",
            "   Progress: 72,000 total samples (32 samples/sec)\n",
            "   ✓ Collected 12000 samples for DoS-UDP_Flood\n",
            "\n",
            "📦 Processing label: Mirai-udpplain\n",
            "   Reading Mirai-udpplain.pcap (1953.1 MB)...\n",
            "   Extracted 100000 packets\n",
            "   Progress: 73,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/grayscale_32x32/train\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/grayscale_39x39/train\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/grayscale_64x64/train\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/5channel_32x32/train\n",
            "   Progress: 74,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/grayscale_32x32/train\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/grayscale_39x39/train\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/grayscale_64x64/train\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/5channel_32x32/train\n",
            "   Progress: 75,000 total samples (29 samples/sec)\n",
            "   Progress: 76,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 2 for Mirai-udpplain/grayscale_32x32/train\n",
            "   ✓ Wrote shard 2 for Mirai-udpplain/grayscale_39x39/train\n",
            "   ✓ Wrote shard 2 for Mirai-udpplain/grayscale_64x64/train\n",
            "   ✓ Wrote shard 2 for Mirai-udpplain/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 2 for Mirai-udpplain/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 2 for Mirai-udpplain/5channel_32x32/train\n",
            "   Progress: 77,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 3 for Mirai-udpplain/grayscale_32x32/train\n",
            "   ✓ Wrote shard 3 for Mirai-udpplain/grayscale_39x39/train\n",
            "   ✓ Wrote shard 3 for Mirai-udpplain/grayscale_64x64/train\n",
            "   ✓ Wrote shard 3 for Mirai-udpplain/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 3 for Mirai-udpplain/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 3 for Mirai-udpplain/5channel_32x32/train\n",
            "   Progress: 78,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/grayscale_32x32/val\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/grayscale_39x39/val\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/grayscale_64x64/val\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/5channel_32x32/val\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/grayscale_32x32/test\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/grayscale_39x39/test\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/grayscale_64x64/test\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 0 for Mirai-udpplain/5channel_32x32/test\n",
            "   Progress: 79,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 4 for Mirai-udpplain/grayscale_32x32/train\n",
            "   ✓ Wrote shard 4 for Mirai-udpplain/grayscale_39x39/train\n",
            "   ✓ Wrote shard 4 for Mirai-udpplain/grayscale_64x64/train\n",
            "   ✓ Wrote shard 4 for Mirai-udpplain/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 4 for Mirai-udpplain/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 4 for Mirai-udpplain/5channel_32x32/train\n",
            "   Progress: 80,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 5 for Mirai-udpplain/grayscale_32x32/train\n",
            "   ✓ Wrote shard 5 for Mirai-udpplain/grayscale_39x39/train\n",
            "   ✓ Wrote shard 5 for Mirai-udpplain/grayscale_64x64/train\n",
            "   ✓ Wrote shard 5 for Mirai-udpplain/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 5 for Mirai-udpplain/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 5 for Mirai-udpplain/5channel_32x32/train\n",
            "   Progress: 81,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 6 for Mirai-udpplain/grayscale_32x32/train\n",
            "   ✓ Wrote shard 6 for Mirai-udpplain/grayscale_39x39/train\n",
            "   ✓ Wrote shard 6 for Mirai-udpplain/grayscale_64x64/train\n",
            "   ✓ Wrote shard 6 for Mirai-udpplain/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 6 for Mirai-udpplain/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 6 for Mirai-udpplain/5channel_32x32/train\n",
            "   Progress: 82,000 total samples (31 samples/sec)\n",
            "   Progress: 83,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 7 for Mirai-udpplain/grayscale_32x32/train\n",
            "   ✓ Wrote shard 7 for Mirai-udpplain/grayscale_39x39/train\n",
            "   ✓ Wrote shard 7 for Mirai-udpplain/grayscale_64x64/train\n",
            "   ✓ Wrote shard 7 for Mirai-udpplain/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 7 for Mirai-udpplain/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 7 for Mirai-udpplain/5channel_32x32/train\n",
            "   Progress: 84,000 total samples (32 samples/sec)\n",
            "   ✓ Collected 12000 samples for Mirai-udpplain\n",
            "\n",
            "📦 Processing label: Recon-PortScan\n",
            "   Reading Recon-PortScan.pcap (191.6 MB)...\n",
            "   Extracted 100000 packets\n",
            "   Progress: 85,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/grayscale_32x32/train\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/grayscale_39x39/train\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/grayscale_64x64/train\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/5channel_32x32/train\n",
            "   Progress: 86,000 total samples (29 samples/sec)\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/grayscale_32x32/train\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/grayscale_39x39/train\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/grayscale_64x64/train\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/5channel_32x32/train\n",
            "   Progress: 87,000 total samples (30 samples/sec)\n",
            "   Progress: 88,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 2 for Recon-PortScan/grayscale_32x32/train\n",
            "   ✓ Wrote shard 2 for Recon-PortScan/grayscale_39x39/train\n",
            "   ✓ Wrote shard 2 for Recon-PortScan/grayscale_64x64/train\n",
            "   ✓ Wrote shard 2 for Recon-PortScan/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 2 for Recon-PortScan/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 2 for Recon-PortScan/5channel_32x32/train\n",
            "   Progress: 89,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 3 for Recon-PortScan/grayscale_32x32/train\n",
            "   ✓ Wrote shard 3 for Recon-PortScan/grayscale_39x39/train\n",
            "   ✓ Wrote shard 3 for Recon-PortScan/grayscale_64x64/train\n",
            "   ✓ Wrote shard 3 for Recon-PortScan/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 3 for Recon-PortScan/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 3 for Recon-PortScan/5channel_32x32/train\n",
            "   Progress: 90,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/grayscale_32x32/test\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/grayscale_39x39/test\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/grayscale_64x64/test\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/5channel_32x32/test\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/grayscale_32x32/val\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/grayscale_39x39/val\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/grayscale_64x64/val\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 0 for Recon-PortScan/5channel_32x32/val\n",
            "   Progress: 91,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 4 for Recon-PortScan/grayscale_32x32/train\n",
            "   ✓ Wrote shard 4 for Recon-PortScan/grayscale_39x39/train\n",
            "   ✓ Wrote shard 4 for Recon-PortScan/grayscale_64x64/train\n",
            "   ✓ Wrote shard 4 for Recon-PortScan/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 4 for Recon-PortScan/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 4 for Recon-PortScan/5channel_32x32/train\n",
            "   Progress: 92,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 5 for Recon-PortScan/grayscale_32x32/train\n",
            "   ✓ Wrote shard 5 for Recon-PortScan/grayscale_39x39/train\n",
            "   ✓ Wrote shard 5 for Recon-PortScan/grayscale_64x64/train\n",
            "   ✓ Wrote shard 5 for Recon-PortScan/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 5 for Recon-PortScan/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 5 for Recon-PortScan/5channel_32x32/train\n",
            "   Progress: 93,000 total samples (31 samples/sec)\n",
            "   Progress: 94,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 6 for Recon-PortScan/grayscale_32x32/train\n",
            "   ✓ Wrote shard 6 for Recon-PortScan/grayscale_39x39/train\n",
            "   ✓ Wrote shard 6 for Recon-PortScan/grayscale_64x64/train\n",
            "   ✓ Wrote shard 6 for Recon-PortScan/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 6 for Recon-PortScan/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 6 for Recon-PortScan/5channel_32x32/train\n",
            "   Progress: 95,000 total samples (32 samples/sec)\n",
            "   ✓ Wrote shard 7 for Recon-PortScan/grayscale_32x32/train\n",
            "   ✓ Wrote shard 7 for Recon-PortScan/grayscale_39x39/train\n",
            "   ✓ Wrote shard 7 for Recon-PortScan/grayscale_64x64/train\n",
            "   ✓ Wrote shard 7 for Recon-PortScan/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 7 for Recon-PortScan/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 7 for Recon-PortScan/5channel_32x32/train\n",
            "   Progress: 96,000 total samples (32 samples/sec)\n",
            "   ✓ Collected 12000 samples for Recon-PortScan\n",
            "\n",
            "📦 Processing label: SqlInjection\n",
            "   Reading SqlInjection.pcap (9.4 MB)...\n",
            "   Extracted 53462 packets\n",
            "   Progress: 97,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 0 for SqlInjection/grayscale_32x32/train\n",
            "   ✓ Wrote shard 0 for SqlInjection/grayscale_39x39/train\n",
            "   ✓ Wrote shard 0 for SqlInjection/grayscale_64x64/train\n",
            "   ✓ Wrote shard 0 for SqlInjection/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 0 for SqlInjection/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 0 for SqlInjection/5channel_32x32/train\n",
            "   Progress: 98,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 1 for SqlInjection/grayscale_32x32/train\n",
            "   ✓ Wrote shard 1 for SqlInjection/grayscale_39x39/train\n",
            "   ✓ Wrote shard 1 for SqlInjection/grayscale_64x64/train\n",
            "   ✓ Wrote shard 1 for SqlInjection/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 1 for SqlInjection/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 1 for SqlInjection/5channel_32x32/train\n",
            "   Progress: 99,000 total samples (30 samples/sec)\n",
            "   Progress: 100,000 total samples (30 samples/sec)\n",
            "   ✓ Wrote shard 2 for SqlInjection/grayscale_32x32/train\n",
            "   ✓ Wrote shard 2 for SqlInjection/grayscale_39x39/train\n",
            "   ✓ Wrote shard 2 for SqlInjection/grayscale_64x64/train\n",
            "   ✓ Wrote shard 2 for SqlInjection/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 2 for SqlInjection/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 2 for SqlInjection/5channel_32x32/train\n",
            "   Progress: 101,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 3 for SqlInjection/grayscale_32x32/train\n",
            "   ✓ Wrote shard 3 for SqlInjection/grayscale_39x39/train\n",
            "   ✓ Wrote shard 3 for SqlInjection/grayscale_64x64/train\n",
            "   ✓ Wrote shard 3 for SqlInjection/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 3 for SqlInjection/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 3 for SqlInjection/5channel_32x32/train\n",
            "   Progress: 102,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 0 for SqlInjection/grayscale_32x32/test\n",
            "   ✓ Wrote shard 0 for SqlInjection/grayscale_39x39/test\n",
            "   ✓ Wrote shard 0 for SqlInjection/grayscale_64x64/test\n",
            "   ✓ Wrote shard 0 for SqlInjection/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 0 for SqlInjection/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 0 for SqlInjection/5channel_32x32/test\n",
            "   ✓ Wrote shard 0 for SqlInjection/grayscale_32x32/val\n",
            "   ✓ Wrote shard 0 for SqlInjection/grayscale_39x39/val\n",
            "   ✓ Wrote shard 0 for SqlInjection/grayscale_64x64/val\n",
            "   ✓ Wrote shard 0 for SqlInjection/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 0 for SqlInjection/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 0 for SqlInjection/5channel_32x32/val\n",
            "   Progress: 103,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 4 for SqlInjection/grayscale_32x32/train\n",
            "   ✓ Wrote shard 4 for SqlInjection/grayscale_39x39/train\n",
            "   ✓ Wrote shard 4 for SqlInjection/grayscale_64x64/train\n",
            "   ✓ Wrote shard 4 for SqlInjection/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 4 for SqlInjection/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 4 for SqlInjection/5channel_32x32/train\n",
            "   Progress: 104,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 5 for SqlInjection/grayscale_32x32/train\n",
            "   ✓ Wrote shard 5 for SqlInjection/grayscale_39x39/train\n",
            "   ✓ Wrote shard 5 for SqlInjection/grayscale_64x64/train\n",
            "   ✓ Wrote shard 5 for SqlInjection/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 5 for SqlInjection/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 5 for SqlInjection/5channel_32x32/train\n",
            "   Progress: 105,000 total samples (31 samples/sec)\n",
            "   ✓ Wrote shard 6 for SqlInjection/grayscale_32x32/train\n",
            "   ✓ Wrote shard 6 for SqlInjection/grayscale_39x39/train\n",
            "   ✓ Wrote shard 6 for SqlInjection/grayscale_64x64/train\n",
            "   ✓ Wrote shard 6 for SqlInjection/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 6 for SqlInjection/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 6 for SqlInjection/5channel_32x32/train\n",
            "   Progress: 106,000 total samples (31 samples/sec)\n",
            "   Progress: 107,000 total samples (32 samples/sec)\n",
            "   ✓ Wrote shard 7 for SqlInjection/grayscale_32x32/train\n",
            "   ✓ Wrote shard 7 for SqlInjection/grayscale_39x39/train\n",
            "   ✓ Wrote shard 7 for SqlInjection/grayscale_64x64/train\n",
            "   ✓ Wrote shard 7 for SqlInjection/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 7 for SqlInjection/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 7 for SqlInjection/5channel_32x32/train\n",
            "   Progress: 108,000 total samples (32 samples/sec)\n",
            "   ✓ Collected 12000 samples for SqlInjection\n",
            "\n",
            "💾 Finalizing all storage formats...\n",
            "   ✓ Wrote shard 8 for Benign_Final/grayscale_32x32/train\n",
            "   ✓ Wrote shard 8 for Benign_Final/grayscale_39x39/train\n",
            "   ✓ Wrote shard 8 for Benign_Final/grayscale_64x64/train\n",
            "   ✓ Wrote shard 8 for Benign_Final/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 8 for Benign_Final/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 8 for Benign_Final/5channel_32x32/train\n",
            "   ✓ Wrote shard 1 for Benign_Final/grayscale_32x32/val\n",
            "   ✓ Wrote shard 1 for Benign_Final/grayscale_39x39/val\n",
            "   ✓ Wrote shard 1 for Benign_Final/grayscale_64x64/val\n",
            "   ✓ Wrote shard 1 for Benign_Final/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 1 for Benign_Final/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 1 for Benign_Final/5channel_32x32/val\n",
            "   ✓ Wrote shard 1 for Benign_Final/grayscale_32x32/test\n",
            "   ✓ Wrote shard 1 for Benign_Final/grayscale_39x39/test\n",
            "   ✓ Wrote shard 1 for Benign_Final/grayscale_64x64/test\n",
            "   ✓ Wrote shard 1 for Benign_Final/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 1 for Benign_Final/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 1 for Benign_Final/5channel_32x32/test\n",
            "   ✓ Wrote shard 8 for DDoS-HTTP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 8 for DDoS-HTTP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 8 for DDoS-HTTP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 8 for DDoS-HTTP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 8 for DDoS-HTTP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 8 for DDoS-HTTP_Flood/5channel_32x32/train\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/grayscale_32x32/test\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/grayscale_39x39/test\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/grayscale_64x64/test\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/5channel_32x32/test\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/grayscale_32x32/val\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/grayscale_39x39/val\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/grayscale_64x64/val\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 1 for DDoS-HTTP_Flood/5channel_32x32/val\n",
            "   ✓ Wrote shard 8 for DDoS-SYN_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 8 for DDoS-SYN_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 8 for DDoS-SYN_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 8 for DDoS-SYN_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 8 for DDoS-SYN_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 8 for DDoS-SYN_Flood/5channel_32x32/train\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/grayscale_32x32/test\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/grayscale_39x39/test\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/grayscale_64x64/test\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/5channel_32x32/test\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/grayscale_32x32/val\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/grayscale_39x39/val\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/grayscale_64x64/val\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 1 for DDoS-SYN_Flood/5channel_32x32/val\n",
            "   ✓ Wrote shard 8 for DictionaryBruteForce/grayscale_32x32/train\n",
            "   ✓ Wrote shard 8 for DictionaryBruteForce/grayscale_39x39/train\n",
            "   ✓ Wrote shard 8 for DictionaryBruteForce/grayscale_64x64/train\n",
            "   ✓ Wrote shard 8 for DictionaryBruteForce/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 8 for DictionaryBruteForce/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 8 for DictionaryBruteForce/5channel_32x32/train\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/grayscale_32x32/test\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/grayscale_39x39/test\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/grayscale_64x64/test\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/5channel_32x32/test\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/grayscale_32x32/val\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/grayscale_39x39/val\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/grayscale_64x64/val\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 1 for DictionaryBruteForce/5channel_32x32/val\n",
            "   ✓ Wrote shard 8 for DoS-TCP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 8 for DoS-TCP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 8 for DoS-TCP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 8 for DoS-TCP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 8 for DoS-TCP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 8 for DoS-TCP_Flood/5channel_32x32/train\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/grayscale_32x32/val\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/grayscale_39x39/val\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/grayscale_64x64/val\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/5channel_32x32/val\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/grayscale_32x32/test\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/grayscale_39x39/test\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/grayscale_64x64/test\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 1 for DoS-TCP_Flood/5channel_32x32/test\n",
            "   ✓ Wrote shard 8 for DoS-UDP_Flood/grayscale_32x32/train\n",
            "   ✓ Wrote shard 8 for DoS-UDP_Flood/grayscale_39x39/train\n",
            "   ✓ Wrote shard 8 for DoS-UDP_Flood/grayscale_64x64/train\n",
            "   ✓ Wrote shard 8 for DoS-UDP_Flood/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 8 for DoS-UDP_Flood/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 8 for DoS-UDP_Flood/5channel_32x32/train\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/grayscale_32x32/val\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/grayscale_39x39/val\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/grayscale_64x64/val\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/5channel_32x32/val\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/grayscale_32x32/test\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/grayscale_39x39/test\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/grayscale_64x64/test\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 1 for DoS-UDP_Flood/5channel_32x32/test\n",
            "   ✓ Wrote shard 8 for Mirai-udpplain/grayscale_32x32/train\n",
            "   ✓ Wrote shard 8 for Mirai-udpplain/grayscale_39x39/train\n",
            "   ✓ Wrote shard 8 for Mirai-udpplain/grayscale_64x64/train\n",
            "   ✓ Wrote shard 8 for Mirai-udpplain/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 8 for Mirai-udpplain/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 8 for Mirai-udpplain/5channel_32x32/train\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/grayscale_32x32/val\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/grayscale_39x39/val\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/grayscale_64x64/val\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/5channel_32x32/val\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/grayscale_32x32/test\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/grayscale_39x39/test\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/grayscale_64x64/test\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 1 for Mirai-udpplain/5channel_32x32/test\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/grayscale_32x32/test\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/grayscale_39x39/test\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/grayscale_64x64/test\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/5channel_32x32/test\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/grayscale_32x32/val\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/grayscale_39x39/val\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/grayscale_64x64/val\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 1 for Recon-PortScan/5channel_32x32/val\n",
            "   ✓ Wrote shard 8 for Recon-PortScan/grayscale_32x32/train\n",
            "   ✓ Wrote shard 8 for Recon-PortScan/grayscale_39x39/train\n",
            "   ✓ Wrote shard 8 for Recon-PortScan/grayscale_64x64/train\n",
            "   ✓ Wrote shard 8 for Recon-PortScan/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 8 for Recon-PortScan/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 8 for Recon-PortScan/5channel_32x32/train\n",
            "   ✓ Wrote shard 8 for SqlInjection/grayscale_32x32/train\n",
            "   ✓ Wrote shard 8 for SqlInjection/grayscale_39x39/train\n",
            "   ✓ Wrote shard 8 for SqlInjection/grayscale_64x64/train\n",
            "   ✓ Wrote shard 8 for SqlInjection/rgb_hilbert_32x32/train\n",
            "   ✓ Wrote shard 8 for SqlInjection/rgb_spiral_32x32/train\n",
            "   ✓ Wrote shard 8 for SqlInjection/5channel_32x32/train\n",
            "   ✓ Wrote shard 1 for SqlInjection/grayscale_32x32/test\n",
            "   ✓ Wrote shard 1 for SqlInjection/grayscale_39x39/test\n",
            "   ✓ Wrote shard 1 for SqlInjection/grayscale_64x64/test\n",
            "   ✓ Wrote shard 1 for SqlInjection/rgb_hilbert_32x32/test\n",
            "   ✓ Wrote shard 1 for SqlInjection/rgb_spiral_32x32/test\n",
            "   ✓ Wrote shard 1 for SqlInjection/5channel_32x32/test\n",
            "   ✓ Wrote shard 1 for SqlInjection/grayscale_32x32/val\n",
            "   ✓ Wrote shard 1 for SqlInjection/grayscale_39x39/val\n",
            "   ✓ Wrote shard 1 for SqlInjection/grayscale_64x64/val\n",
            "   ✓ Wrote shard 1 for SqlInjection/rgb_hilbert_32x32/val\n",
            "   ✓ Wrote shard 1 for SqlInjection/rgb_spiral_32x32/val\n",
            "   ✓ Wrote shard 1 for SqlInjection/5channel_32x32/val\n",
            "\n",
            "✅ Dataset creation complete!\n",
            "📁 Location: gs://ai-cyber/datasets/pcap-organized-by-label/20250721_141133/\n",
            "📊 Total samples: 108,000\n",
            "⏱️ Total time: 57.7 minutes\n",
            "\n",
            "📈 Samples per label:\n",
            "   Benign_Final: 12,000 samples\n",
            "   DDoS-HTTP_Flood: 12,000 samples\n",
            "   DDoS-SYN_Flood: 12,000 samples\n",
            "   DictionaryBruteForce: 12,000 samples\n",
            "   DoS-TCP_Flood: 12,000 samples\n",
            "   DoS-UDP_Flood: 12,000 samples\n",
            "   Mirai-udpplain: 12,000 samples\n",
            "   Recon-PortScan: 12,000 samples\n",
            "   SqlInjection: 12,000 samples\n"
          ]
        }
      ],
      "source": [
        "# Initialize writer\n",
        "output_base = f\"{CONFIG['output_prefix']}{CONFIG['timestamp']}/\"\n",
        "writer = MultiFormatDataWriter(bucket, output_base, CONFIG['shard_size'])\n",
        "\n",
        "print(\"\\n🚀 Processing PCAP files and creating dataset...\")\n",
        "print(f\"Target: {CONFIG['samples_per_class']} samples per label\\n\")\n",
        "\n",
        "# Track progress\n",
        "sample_count = 0\n",
        "label_counts = Counter()\n",
        "start_time = time.time()\n",
        "\n",
        "# Process each label\n",
        "for label in sorted(files_by_label.keys()):\n",
        "    print(f\"\\n📦 Processing label: {label}\")\n",
        "    label_sample_count = 0\n",
        "\n",
        "    # Process PCAP files for this label\n",
        "    for file_info in files_by_label[label]:\n",
        "        if label_counts[label] >= CONFIG['samples_per_class']:\n",
        "            break\n",
        "\n",
        "        print(f\"   Reading {file_info['path'].split('/')[-1]} ({file_info['size_mb']:.1f} MB)...\")\n",
        "\n",
        "        try:\n",
        "            # Download PCAP file\n",
        "            blob = bucket.blob(file_info['path'])\n",
        "            pcap_data = blob.download_as_bytes()\n",
        "\n",
        "            # Extract packets\n",
        "            packets = read_pcap_packets(pcap_data, max_packets=CONFIG['packets_per_pcap'])\n",
        "            print(f\"   Extracted {len(packets)} packets\")\n",
        "\n",
        "            # Process packets\n",
        "            for packet in packets:\n",
        "                if label_counts[label] >= CONFIG['samples_per_class']:\n",
        "                    break\n",
        "\n",
        "                # Ensure payload is bytes\n",
        "                payload = packet['payload']\n",
        "                if isinstance(payload, (bytes, bytearray)):\n",
        "                    payload_array = np.frombuffer(payload, dtype=np.uint8)\n",
        "                else:\n",
        "                    payload_array = np.array(payload, dtype=np.uint8)\n",
        "\n",
        "                # Pad to 1500 bytes if needed\n",
        "                if len(payload_array) < CONFIG['payload_bytes']:\n",
        "                    payload_array = np.pad(payload_array,\n",
        "                                         (0, CONFIG['payload_bytes'] - len(payload_array)),\n",
        "                                         'constant')\n",
        "                else:\n",
        "                    payload_array = payload_array[:CONFIG['payload_bytes']]\n",
        "\n",
        "                # Generate sample ID\n",
        "                sample_id = f\"{label}_{label_counts[label]:06d}\"\n",
        "\n",
        "                # Determine split\n",
        "                rand_val = np.random.random()\n",
        "                if rand_val < CONFIG['test_size']:\n",
        "                    split = 'test'\n",
        "                elif rand_val < CONFIG['test_size'] + CONFIG['val_size']:\n",
        "                    split = 'val'\n",
        "                else:\n",
        "                    split = 'train'\n",
        "\n",
        "                # Create images for all formats and save\n",
        "                for format_name, format_config in IMAGE_FORMATS.items():\n",
        "                    image = encode_payload_multiformat(payload_array, format_config)\n",
        "\n",
        "                    # Add sample (will be saved in Parquet and PNG)\n",
        "                    writer.add_sample({\n",
        "                        'image': image,\n",
        "                        'label': label,\n",
        "                        'sample_id': sample_id,\n",
        "                        'payload_bytes': payload_array\n",
        "                    }, split, format_name)\n",
        "\n",
        "                label_counts[label] += 1\n",
        "                sample_count += 1\n",
        "\n",
        "                if sample_count % 1000 == 0:\n",
        "                    elapsed = time.time() - start_time\n",
        "                    rate = sample_count / elapsed\n",
        "                    print(f\"   Progress: {sample_count:,} total samples ({rate:.0f} samples/sec)\")\n",
        "\n",
        "            # Clear memory\n",
        "            del pcap_data\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Error processing {file_info['path']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"   ✓ Collected {label_counts[label]} samples for {label}\")\n",
        "\n",
        "# Finalize\n",
        "print(\"\\n💾 Finalizing all storage formats...\")\n",
        "manifest = writer.finalize()\n",
        "\n",
        "print(f\"\\n✅ Dataset creation complete!\")\n",
        "print(f\"📁 Location: gs://{CONFIG['bucket_name']}/{output_base}\")\n",
        "print(f\"📊 Total samples: {sample_count:,}\")\n",
        "print(f\"⏱️ Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
        "print(f\"\\n📈 Samples per label:\")\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"   {label}: {count:,} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yHOJADuUvoQ"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook processes PCAP files to create a comprehensive multi-format dataset:\n",
        "\n",
        "### What's Created:\n",
        "\n",
        "1. **Parquet Files** (`.parquet`)\n",
        "   - Columnar format for data analysis and ML training\n",
        "   - Easy to load into Pandas/PyTorch/TensorFlow\n",
        "   - Contains: images (flattened), labels, raw payload bytes\n",
        "\n",
        "2. **PNG Files** (`.png`)\n",
        "   - Sample images for visualization\n",
        "   - First 100 samples per class\n",
        "   - Organized by label for easy browsing\n",
        "\n",
        "### Dataset Structure - ORGANIZED BY LABEL:\n",
        "```\n",
        "gs://ai-cyber/datasets/pcap-organized-by-label/[timestamp]/\n",
        "├── parquet/\n",
        "│   ├── grayscale_39x39/\n",
        "│   │   ├── Benign_Final/\n",
        "│   │   │   ├── train/\n",
        "│   │   │   │   ├── shard_00000.parquet\n",
        "│   │   │   │   └── ...\n",
        "│   │   │   ├── val/\n",
        "│   │   │   └── test/\n",
        "│   │   ├── DDoS-HTTP_Flood/\n",
        "│   │   │   ├── train/\n",
        "│   │   │   ├── val/\n",
        "│   │   │   └── test/\n",
        "│   │   └── DoS-TCP_Flood/\n",
        "│   │       ├── train/\n",
        "│   │       ├── val/\n",
        "│   │       └── test/\n",
        "├── png/\n",
        "│   ├── grayscale_39x39/\n",
        "│   │   ├── Benign_Final/\n",
        "│   │   │   ├── train/\n",
        "│   │   │   │   ├── Benign_Final_000001.png\n",
        "│   │   │   │   └── ...\n",
        "│   │   │   ├── val/\n",
        "│   │   │   └── test/\n",
        "│   │   └── [other labels...]\n",
        "└── manifest.json\n",
        "```\n",
        "\n",
        "### Key Features:\n",
        "- **ORGANIZED BY LABEL** - Each label has its own folder!\n",
        "- Easy to download specific labels without parsing everything\n",
        "- Clear structure: format → label → split → shards\n",
        "- Manifest includes label list for easy discovery\n",
        "- Balanced sampling with up to 12k samples per label\n",
        "- No TFRecord format (removed for simplicity)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
