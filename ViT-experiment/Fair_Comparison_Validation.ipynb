{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair Comparison Validation: 5-Channel vs Multi-Channel Hilbert\n",
    "\n",
    "This notebook ensures both experiments use:\n",
    "1. Correct data paths\n",
    "2. Same data loading approach\n",
    "3. Same train/val/test splits\n",
    "4. Same evaluation methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_parquet_data_properly(base_path, dataset_name):\n",
    "    \"\"\"Load data preserving original splits OR mixed for compatibility\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading {dataset_name} from: {base_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Method 1: Load respecting original splits (RECOMMENDED)\n",
    "    print(\"\\nMethod 1: Respecting original train/val/test splits\")\n",
    "    data_splits = {'train': [], 'val': [], 'test': []}\n",
    "    label_splits = {'train': [], 'val': [], 'test': []}\n",
    "    \n",
    "    # Get all class directories\n",
    "    class_dirs = sorted([d for d in glob.glob(f\"{base_path}*/\") \n",
    "                        if not any(s in d for s in ['train', 'val', 'test'])])\n",
    "    class_names = [d.split('/')[-2] for d in class_dirs]\n",
    "    print(f\"Found {len(class_names)} classes: {class_names}\")\n",
    "    \n",
    "    for class_dir in class_dirs:\n",
    "        class_name = class_dir.split('/')[-2]\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            split_path = f\"{class_dir}{split}/\"\n",
    "            parquet_files = sorted(glob.glob(f\"{split_path}*.parquet\"))\n",
    "            \n",
    "            split_count = 0\n",
    "            for file_path in parquet_files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(file_path)\n",
    "                    if 'image_data' in df.columns:\n",
    "                        for idx, row in df.iterrows():\n",
    "                            image_data = np.array(row['image_data'], dtype=np.float32)\n",
    "                            data_splits[split].append(image_data)\n",
    "                            label_splits[split].append(class_name)\n",
    "                            split_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to arrays\n",
    "    X_train_orig = np.array(data_splits['train'], dtype=np.float32)\n",
    "    X_val_orig = np.array(data_splits['val'], dtype=np.float32)\n",
    "    X_test_orig = np.array(data_splits['test'], dtype=np.float32)\n",
    "    \n",
    "    print(f\"\\nOriginal splits:\")\n",
    "    print(f\"  Train: {len(X_train_orig):,} samples\")\n",
    "    print(f\"  Val:   {len(X_val_orig):,} samples\")\n",
    "    print(f\"  Test:  {len(X_test_orig):,} samples\")\n",
    "    print(f\"  Total: {len(X_train_orig) + len(X_val_orig) + len(X_test_orig):,} samples\")\n",
    "    \n",
    "    # Method 2: Mix all data (for backward compatibility)\n",
    "    print(\"\\nMethod 2: Mixed data (current notebook approach)\")\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        all_data.extend(data_splits[split])\n",
    "        all_labels.extend(label_splits[split])\n",
    "    \n",
    "    X_mixed = np.array(all_data, dtype=np.float32)\n",
    "    y_mixed = np.array(all_labels)\n",
    "    \n",
    "    print(f\"\\nMixed approach:\")\n",
    "    print(f\"  Total samples: {len(X_mixed):,}\")\n",
    "    print(f\"  Will be re-split: 70% train, 15% val, 15% test\")\n",
    "    print(f\"  New sizes: Train={int(len(X_mixed)*0.7):,}, Val={int(len(X_mixed)*0.15):,}, Test={int(len(X_mixed)*0.15):,}\")\n",
    "    \n",
    "    return X_mixed, y_mixed, X_train_orig, X_val_orig, X_test_orig\n",
    "\n",
    "# Test both datasets\n",
    "datasets = {\n",
    "    '5channel': '/home/ubuntu/analyst/notebooks/ViT-experiment/pcap-dataset-samples/parquet/5channel_32x32/',\n",
    "    'multichannel_hilbert': '/home/ubuntu/analyst/notebooks/ViT-experiment/pcap-dataset-samples/parquet/multichannel_hilbert_32x32/'\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, path in datasets.items():\n",
    "    try:\n",
    "        X_mixed, y_mixed, X_train, X_val, X_test = load_parquet_data_properly(path, name)\n",
    "        results[name] = {\n",
    "            'loaded': True,\n",
    "            'mixed_shape': X_mixed.shape,\n",
    "            'orig_train': X_train.shape,\n",
    "            'orig_val': X_val.shape,\n",
    "            'orig_test': X_test.shape\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results[name] = {'loaded': False, 'error': str(e)}\n",
    "        print(f\"\\n‚ùå Error loading {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if all(r.get('loaded', False) for r in results.values()):\n",
    "    print(\"\\n‚úÖ Both datasets loaded successfully!\")\n",
    "    \n",
    "    print(\"\\nüìä Data Summary:\")\n",
    "    for name, result in results.items():\n",
    "        if result['loaded']:\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Mixed approach: {result['mixed_shape']}\")\n",
    "            print(f\"  Original splits: Train={result['orig_train']}, Val={result['orig_val']}, Test={result['orig_test']}\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  IMPORTANT FINDINGS:\")\n",
    "    print(\"\\n1. Data Leakage Issue:\")\n",
    "    print(\"   - Current notebooks mix train/val/test then re-split\")\n",
    "    print(\"   - This can lead to similar samples in train and test sets\")\n",
    "    print(\"   - Results may be overly optimistic\")\n",
    "    \n",
    "    print(\"\\n2. Path Issue in 5-channel notebook:\")\n",
    "    print(\"   - Wrong: '/home/ubuntu/analyst/pcap-dataset-samples/parquet/5channel_32x32/'\")\n",
    "    print(\"   - Correct: '/home/ubuntu/analyst/notebooks/ViT-experiment/pcap-dataset-samples/parquet/5channel_32x32/'\")\n",
    "    \n",
    "    print(\"\\nüîß FIXES NEEDED:\")\n",
    "    print(\"\\n1. Fix the data path in 5-channel notebook\")\n",
    "    print(\"\\n2. Choose one approach:\")\n",
    "    print(\"   Option A: Keep mixed approach (current) - easier but has data leakage\")\n",
    "    print(\"   Option B: Use original splits - more correct but requires notebook changes\")\n",
    "    \n",
    "    print(\"\\n3. Ensure both notebooks use the SAME approach\")\n",
    "    \n",
    "    print(\"\\nüìã Quick Fix for Path (5-channel notebook):\")\n",
    "    print(\"   Change line:\")\n",
    "    print(\"   data_path = '/home/ubuntu/analyst/pcap-dataset-samples/parquet/5channel_32x32/'\")\n",
    "    print(\"   To:\")\n",
    "    print(\"   data_path = '/home/ubuntu/analyst/notebooks/ViT-experiment/pcap-dataset-samples/parquet/5channel_32x32/'\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå One or more datasets failed to load\")\n",
    "    for name, result in results.items():\n",
    "        if not result.get('loaded', False):\n",
    "            print(f\"\\n{name}: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fixed data loading code for both notebooks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIXED DATA LOADING CODE\")\n",
    "print(\"Copy this into both notebooks for fair comparison:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fixed_code = '''\n",
    "# Fixed data loading code for fair comparison\n",
    "# UPDATE THIS PATH for each notebook:\n",
    "# For 5-channel: data_path = '/home/ubuntu/analyst/notebooks/ViT-experiment/pcap-dataset-samples/parquet/5channel_32x32/'\n",
    "# For multichannel_hilbert: data_path = '/home/ubuntu/analyst/notebooks/ViT-experiment/pcap-dataset-samples/parquet/multichannel_hilbert_32x32/'\n",
    "\n",
    "def load_parquet_data(base_path):\n",
    "    \"\"\"Load all parquet files from train/val/test splits\"\"\"\n",
    "    all_image_data = []\n",
    "    all_labels = []\n",
    "    splits = ['train', 'val', 'test']\n",
    "    \n",
    "    # Get all class directories - SORT for consistency\n",
    "    class_dirs = sorted([d for d in glob.glob(f\"{base_path}*/\") if not any(s in d for s in splits)])\n",
    "    class_names = [d.split('/')[-2] for d in class_dirs]\n",
    "    print(f\"Found classes: {class_names}\")\n",
    "    \n",
    "    for class_dir in class_dirs:\n",
    "        class_name = class_dir.split('/')[-2]\n",
    "        print(f\"Loading {class_name}...\")\n",
    "        \n",
    "        for split in splits:\n",
    "            split_path = f\"{class_dir}{split}/\"\n",
    "            parquet_files = sorted(glob.glob(f\"{split_path}*.parquet\"))  # SORT for consistency\n",
    "            \n",
    "            for file_path in parquet_files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(file_path)\n",
    "                    \n",
    "                    if 'image_data' in df.columns:\n",
    "                        for idx, row in df.iterrows():\n",
    "                            image_data = np.array(row['image_data'], dtype=np.float32)\n",
    "                            all_image_data.append(image_data)\n",
    "                            all_labels.append(class_name)\n",
    "                        \n",
    "                        print(f\"   Loaded {len(df)} samples from {file_path.split('/')[-1]}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   Error loading {file_path}: {e}\")\n",
    "    \n",
    "    if not all_image_data:\n",
    "        raise ValueError(\"No image data was loaded successfully!\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(all_image_data, dtype=np.float32)\n",
    "    y = np.array(all_labels)\n",
    "    \n",
    "    print(f\"\\\\n‚úì Total samples loaded: {len(X)}\")\n",
    "    print(f\"‚úì Image data shape: {X.shape}\")\n",
    "    print(f\"‚úì Unique labels: {np.unique(y)}\")\n",
    "    \n",
    "    return X, y\n",
    "'''\n",
    "\n",
    "print(fixed_code)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}