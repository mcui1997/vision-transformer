{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LSTM Architecture: RGB Hilbert 32Ã—32 â€“ 6-Class Training (Few-Shot Holdout)\n",
        "\n",
        "## Sequential Learning Setup\n",
        "**Objective**: Train an LSTM on the same 6-class RGB Hilbert dataset used for ViT/CNN, holding out 3 classes for few-shot experiments.\n",
        "\n",
        "**Dataset**: RGB Hilbert 32Ã—32 (3 channels)\n",
        "- Training on 6 classes; held out: DDoS-HTTP_Flood, DoS-UDP_Flood, Recon-PortScan\n",
        "\n",
        "**Input**: Treat each 32Ã—32 RGB image as a sequence of 32 timesteps with 96 features per step (32 columns Ã— 3 channels)  \n",
        "**Architecture**: Long Short-Term Memory (LSTM) with attention  \n",
        "\n",
        "**Questions**:\n",
        "1. Does sequential modeling improve over CNN/ViT on Hilbert-encoded data?\n",
        "2. How do temporal dynamics extracted from RGB Hilbert images compare?\n",
        "3. Does the LSTM generalize similarly to held-out classes?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ LSTM SEQUENTIAL ARCHITECTURE INITIALIZED (RGB Hilbert 6-class)\n",
            "ðŸ“‹ Notebook: LSTM_Prototype_rgb_hilbert_32x32_6_class.ipynb\n",
            "ðŸ“Š Device: cpu\n",
            "ðŸ“Š Dataset: RGB Hilbert 32Ã—32 (6 training classes; held out: ['DDoS-HTTP_Flood', 'DoS-UDP_Flood', 'Recon-PortScan'])\n",
            "ðŸ“Š Sequence format: 32 timesteps Ã— 96 features\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup and Configuration\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration  \n",
        "CONFIG = {\n",
        "    'data_path': '/home/ubuntu/Cyber_AI/ai-cyber/notebooks/ViT-experiment/pcap-dataset-samples/parquet/rgb_hilbert_32x32/',\n",
        "    'test_size': 0.2,\n",
        "    'val_size': 0.2,\n",
        "    'random_state': 42,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,  # Higher LR for LSTM\n",
        "    'epochs': 50,\n",
        "    'patience': 7,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'num_workers': 4,\n",
        "    # LSTM-specific config for RGB Hilbert\n",
        "    'sequence_length': 32,     # 32 time steps (rows)\n",
        "    'input_features': 96,      # 32 columns Ã— 3 channels\n",
        "    'hidden_size': 128,        # LSTM hidden dimension\n",
        "    'num_layers': 2,           # Stacked LSTM layers\n",
        "    'dropout': 0.3,            # Dropout for regularization\n",
        "    'num_classes': 6\n",
        "}\n",
        "\n",
        "# 6-class training with 3 held-out classes (few-shot)\n",
        "HELD_OUT_CLASSES = ['DDoS-HTTP_Flood', 'DoS-UDP_Flood', 'Recon-PortScan']\n",
        "\n",
        "print(\"ðŸ”„ LSTM SEQUENTIAL ARCHITECTURE INITIALIZED (RGB Hilbert 6-class)\")\n",
        "print(\"ðŸ“‹ Notebook: LSTM_Prototype_rgb_hilbert_32x32_6_class.ipynb\")\n",
        "print(f\"ðŸ“Š Device: {CONFIG['device']}\")\n",
        "print(f\"ðŸ“Š Dataset: RGB Hilbert 32Ã—32 (6 training classes; held out: {HELD_OUT_CLASSES})\")\n",
        "print(f\"ðŸ“Š Sequence format: {CONFIG['sequence_length']} timesteps Ã— {CONFIG['input_features']} features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”„ LSTM ARCHITECTURE SUMMARY:\n",
            "ðŸ“Š Total parameters: 322,758\n",
            "ðŸ“Š Trainable parameters: 322,758\n",
            "ðŸ“Š Model size: ~1.2 MB\n",
            "\n",
            "ðŸ” Architecture Details:\n",
            "   â€¢ 2-layer LSTM with attention\n",
            "   â€¢ Hidden size: 128 per layer\n",
            "   â€¢ Multi-head attention mechanism (8 heads)\n",
            "   â€¢ Layer normalization and dropout (0.3)\n",
            "   â€¢ Sequential input: 32 Ã— 96\n"
          ]
        }
      ],
      "source": [
        "# Multi-Layer LSTM Architecture for RGB Hilbert Sequential Analysis\n",
        "class MultiLayerLSTM(nn.Module):\n",
        "    def __init__(self, input_size=96, hidden_size=128, num_layers=2, num_classes=6, dropout=0.3):\n",
        "        super(MultiLayerLSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,  # (batch, seq, feature)\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        \n",
        "        # Attention mechanism for focusing on important timesteps\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=8,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, num_classes)\n",
        "        )\n",
        "        \n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, input_features)\n",
        "        # Expected: (batch_size, 32, 96)\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Initialize hidden states\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
        "        # lstm_out shape: (batch_size, sequence_length, hidden_size)\n",
        "        \n",
        "        # Apply attention to focus on important timesteps\n",
        "        attended_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        # attended_out shape: (batch_size, sequence_length, hidden_size)\n",
        "        \n",
        "        # Global average pooling over sequence dimension\n",
        "        pooled = torch.mean(attended_out, dim=1)  # (batch_size, hidden_size)\n",
        "        \n",
        "        # Classification\n",
        "        output = self.classifier(pooled)  # (batch_size, num_classes)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                # Input-to-hidden weights\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                # Hidden-to-hidden weights\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            elif 'bias' in name:\n",
        "                param.data.fill_(0.)\n",
        "                # Set forget gate bias to 1 (LSTM best practice)\n",
        "                if 'bias_ih' in name:\n",
        "                    n = param.size(0)\n",
        "                    param.data[n//4:n//2].fill_(1.)\n",
        "\n",
        "# Initialize LSTM model\n",
        "model = MultiLayerLSTM(\n",
        "    input_size=CONFIG['input_features'],\n",
        "    hidden_size=CONFIG['hidden_size'],\n",
        "    num_layers=CONFIG['num_layers'],\n",
        "    num_classes=CONFIG['num_classes'],\n",
        "    dropout=CONFIG['dropout']\n",
        ")\n",
        "model = model.to(CONFIG['device'])\n",
        "\n",
        "# Count parameters for comparison\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"\\nðŸ”„ LSTM ARCHITECTURE SUMMARY:\")\n",
        "print(f\"ðŸ“Š Total parameters: {total_params:,}\")\n",
        "print(f\"ðŸ“Š Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"ðŸ“Š Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
        "print(\"\\nðŸ” Architecture Details:\")\n",
        "print(f\"   â€¢ {CONFIG['num_layers']}-layer LSTM with attention\")\n",
        "print(f\"   â€¢ Hidden size: {CONFIG['hidden_size']} per layer\")\n",
        "print(f\"   â€¢ Multi-head attention mechanism (8 heads)\")\n",
        "print(f\"   â€¢ Layer normalization and dropout ({CONFIG['dropout']})\")\n",
        "print(f\"   â€¢ Sequential input: {CONFIG['sequence_length']} Ã— {CONFIG['input_features']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‚ Loading RGB Hilbert 6-class dataset from: /home/ubuntu/Cyber_AI/ai-cyber/notebooks/ViT-experiment/pcap-dataset-samples/parquet/rgb_hilbert_32x32/\n",
            "ðŸ”’ Excluding held-out classes: ['DDoS-HTTP_Flood', 'DoS-UDP_Flood', 'Recon-PortScan']\n",
            "âœ“ Training classes (6): ['Benign_Final', 'DDoS-SYN_Flood', 'DictionaryBruteForce', 'DoS-TCP_Flood', 'Mirai-udpplain', 'SqlInjection']\n",
            "  ðŸ“‚ Loading Benign_Final...\n",
            "  ðŸ“‚ Loading DDoS-SYN_Flood...\n",
            "  ðŸ“‚ Loading DictionaryBruteForce...\n",
            "  ðŸ“‚ Loading DoS-TCP_Flood...\n",
            "  ðŸ“‚ Loading Mirai-udpplain...\n",
            "  ðŸ“‚ Loading SqlInjection...\n",
            "\n",
            "âœ“ Loaded training data: 72,000 samples\n",
            "âœ“ Shape: (72000, 3072)\n",
            "âœ“ Unique classes: ['Benign_Final' 'DDoS-SYN_Flood' 'DictionaryBruteForce' 'DoS-TCP_Flood'\n",
            " 'Mirai-udpplain' 'SqlInjection']\n",
            "\n",
            "ðŸ”„ Reshaping data for LSTM sequential input...\n",
            "   Original shape: (72000, 3072) (flattened: samples Ã— 3072 features)\n",
            "   LSTM shape: (72000, 32, 96) (samples Ã— timesteps Ã— features)\n",
            "\n",
            "ðŸ·ï¸ 6-class label distribution (training classes):\n",
            "   0: Benign_Final (12,000 samples)\n",
            "   1: DDoS-SYN_Flood (12,000 samples)\n",
            "   2: DictionaryBruteForce (12,000 samples)\n",
            "   3: DoS-TCP_Flood (12,000 samples)\n",
            "   4: Mirai-udpplain (12,000 samples)\n",
            "   5: SqlInjection (12,000 samples)\n",
            "\n",
            "ðŸ“ˆ LSTM data ready: range=[0.000, 1.000], shape=(72000, 32, 96)\n"
          ]
        }
      ],
      "source": [
        "# Data Loading for RGB Hilbert 6-Class (Few-Shot Holdout)\n",
        "import glob\n",
        "\n",
        "def load_rgb_hilbert_6class(base_path, held_out_classes):\n",
        "    print(f\"ðŸ“‚ Loading RGB Hilbert 6-class dataset from: {base_path}\")\n",
        "    print(f\"ðŸ”’ Excluding held-out classes: {held_out_classes}\")\n",
        "    all_image_data = []\n",
        "    all_labels = []\n",
        "    splits = ['train', 'val', 'test']\n",
        "\n",
        "    # Discover all classes present\n",
        "    class_dirs = sorted([d for d in glob.glob(f\"{base_path}*/\") if not any(s in d for s in splits)])\n",
        "    class_names = [d.split('/')[-2] for d in class_dirs]\n",
        "\n",
        "    training_classes = [c for c in class_names if c not in held_out_classes]\n",
        "    print(f\"âœ“ Training classes ({len(training_classes)}): {training_classes}\")\n",
        "\n",
        "    for class_dir in class_dirs:\n",
        "        class_name = class_dir.split('/')[-2]\n",
        "        if class_name in held_out_classes:\n",
        "            continue\n",
        "        print(f\"  ðŸ“‚ Loading {class_name}...\")\n",
        "        for split in splits:\n",
        "            parquet_files = sorted(glob.glob(f\"{class_dir}{split}/*.parquet\"))\n",
        "            for file_path in parquet_files:\n",
        "                try:\n",
        "                    df = pd.read_parquet(file_path)\n",
        "                    if 'image_data' in df.columns:\n",
        "                        for _, row in df.iterrows():\n",
        "                            image_data = np.array(row['image_data'], dtype=np.float32)\n",
        "                            all_image_data.append(image_data)\n",
        "                            all_labels.append(class_name)\n",
        "                except Exception as e:\n",
        "                    print(f\"    âš ï¸ Error loading {file_path}: {e}\")\n",
        "\n",
        "    X = np.array(all_image_data, dtype=np.float32)\n",
        "    y = np.array(all_labels)\n",
        "\n",
        "    print(f\"\\nâœ“ Loaded training data: {len(X):,} samples\")\n",
        "    print(f\"âœ“ Shape: {X.shape}\")\n",
        "    print(f\"âœ“ Unique classes: {np.unique(y)}\")\n",
        "    return X, y\n",
        "\n",
        "# Load dataset (excluding held-out classes)\n",
        "X, y = load_rgb_hilbert_6class(CONFIG['data_path'], HELD_OUT_CLASSES)\n",
        "\n",
        "# Reshape for LSTM: each image â†’ sequence of 32 timesteps Ã— 96 features\n",
        "print(f\"\\nðŸ”„ Reshaping data for LSTM sequential input...\")\n",
        "print(f\"   Original shape: {X.shape} (flattened: samples Ã— 3072 features)\")\n",
        "expected_features = CONFIG['sequence_length'] * CONFIG['input_features']  # 32 Ã— 96 = 3072\n",
        "if X.shape[1] == expected_features:\n",
        "    X = X.reshape(-1, CONFIG['sequence_length'], CONFIG['input_features'])\n",
        "else:\n",
        "    if X.shape[1] > expected_features:\n",
        "        X = X[:, :expected_features].reshape(-1, CONFIG['sequence_length'], CONFIG['input_features'])\n",
        "    else:\n",
        "        pad = np.zeros((X.shape[0], expected_features - X.shape[1]), dtype=np.float32)\n",
        "        X = np.concatenate([X, pad], axis=1).reshape(-1, CONFIG['sequence_length'], CONFIG['input_features'])\n",
        "print(f\"   LSTM shape: {X.shape} (samples Ã— timesteps Ã— features)\")\n",
        "\n",
        "# Normalize if needed\n",
        "if X.max() > 1.0:\n",
        "    X = X / 255.0\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"\\nðŸ·ï¸ 6-class label distribution (training classes):\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    count = np.sum(y == label)\n",
        "    print(f\"   {i}: {label} ({count:,} samples)\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ LSTM data ready: range=[{X.min():.3f}, {X.max():.3f}], shape={X.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Data Split Summary:\n",
            "   Training: 43,200 samples (60.0%)\n",
            "   Validation: 14,400 samples (20.0%)\n",
            "   Test: 14,400 samples (20.0%)\n",
            "   Sequential format: timesteps=32, features=96\n",
            "\n",
            "âš–ï¸  Class weights: {np.str_('Benign_Final'): np.float64(1.0), np.str_('DDoS-SYN_Flood'): np.float64(1.0), np.str_('DictionaryBruteForce'): np.float64(1.0), np.str_('DoS-TCP_Flood'): np.float64(1.0), np.str_('Mirai-udpplain'): np.float64(1.0), np.str_('SqlInjection'): np.float64(1.0)}\n",
            "\n",
            "ðŸŽ¯ LSTM Training Configuration:\n",
            "   ðŸ“Š Optimizer: Adam (lr=0.001, weight_decay=1e-4)\n",
            "   ðŸ“Š Loss: Weighted CrossEntropyLoss\n",
            "   ðŸ“Š Scheduler: ReduceLROnPlateau (patience=3)\n",
            "   ðŸ“Š Batch size: 64\n",
            "   ðŸ“Š Max epochs: 50\n",
            "   ðŸ“Š Early stopping patience: 7\n",
            "   ðŸ”„ Sequential processing: 32 timesteps per sample\n",
            "\n",
            "ðŸŽ¯ Architecture Comparison Setup:\n",
            "   ðŸ¤– ViT: Global attention, 96.94% target\n",
            "   ðŸ—ï¸  CNN: Local convolution, ~93%+ (training)\n",
            "   ðŸ”„ LSTM: Sequential memory, starting training next...\n"
          ]
        }
      ],
      "source": [
        "# Data Splitting and Training Setup\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Split data into train/val/test (same random_state as CNN/ViT for fairness)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=CONFIG['test_size'], \n",
        "    random_state=CONFIG['random_state'], \n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=CONFIG['val_size']/(1-CONFIG['test_size']), \n",
        "    random_state=CONFIG['random_state'], \n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"ðŸ“Š Data Split Summary:\")\n",
        "print(f\"   Training: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   Sequential format: timesteps={X_train.shape[1]}, features={X_train.shape[2]}\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_val_tensor = torch.FloatTensor(X_val)\n",
        "y_val_tensor = torch.LongTensor(y_val)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "\n",
        "# Compute class weights for balanced training\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_tensor = torch.FloatTensor(class_weights).to(CONFIG['device'])\n",
        "\n",
        "print(f\"\\nâš–ï¸  Class weights: {dict(zip(label_encoder.classes_, class_weights))}\")\n",
        "\n",
        "# Initialize optimizer and loss function\n",
        "# Higher learning rate for LSTM compared to CNN/ViT\n",
        "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', patience=3, factor=0.5\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ LSTM Training Configuration:\")\n",
        "print(f\"   ðŸ“Š Optimizer: Adam (lr={CONFIG['learning_rate']}, weight_decay=1e-4)\")\n",
        "print(f\"   ðŸ“Š Loss: Weighted CrossEntropyLoss\")\n",
        "print(f\"   ðŸ“Š Scheduler: ReduceLROnPlateau (patience=3)\")\n",
        "print(f\"   ðŸ“Š Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"   ðŸ“Š Max epochs: {CONFIG['epochs']}\")\n",
        "print(f\"   ðŸ“Š Early stopping patience: {CONFIG['patience']}\")\n",
        "print(f\"   ðŸ”„ Sequential processing: {CONFIG['sequence_length']} timesteps per sample\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Architecture Comparison Setup:\")\n",
        "print(f\"   ðŸ¤– ViT: Global attention, 96.94% target\")\n",
        "print(f\"   ðŸ—ï¸  CNN: Local convolution, ~93%+ (training)\")\n",
        "print(f\"   ðŸ”„ LSTM: Sequential memory, starting training next...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Starting LSTM sequential learning...\n",
            "ðŸŽ¯ Target: Match ViT 6-class performance on RGB Hilbert\n",
            "ðŸ’¡ Hypothesis: Sequential patterns > Spatial patterns for IoT security\n",
            "\n",
            "Epoch  1/50 | Train: 0.5981 (1.0164) | Val: 0.6453 (0.8947) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.6453\n",
            "Epoch  2/50 | Train: 0.6795 (0.8289) | Val: 0.7156 (0.7516) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.7156\n",
            "Epoch  3/50 | Train: 0.7147 (0.7699) | Val: 0.7389 (0.7036) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.7389\n",
            "Epoch  4/50 | Train: 0.7385 (0.7251) | Val: 0.7635 (0.6588) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.7635\n",
            "Epoch  5/50 | Train: 0.7546 (0.6769) | Val: 0.7678 (0.6313) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.7678\n",
            "Epoch  6/50 | Train: 0.7680 (0.6243) | Val: 0.7771 (0.5707) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.7771\n",
            "Epoch  7/50 | Train: 0.7795 (0.5861) | Val: 0.7914 (0.5607) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.7914\n",
            "Epoch  8/50 | Train: 0.7886 (0.5602) | Val: 0.7899 (0.5715) | LR: 0.001000\n",
            "Epoch  9/50 | Train: 0.7968 (0.5362) | Val: 0.8039 (0.5026) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8039\n",
            "Epoch 10/50 | Train: 0.8026 (0.5198) | Val: 0.8154 (0.4669) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8154\n",
            "Epoch 11/50 | Train: 0.8094 (0.4979) | Val: 0.8165 (0.4688) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8165\n",
            "Epoch 12/50 | Train: 0.8138 (0.4880) | Val: 0.8235 (0.4658) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8235\n",
            "Epoch 13/50 | Train: 0.8205 (0.4682) | Val: 0.8265 (0.4365) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8265\n",
            "Epoch 14/50 | Train: 0.8255 (0.4560) | Val: 0.8299 (0.4346) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8299\n",
            "Epoch 15/50 | Train: 0.8304 (0.4433) | Val: 0.8390 (0.4160) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8390\n",
            "Epoch 16/50 | Train: 0.8357 (0.4298) | Val: 0.8456 (0.4026) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8456\n",
            "Epoch 17/50 | Train: 0.8418 (0.4159) | Val: 0.8456 (0.3969) | LR: 0.001000\n",
            "Epoch 18/50 | Train: 0.8430 (0.4112) | Val: 0.8490 (0.3930) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8490\n",
            "Epoch 19/50 | Train: 0.8482 (0.4002) | Val: 0.8517 (0.4002) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8517\n",
            "Epoch 20/50 | Train: 0.8504 (0.3941) | Val: 0.8519 (0.3867) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8519\n",
            "Epoch 21/50 | Train: 0.8533 (0.3883) | Val: 0.8495 (0.4012) | LR: 0.001000\n",
            "Epoch 22/50 | Train: 0.8591 (0.3806) | Val: 0.8551 (0.3900) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8551\n",
            "Epoch 23/50 | Train: 0.8606 (0.3731) | Val: 0.8603 (0.3605) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8603\n",
            "Epoch 24/50 | Train: 0.8617 (0.3667) | Val: 0.8582 (0.3701) | LR: 0.001000\n",
            "Epoch 25/50 | Train: 0.8663 (0.3575) | Val: 0.8684 (0.3505) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8684\n",
            "Epoch 26/50 | Train: 0.8696 (0.3543) | Val: 0.8652 (0.3564) | LR: 0.001000\n",
            "Epoch 27/50 | Train: 0.8721 (0.3456) | Val: 0.8729 (0.3586) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8729\n",
            "Epoch 28/50 | Train: 0.8764 (0.3384) | Val: 0.8622 (0.3640) | LR: 0.001000\n",
            "Epoch 29/50 | Train: 0.8768 (0.3357) | Val: 0.8731 (0.3504) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8731\n",
            "Epoch 30/50 | Train: 0.8809 (0.3270) | Val: 0.8752 (0.3497) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8752\n",
            "Epoch 31/50 | Train: 0.8847 (0.3190) | Val: 0.8758 (0.3352) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8758\n",
            "Epoch 32/50 | Train: 0.8869 (0.3161) | Val: 0.8809 (0.3336) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8809\n",
            "Epoch 33/50 | Train: 0.8877 (0.3120) | Val: 0.8792 (0.3359) | LR: 0.001000\n",
            "Epoch 34/50 | Train: 0.8919 (0.3036) | Val: 0.8812 (0.3244) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8812\n",
            "Epoch 35/50 | Train: 0.8920 (0.3013) | Val: 0.8849 (0.3220) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8849\n",
            "Epoch 36/50 | Train: 0.8944 (0.2965) | Val: 0.8840 (0.3214) | LR: 0.001000\n",
            "Epoch 37/50 | Train: 0.8961 (0.2922) | Val: 0.8849 (0.3127) | LR: 0.001000\n",
            "Epoch 38/50 | Train: 0.8980 (0.2882) | Val: 0.8825 (0.3229) | LR: 0.001000\n",
            "Epoch 39/50 | Train: 0.9008 (0.2816) | Val: 0.8857 (0.3352) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8857\n",
            "Epoch 40/50 | Train: 0.9022 (0.2768) | Val: 0.8917 (0.3084) | LR: 0.001000\n",
            "âœ… New best validation accuracy: 0.8917\n",
            "Epoch 41/50 | Train: 0.9044 (0.2720) | Val: 0.8903 (0.3098) | LR: 0.001000\n",
            "Epoch 42/50 | Train: 0.9061 (0.2699) | Val: 0.8851 (0.3282) | LR: 0.001000\n",
            "Epoch 43/50 | Train: 0.9061 (0.2673) | Val: 0.8908 (0.3089) | LR: 0.001000\n",
            "Epoch 44/50 | Train: 0.9077 (0.2641) | Val: 0.8909 (0.3183) | LR: 0.000500\n",
            "Epoch 45/50 | Train: 0.9214 (0.2224) | Val: 0.9062 (0.2828) | LR: 0.000500\n",
            "âœ… New best validation accuracy: 0.9062\n",
            "Epoch 46/50 | Train: 0.9253 (0.2130) | Val: 0.9098 (0.2754) | LR: 0.000500\n",
            "âœ… New best validation accuracy: 0.9098\n",
            "Epoch 47/50 | Train: 0.9269 (0.2051) | Val: 0.9085 (0.2792) | LR: 0.000500\n",
            "Epoch 48/50 | Train: 0.9294 (0.2021) | Val: 0.9108 (0.2896) | LR: 0.000500\n",
            "âœ… New best validation accuracy: 0.9108\n",
            "Epoch 49/50 | Train: 0.9294 (0.2006) | Val: 0.9060 (0.2950) | LR: 0.000500\n",
            "Epoch 50/50 | Train: 0.9314 (0.1949) | Val: 0.9072 (0.2954) | LR: 0.000500\n",
            "\n",
            "ðŸŽ¯ LSTM Training Complete!\n",
            "   â±ï¸  Total time: 0:43:57.460931\n",
            "   ðŸ† Best validation accuracy: 0.9108\n",
            "   ðŸ“Š Total epochs: 50\n",
            "\n",
            "ðŸ“Š ARCHITECTURE COMPARISON:\n",
            "   ðŸ¤– ViT (Global Attention): 0.9694\n",
            "   ðŸ—ï¸  CNN (Local Features): ~0.93+ (training)\n",
            "   ðŸ”„ LSTM (Sequential): 0.9108\n",
            "\n",
            "ðŸ“Š LSTM provides alternative perspective\n",
            "   All architectures show competitive performance\n"
          ]
        }
      ],
      "source": [
        "# LSTM Training Pipeline with Sequential Processing\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        # data shape: (batch_size, sequence_length, input_features)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping for LSTM stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "    \n",
        "    return total_loss / len(train_loader), correct / total\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    \n",
        "    return total_loss / len(val_loader), correct / total\n",
        "\n",
        "# Training loop with early stopping\n",
        "print(\"ðŸ”„ Starting LSTM sequential learning...\")\n",
        "print(\"ðŸŽ¯ Target: Match ViT 6-class performance on RGB Hilbert\")\n",
        "print(\"ðŸ’¡ Hypothesis: Sequential patterns > Spatial patterns for IoT security\\n\")\n",
        "\n",
        "best_val_acc = 0\n",
        "patience_counter = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    # Train for one epoch\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, CONFIG['device'])\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, CONFIG['device'])\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step(val_acc)\n",
        "    \n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    # Print progress with architecture comparison\n",
        "    print(f\"Epoch {epoch+1:2d}/{CONFIG['epochs']} | \"\n",
        "          f\"Train: {train_acc:.4f} ({train_loss:.4f}) | \"\n",
        "          f\"Val: {val_acc:.4f} ({val_loss:.4f}) | \"\n",
        "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Architecture comparison updates\n",
        "    if val_acc > 0.93:\n",
        "        print(f\"ðŸ”„ LSTM approaching CNN performance! Current: {val_acc:.4f} vs CNN: ~0.93+\")\n",
        "    if val_acc > 0.96:\n",
        "        print(f\"ðŸŽ¯ LSTM approaching ViT performance! Current: {val_acc:.4f} vs ViT: 0.9694\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_lstm_rgb_hilbert_6class_model.pth')\n",
        "        print(f\"âœ… New best validation accuracy: {val_acc:.4f}\")\n",
        "        \n",
        "        # Check performance milestones\n",
        "        if val_acc > 0.9694:\n",
        "            print(f\"ðŸ† LSTM BEATS ViT! {val_acc:.4f} > 0.9694\")\n",
        "            print(f\"ðŸ”„ Sequential modeling proves superior for IoT cybersecurity!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        \n",
        "        if patience_counter >= CONFIG['patience']:\n",
        "            print(f\"\\nâ° Early stopping triggered after {epoch+1} epochs\")\n",
        "            print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
        "            break\n",
        "\n",
        "training_time = datetime.now() - start_time\n",
        "print(f\"\\nðŸŽ¯ LSTM Training Complete!\")\n",
        "print(f\"   â±ï¸  Total time: {training_time}\")\n",
        "print(f\"   ðŸ† Best validation accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"   ðŸ“Š Total epochs: {epoch+1}\")\n",
        "\n",
        "# Compare with baselines\n",
        "vit_accuracy = 0.9694\n",
        "cnn_accuracy = 0.93  # Conservative estimate\n",
        "\n",
        "print(f\"\\nðŸ“Š ARCHITECTURE COMPARISON:\")\n",
        "print(f\"   ðŸ¤– ViT (Global Attention): {vit_accuracy:.4f}\")\n",
        "print(f\"   ðŸ—ï¸  CNN (Local Features): ~{cnn_accuracy:.2f}+ (training)\")\n",
        "print(f\"   ðŸ”„ LSTM (Sequential): {best_val_acc:.4f}\")\n",
        "\n",
        "if best_val_acc > vit_accuracy:\n",
        "    improvement = (best_val_acc - vit_accuracy) * 100\n",
        "    print(f\"\\nðŸŽ‰ LSTM OUTPERFORMS ALL BASELINES!\")\n",
        "    print(f\"   LSTM beats ViT by +{improvement:.2f} percentage points\")\n",
        "    print(f\"   ðŸ”„ Sequential modeling is superior for IoT cybersecurity\")\n",
        "elif best_val_acc > cnn_accuracy:\n",
        "    print(f\"\\nðŸ¥ˆ LSTM BEATS CNN but trails ViT\")\n",
        "    print(f\"   Sequential > Local features, but Global attention still leads\")\n",
        "else:\n",
        "    print(f\"\\nðŸ“Š LSTM provides alternative perspective\")\n",
        "    print(f\"   All architectures show competitive performance\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§ª Final LSTM Test Set Evaluation...\n",
            "\n",
            "ðŸŽ¯ FINAL LSTM RESULTS:\n",
            "   ðŸ“Š Test Accuracy: 0.9059 (90.59%)\n",
            "\n",
            "ðŸ† Baseline Comparison:\n",
            "   ViT (Global Attention): 0.9694\n",
            "   LSTM (Sequential): 0.9059\n",
            "\n",
            "ðŸ¥‰ LSTM VERY GOOD Performance!\n",
            "   LSTM: 0.9059 vs ViT: 0.9694\n",
            "   Deficit: -6.35 percentage points\n",
            "\n",
            "ðŸ“‹ Detailed LSTM Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "        Benign_Final     0.9142    0.8658    0.8894      2400\n",
            "      DDoS-SYN_Flood     0.9535    0.9225    0.9377      2400\n",
            "DictionaryBruteForce     0.8658    0.8279    0.8464      2400\n",
            "       DoS-TCP_Flood     0.9957    0.9542    0.9745      2400\n",
            "      Mirai-udpplain     0.9948    0.9529    0.9734      2400\n",
            "        SqlInjection     0.7520    0.9121    0.8243      2400\n",
            "\n",
            "            accuracy                         0.9059     14400\n",
            "           macro avg     0.9127    0.9059    0.9076     14400\n",
            "        weighted avg     0.9127    0.9059    0.9076     14400\n",
            "\n",
            "\n",
            "ðŸŽ¯ LSTM Prediction Confidence Analysis:\n",
            "   Mean confidence: 0.9323\n",
            "   High confidence (>0.9): 12,134 samples\n",
            "   Low confidence (<0.7): 1,500 samples\n",
            "\n",
            "ðŸ’¾ Results saved to: results_lstm_rgb_hilbert_6class.json\n",
            "\n",
            "ðŸŽ¯ SUMMARY:\n",
            "   ðŸ¤– ViT (Attention) baseline: 0.9694\n",
            "   ðŸ”„ LSTM (Sequential): 0.9059 (322,758 params)\n",
            "   ðŸ“Š Performance Tier: ðŸ¥‰ VERY GOOD\n"
          ]
        }
      ],
      "source": [
        "# LSTM Evaluation\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_lstm_rgb_hilbert_6class_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Test set evaluation\n",
        "def evaluate_model(model, test_loader, device, label_encoder):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "    \n",
        "    return np.array(all_preds), np.array(all_targets), np.array(all_probs)\n",
        "\n",
        "print(\"ðŸ§ª Final LSTM Test Set Evaluation...\")\n",
        "test_preds, test_targets, test_probs = evaluate_model(model, test_loader, CONFIG['device'], label_encoder)\n",
        "test_accuracy = accuracy_score(test_targets, test_preds)\n",
        "\n",
        "print(f\"\\nðŸŽ¯ FINAL LSTM RESULTS:\")\n",
        "print(f\"   ðŸ“Š Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Baseline comparison (optional)\n",
        "vit_test_accuracy = 0.9694\n",
        "print(f\"\\nðŸ† Baseline Comparison:\")\n",
        "print(f\"   ViT (Global Attention): {vit_test_accuracy:.4f}\")\n",
        "print(f\"   LSTM (Sequential): {test_accuracy:.4f}\")\n",
        "\n",
        "# Performance tier classification\n",
        "if test_accuracy > vit_test_accuracy:\n",
        "    improvement = (test_accuracy - vit_test_accuracy) * 100\n",
        "    print(f\"\\nðŸŽ‰ LSTM ACHIEVES NEW STATE-OF-THE-ART!\")\n",
        "    print(f\"   LSTM: {test_accuracy:.4f} vs ViT: {vit_test_accuracy:.4f}\")\n",
        "    print(f\"   Improvement: +{improvement:.2f} percentage points\")\n",
        "    print(f\"   ðŸ”„ Sequential modeling proves superior for IoT cybersecurity!\")\n",
        "    tier = \"ðŸ¥‡ STATE-OF-THE-ART\"\n",
        "elif test_accuracy > 0.95:\n",
        "    deficit = (vit_test_accuracy - test_accuracy) * 100\n",
        "    print(f\"\\nðŸ¥ˆ LSTM EXCELLENT Performance!\")\n",
        "    print(f\"   LSTM: {test_accuracy:.4f} vs ViT: {vit_test_accuracy:.4f}\")\n",
        "    print(f\"   Deficit: -{deficit:.2f} percentage points\")\n",
        "    tier = \"ðŸ¥ˆ EXCELLENT\"\n",
        "elif test_accuracy > 0.90:\n",
        "    deficit = (vit_test_accuracy - test_accuracy) * 100\n",
        "    print(f\"\\nðŸ¥‰ LSTM VERY GOOD Performance!\")\n",
        "    print(f\"   LSTM: {test_accuracy:.4f} vs ViT: {vit_test_accuracy:.4f}\")\n",
        "    print(f\"   Deficit: -{deficit:.2f} percentage points\")\n",
        "    tier = \"ðŸ¥‰ VERY GOOD\"\n",
        "else:\n",
        "    deficit = (vit_test_accuracy - test_accuracy) * 100\n",
        "    print(f\"\\nðŸ“Š LSTM Performance Analysis:\")\n",
        "    print(f\"   LSTM: {test_accuracy:.4f} vs ViT: {vit_test_accuracy:.4f}\")\n",
        "    print(f\"   Deficit: -{deficit:.2f} percentage points\")\n",
        "    tier = \"ðŸ“Š BASELINE\"\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\nðŸ“‹ Detailed LSTM Classification Report:\")\n",
        "class_names = label_encoder.classes_\n",
        "report = classification_report(test_targets, test_preds, target_names=class_names, digits=4)\n",
        "print(report)\n",
        "\n",
        "# Confidence analysis\n",
        "confidence_scores = np.max(test_probs, axis=1)\n",
        "mean_confidence = np.mean(confidence_scores)\n",
        "print(f\"\\nðŸŽ¯ LSTM Prediction Confidence Analysis:\")\n",
        "print(f\"   Mean confidence: {mean_confidence:.4f}\")\n",
        "print(f\"   High confidence (>0.9): {np.sum(confidence_scores > 0.9):,} samples\")\n",
        "print(f\"   Low confidence (<0.7): {np.sum(confidence_scores < 0.7):,} samples\")\n",
        "\n",
        "# Save comprehensive results\n",
        "results = {\n",
        "    'experiment': 'LSTM_RGB_Hilbert_6class',\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'dataset': 'RGB_Hilbert_32x32',\n",
        "    'approach': '6-class training with few-shot holdout',\n",
        "    'architecture': 'LSTM_Sequential',\n",
        "    'total_samples': len(X),\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'validation_accuracy': float(best_val_acc),\n",
        "    'training_epochs': epoch + 1,\n",
        "    'training_time': str(training_time),\n",
        "    'parameters': trainable_params,\n",
        "    'sequence_config': {\n",
        "        'sequence_length': CONFIG['sequence_length'],\n",
        "        'input_features': CONFIG['input_features'],\n",
        "        'hidden_size': CONFIG['hidden_size'],\n",
        "        'num_layers': CONFIG['num_layers']\n",
        "    },\n",
        "    'baselines': {\n",
        "        'vit_baseline': vit_test_accuracy\n",
        "    },\n",
        "    'confidence_analysis': {\n",
        "        'mean_confidence': float(mean_confidence),\n",
        "        'high_confidence_samples': int(np.sum(confidence_scores > 0.9)),\n",
        "        'low_confidence_samples': int(np.sum(confidence_scores < 0.7))\n",
        "    },\n",
        "    'performance_tier': tier,\n",
        "    'classification_report': report,\n",
        "    'class_names': list(class_names)\n",
        "}\n",
        "\n",
        "with open('results_lstm_rgb_hilbert_6class.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nðŸ’¾ Results saved to: results_lstm_rgb_hilbert_6class.json\")\n",
        "print(f\"\\nðŸŽ¯ SUMMARY:\")\n",
        "print(f\"   ðŸ¤– ViT (Attention) baseline: {vit_test_accuracy:.4f}\")\n",
        "print(f\"   ðŸ”„ LSTM (Sequential): {test_accuracy:.4f} ({trainable_params:,} params)\")\n",
        "print(f\"   ðŸ“Š Performance Tier: {tier}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
