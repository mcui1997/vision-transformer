{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-6ktiuoUvoJ"
   },
   "source": [
    "# Create Multi-Format Dataset from PCAP Files (Organized by Label)\n",
    "\n",
    "**Objective:** Process PCAP files from GCS to extract packet payloads and create a balanced dataset with 12k samples per label in Parquet and PNG formats.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook:\n",
    "1. Reads PCAP files from `gs://ai-cyber/datasets/unsw-nb15/pcap/pcaps 17-2-2015/`\n",
    "2. Extracts packet payloads (first 1500 bytes)\n",
    "3. Uses CSV files to determine attack types as labels\n",
    "4. Creates 5-channel image encoding format\n",
    "5. **Saves data ORGANIZED BY LABEL** for easy selective downloading\n",
    "6. Outputs in Parquet (for ML) and PNG (for visualization) formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uQ6LTSWUvoM",
    "outputId": "bb485635-a425-4efe-cf9a-53d67f87dabd"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'bucket_name': 'ai-cyber',\n",
    "    'input_prefix': 'datasets/unsw-nb15/pcap/pcaps 17-2-2015/',\n",
    "    'output_prefix': 'datasets/unsw-organized-by-label/',\n",
    "    'samples_per_class': 12000,\n",
    "    'payload_bytes': 1500,  # First 1500 bytes of packet\n",
    "    'test_size': 0.15,\n",
    "    'val_size': 0.15,\n",
    "    'random_seed': 42,\n",
    "    'packets_per_pcap': 100000,  # Process in chunks\n",
    "    'shard_size': 1000,  # Samples per shard\n",
    "    'max_workers': 4,\n",
    "    'save_sample_pngs': 100,  # Save first N PNGs per class\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "}\n",
    "\n",
    "# Image format configurations\n",
    "IMAGE_FORMATS = {\n",
    "    '5channel_32x32': {'shape': (32, 32), 'channels': 5, 'method': 'multiview'}\n",
    "}\n",
    "\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "\n",
    "print(\"‚úì Environment configured\")\n",
    "print(f\"‚úì Will process PCAP files from: gs://{CONFIG['bucket_name']}/{CONFIG['input_prefix']}\")\n",
    "print(f\"‚úì Output: gs://{CONFIG['bucket_name']}/{CONFIG['output_prefix']}\")\n",
    "print(f\"‚úì Target samples per class: {CONFIG['samples_per_class']:,}\")\n",
    "print(f\"‚úì Data will be ORGANIZED BY LABEL for easy access!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHoyEoseUvoN"
   },
   "source": [
    "## PCAP Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f6QgqhpUvoN",
    "outputId": "55199c52-ac33-4730-f584-36102d7bec3b"
   },
   "outputs": [],
   "source": [
    "def read_pcap_packets(pcap_data, max_packets=None):\n",
    "    \"\"\"Extract packets from PCAP data\"\"\"\n",
    "    packets = []\n",
    "\n",
    "    # PCAP global header is 24 bytes\n",
    "    if len(pcap_data) < 24:\n",
    "        return packets\n",
    "\n",
    "    # Check if this is Linux cooked capture (network type 113)\n",
    "    magic, major, minor, thiszone, sigfigs, snaplen, network = struct.unpack('<IHHiIII', pcap_data[:24])\n",
    "    is_linux_cooked = (network == 113)\n",
    "    \n",
    "    # Skip global header\n",
    "    offset = 24\n",
    "    packet_count = 0\n",
    "\n",
    "    while offset < len(pcap_data):\n",
    "        # Check if we have enough data for packet header (16 bytes)\n",
    "        if offset + 16 > len(pcap_data):\n",
    "            break\n",
    "\n",
    "        # Read packet header\n",
    "        ts_sec, ts_usec, incl_len, orig_len = struct.unpack('<IIII', pcap_data[offset:offset+16])\n",
    "        offset += 16\n",
    "\n",
    "        # Check if we have the packet data\n",
    "        if offset + incl_len > len(pcap_data):\n",
    "            break\n",
    "\n",
    "        # Store packet start offset for later use\n",
    "        packet_start = offset\n",
    "        \n",
    "        # Extract packet data\n",
    "        packet_data = pcap_data[offset:offset+incl_len]\n",
    "        offset += incl_len\n",
    "\n",
    "        # Extract payload based on capture type\n",
    "        if is_linux_cooked:\n",
    "            # Linux cooked capture - different header format\n",
    "            if len(packet_data) > 16:  # At least Linux cooked header\n",
    "                # Check if it's IPv4 (0x0800)\n",
    "                proto = struct.unpack('>H', packet_data[14:16])[0]\n",
    "                if proto == 0x0800:\n",
    "                    # IP starts at offset 16 for Linux cooked\n",
    "                    payload = packet_data[16:]\n",
    "                    \n",
    "                    if len(payload) > 20:  # More than just IP header\n",
    "                        packets.append({\n",
    "                            'timestamp': ts_sec + ts_usec/1000000,\n",
    "                            'payload': payload[:CONFIG['payload_bytes']],  # First 1500 bytes\n",
    "                            'length': incl_len,\n",
    "                            'offset': packet_start,  # Add offset for full packet access\n",
    "                            'raw_packet': packet_data  # Store full packet for label matching\n",
    "                        })\n",
    "        else:\n",
    "            # Standard Ethernet capture\n",
    "            if len(packet_data) > 34:  # At least Ethernet + minimal IP header\n",
    "                payload = packet_data[14:]  # Skip Ethernet header\n",
    "                \n",
    "                if len(payload) > 20:  # More than just IP header\n",
    "                    packets.append({\n",
    "                        'timestamp': ts_sec + ts_usec/1000000,\n",
    "                        'payload': payload[:CONFIG['payload_bytes']],\n",
    "                        'length': incl_len,\n",
    "                        'offset': packet_start,\n",
    "                        'raw_packet': packet_data\n",
    "                    })\n",
    "\n",
    "        packet_count += 1\n",
    "        if max_packets and packet_count >= max_packets:\n",
    "            break\n",
    "\n",
    "    return packets\n",
    "\n",
    "print(\"‚úì PCAP processing functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UNSW CSV files for labeling\n",
    "print(\"üîç Loading UNSW CSV files for labeling...\")\n",
    "\n",
    "# Download and load flow CSV files - focusing on Feb 17 data (CSV 2, 3, 4)\n",
    "flow_dfs = []\n",
    "for i in [2, 3, 4]:  # Skip CSV 1 due to January formatting issues\n",
    "    csv_path = f'/tmp/UNSW-NB15_{i}.csv'\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Downloading UNSW-NB15_{i}.csv...\")\n",
    "        subprocess.run(['gsutil', 'cp', f'gs://ai-cyber/datasets/unsw-nb15/csv/UNSW-NB15_{i}.csv', csv_path], check=True)\n",
    "    \n",
    "    # Load with proper column names based on features file\n",
    "    column_names = ['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes', 'dbytes', \n",
    "                    'sttl', 'dttl', 'sloss', 'dloss', 'service', 'Sload', 'Dload', 'Spkts', 'Dpkts',\n",
    "                    'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len',\n",
    "                    'Sjit', 'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
    "                    'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd',\n",
    "                    'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm',\n",
    "                    'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'Label']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, names=column_names, low_memory=False)\n",
    "    flow_dfs.append(df)\n",
    "    print(f\"‚úì Loaded {len(df)} flow records from UNSW-NB15_{i}.csv\")\n",
    "\n",
    "# Combine all flow records\n",
    "all_flows = pd.concat(flow_dfs, ignore_index=True)\n",
    "print(f\"\\\\n‚úì Total flow records loaded: {len(all_flows):,}\")\n",
    "\n",
    "# Filter out records without attack categories\n",
    "labeled_flows = all_flows[all_flows['attack_cat'].notna() & (all_flows['attack_cat'] != '') & (all_flows['attack_cat'] != ' ')]\n",
    "print(f\"‚úì Flow records with attack labels: {len(labeled_flows):,}\")\n",
    "\n",
    "# Add normal flows (those with Label=0 and no attack_cat)\n",
    "normal_flows = all_flows[(all_flows['Label'] == 0) & (all_flows['attack_cat'].isna() | (all_flows['attack_cat'] == '') | (all_flows['attack_cat'] == ' '))]\n",
    "normal_flows['attack_cat'] = 'Normal'\n",
    "labeled_flows = pd.concat([labeled_flows, normal_flows], ignore_index=True)\n",
    "print(f\"‚úì Total labeled flows (including Normal): {len(labeled_flows):,}\")\n",
    "\n",
    "# Get attack category distribution\n",
    "attack_categories = labeled_flows['attack_cat'].value_counts()\n",
    "print(f\"\\\\nüìä Attack categories found:\")\n",
    "for cat, count in attack_categories.items():\n",
    "    print(f\"   {cat}: {count:,} samples\")\n",
    "\n",
    "# Create lookup structure for fast matching\n",
    "print(\"\\\\nüîß Building flow lookup structures...\")\n",
    "\n",
    "def parse_port(port_str):\n",
    "    \"\"\"Parse port number that might be in hex format\"\"\"\n",
    "    if pd.isna(port_str):\n",
    "        return 0\n",
    "    port_str = str(port_str).strip()\n",
    "    if port_str.startswith('0x'):\n",
    "        return int(port_str, 16)\n",
    "    else:\n",
    "        try:\n",
    "            return int(port_str)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "# Create a combined lookup key for each flow\n",
    "flow_lookup = {}\n",
    "for idx, row in labeled_flows.iterrows():\n",
    "    try:\n",
    "        # Parse ports (handle hex format)\n",
    "        src_port = parse_port(row['sport'])\n",
    "        dst_port = parse_port(row['dsport'])\n",
    "        \n",
    "        # Create lookup keys for both directions with time tolerance\n",
    "        # Note: CSV times seem to be off by ~1 second from PCAP times\n",
    "        for time_offset in [-2, -1, 0, 1, 2]:  # Check within 2 seconds\n",
    "            for t in range(int(row['Stime']) + time_offset, int(row['Ltime']) + time_offset + 1):\n",
    "                # Key format: (src_ip, src_port, dst_ip, dst_port, proto, time)\n",
    "                key1 = (row['srcip'], src_port, row['dstip'], dst_port, row['proto'], t)\n",
    "                key2 = (row['dstip'], dst_port, row['srcip'], src_port, row['proto'], t)\n",
    "                \n",
    "                flow_lookup[key1] = row['attack_cat']\n",
    "                flow_lookup[key2] = row['attack_cat']\n",
    "    except Exception as e:\n",
    "        # Skip flows with parsing errors\n",
    "        continue\n",
    "    \n",
    "    if idx % 50000 == 0:\n",
    "        print(f\"   Processed {idx:,} flows...\")\n",
    "\n",
    "print(f\"\\\\n‚úì Flow lookup table ready with {len(flow_lookup):,} entries\")\n",
    "\n",
    "def parse_packet_linux_cooked(packet_data):\n",
    "    \"\"\"Parse packet from Linux cooked capture format\"\"\"\n",
    "    if len(packet_data) < 16:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Linux cooked header is 16 bytes\n",
    "    proto = struct.unpack('>H', packet_data[14:16])[0]\n",
    "    \n",
    "    if proto != 0x0800:  # Not IPv4\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # IP header starts at offset 16\n",
    "    ip_data = packet_data[16:]\n",
    "    if len(ip_data) < 20:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Extract protocol\n",
    "    protocol = ip_data[9]\n",
    "    \n",
    "    # Extract source and destination IPs\n",
    "    src_ip = '.'.join(map(str, ip_data[12:16]))\n",
    "    dst_ip = '.'.join(map(str, ip_data[16:20]))\n",
    "    \n",
    "    # Get IP header length\n",
    "    ip_header_len = (ip_data[0] & 0x0F) * 4\n",
    "    \n",
    "    # Parse transport layer for ports\n",
    "    transport_data = ip_data[ip_header_len:]\n",
    "    \n",
    "    src_port = None\n",
    "    dst_port = None\n",
    "    \n",
    "    if protocol in [6, 17] and len(transport_data) >= 4:  # TCP or UDP\n",
    "        src_port = (transport_data[0] << 8) | transport_data[1]\n",
    "        dst_port = (transport_data[2] << 8) | transport_data[3]\n",
    "    \n",
    "    return src_ip, src_port, dst_ip, dst_port, protocol\n",
    "\n",
    "def get_label_from_packet(packet_data, timestamp):\n",
    "    \"\"\"\n",
    "    Match packet to flow records using fast lookup\n",
    "    \"\"\"\n",
    "    src_ip, src_port, dst_ip, dst_port, protocol = parse_packet_linux_cooked(packet_data)\n",
    "    \n",
    "    if src_ip is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert protocol number to name\n",
    "    proto_map = {6: 'tcp', 17: 'udp', 1: 'icmp'}\n",
    "    proto_name = proto_map.get(protocol, str(protocol))\n",
    "    \n",
    "    # Create lookup key\n",
    "    packet_time = int(timestamp)\n",
    "    \n",
    "    # Try to find exact match\n",
    "    if src_port is not None:\n",
    "        key = (src_ip, src_port, dst_ip, dst_port, proto_name, packet_time)\n",
    "        if key in flow_lookup:\n",
    "            return flow_lookup[key]\n",
    "    \n",
    "    # If no exact match, try without ports (for ICMP or fragmented packets)\n",
    "    for port_combo in [(0, 0), (-1, -1)]:\n",
    "        key = (src_ip, port_combo[0], dst_ip, port_combo[1], proto_name, packet_time)\n",
    "        if key in flow_lookup:\n",
    "            return flow_lookup[key]\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"\\\\n‚úì Packet labeling function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgMEzTE1UvoO"
   },
   "source": [
    "## Image Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVq43fxEUvoO",
    "outputId": "ed357306-15d5-453a-d4ed-bfaaf5cec3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Image encoding functions ready\n"
     ]
    }
   ],
   "source": [
    "def hilbert_curve_positions(n):\n",
    "    \"\"\"Generate Hilbert curve positions for n√ón grid\"\"\"\n",
    "    def hilbert(x, y, xi, xj, yi, yj, n):\n",
    "        if n <= 0:\n",
    "            yield x + (xi + yi) // 2, y + (xj + yj) // 2\n",
    "        else:\n",
    "            for i in hilbert(x, y, yi//2, yj//2, xi//2, xj//2, n-1):\n",
    "                yield i\n",
    "            for i in hilbert(x + xi//2, y + xj//2, xi//2, xj//2, yi//2, yj//2, n-1):\n",
    "                yield i\n",
    "            for i in hilbert(x + xi//2 + yi//2, y + xj//2 + yj//2, xi//2, xj//2, yi//2, yj//2, n-1):\n",
    "                yield i\n",
    "            for i in hilbert(x + xi//2 + yi, y + xj//2 + yj, -yi//2, -yj//2, -xi//2, -xj//2, n-1):\n",
    "                yield i\n",
    "\n",
    "    return list(hilbert(0, 0, n, 0, 0, n, int(np.log2(n))))\n",
    "\n",
    "def spiral_positions(n):\n",
    "    \"\"\"Generate spiral positions for n√ón grid\"\"\"\n",
    "    positions = []\n",
    "    x, y = n // 2, n // 2\n",
    "    dx, dy = 0, -1\n",
    "\n",
    "    for _ in range(n * n):\n",
    "        if 0 <= x < n and 0 <= y < n:\n",
    "            positions.append((x, y))\n",
    "\n",
    "        if x == y or (x < 0 and x == -y) or (x > 0 and x == 1 - y):\n",
    "            dx, dy = -dy, dx\n",
    "        x, y = x + dx, y + dy\n",
    "\n",
    "    return positions\n",
    "\n",
    "def encode_payload_multiformat(payload_bytes, format_config):\n",
    "    \"\"\"Encode payload bytes into various image formats\"\"\"\n",
    "    # Convert payload to numpy array of uint8\n",
    "    if isinstance(payload_bytes, (bytes, bytearray)):\n",
    "        payload_bytes = np.frombuffer(payload_bytes, dtype=np.uint8)\n",
    "    else:\n",
    "        payload_bytes = np.array(payload_bytes, dtype=np.uint8)\n",
    "\n",
    "    height, width = format_config['shape']\n",
    "    channels = format_config['channels']\n",
    "    method = format_config['method']\n",
    "\n",
    "    # Ensure payload is correct length\n",
    "    target_pixels = height * width\n",
    "    if len(payload_bytes) < target_pixels:\n",
    "        payload_bytes = np.pad(payload_bytes, (0, target_pixels - len(payload_bytes)), 'constant')\n",
    "    else:\n",
    "        payload_bytes = payload_bytes[:target_pixels]\n",
    "\n",
    "    if method == 'sequential':\n",
    "        # Simple sequential reshape\n",
    "        image = payload_bytes.reshape(height, width)\n",
    "        if channels == 1:\n",
    "            return image.astype(np.float32) / 255.0\n",
    "        else:\n",
    "            # Create RGB by repeating grayscale\n",
    "            image_norm = image.astype(np.float32) / 255.0\n",
    "            return np.stack([image_norm] * channels, axis=-1)\n",
    "\n",
    "    elif method == 'hilbert':\n",
    "        # Hilbert curve mapping\n",
    "        positions = hilbert_curve_positions(min(height, width))\n",
    "        image = np.zeros((height, width, 3), dtype=np.float32)\n",
    "\n",
    "        for i, (x, y) in enumerate(positions[:len(payload_bytes)]):\n",
    "            if x < height and y < width:\n",
    "                val = payload_bytes[i] / 255.0\n",
    "                image[x, y] = [val, val * 0.7, val * 0.5]\n",
    "\n",
    "        return image\n",
    "\n",
    "    elif method == 'spiral':\n",
    "        # Spiral mapping\n",
    "        positions = spiral_positions(min(height, width))\n",
    "        image = np.zeros((height, width, 3), dtype=np.float32)\n",
    "\n",
    "        for i, (x, y) in enumerate(positions[:len(payload_bytes)]):\n",
    "            if 0 <= x < height and 0 <= y < width:\n",
    "                val = payload_bytes[i] / 255.0\n",
    "                image[x, y] = [val * 0.5, val, val * 0.7]\n",
    "\n",
    "        return image\n",
    "\n",
    "    elif method == 'multiview':\n",
    "        # 5-channel representation\n",
    "        image = np.zeros((height, width, 5), dtype=np.float32)\n",
    "\n",
    "        # Channel 1: Raw bytes\n",
    "        image[:, :, 0] = payload_bytes.reshape(height, width) / 255.0\n",
    "\n",
    "        # Channel 2: Header emphasis (first 64 bytes)\n",
    "        header_channel = np.zeros(target_pixels)\n",
    "        header_channel[:64] = payload_bytes[:64] / 255.0\n",
    "        image[:, :, 1] = header_channel.reshape(height, width)\n",
    "\n",
    "        # Channel 3: Byte frequency\n",
    "        byte_freq = np.bincount(payload_bytes.astype(int), minlength=256)\n",
    "        freq_map = byte_freq[payload_bytes] / (np.max(byte_freq) + 1e-10)\n",
    "        image[:, :, 2] = freq_map.reshape(height, width)\n",
    "\n",
    "        # Channel 4: Local entropy\n",
    "        entropy_map = np.zeros(target_pixels)\n",
    "        window = 16\n",
    "        for i in range(0, len(payload_bytes) - window, window):\n",
    "            window_bytes = payload_bytes[i:i+window]\n",
    "            # Simplified entropy calculation\n",
    "            unique, counts = np.unique(window_bytes, return_counts=True)\n",
    "            probs = counts / window\n",
    "            entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
    "            entropy_map[i:i+window] = entropy / 8  # Normalize by max entropy\n",
    "        image[:, :, 3] = entropy_map.reshape(height, width)\n",
    "\n",
    "        # Channel 5: Gradient magnitude\n",
    "        grad = np.abs(np.diff(payload_bytes.astype(float)))\n",
    "        grad_padded = np.pad(grad, (0, 1), 'edge')\n",
    "        image[:, :, 4] = grad_padded.reshape(height, width) / 255.0\n",
    "\n",
    "        return image\n",
    "\n",
    "print(\"‚úì Image encoding functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpuFjkNvUvoP"
   },
   "source": [
    "## Storage Functions (Parquet and PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCIANmKNUvoP",
    "outputId": "0ec6ba67-60a0-4c7c-8147-06bc41d65efa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Multi-format writer ready - NOW ORGANIZED BY LABEL (Parquet + PNG only)!\n"
     ]
    }
   ],
   "source": [
    "# Parquet helper functions\n",
    "class MultiFormatDataWriter:\n",
    "    \"\"\"Writes data in Parquet and PNG formats - ORGANIZED BY LABEL\"\"\"\n",
    "\n",
    "    def __init__(self, bucket, base_path, shard_size):\n",
    "        self.bucket = bucket\n",
    "        self.base_path = base_path\n",
    "        self.shard_size = shard_size\n",
    "        # Organize by label/format/split\n",
    "        self.current_shard = defaultdict(list)\n",
    "        self.shard_counts = defaultdict(int)\n",
    "        self.png_counts = defaultdict(int)\n",
    "        self.manifest = {\n",
    "            'parquet': defaultdict(lambda: defaultdict(list)),\n",
    "            'png': defaultdict(list)\n",
    "        }\n",
    "\n",
    "    def add_sample(self, sample, split, format_name):\n",
    "        \"\"\"Add a sample and write to all formats\"\"\"\n",
    "        # Include label in the key\n",
    "        label = sample['label']\n",
    "        key = f\"{label}/{format_name}/{split}\"\n",
    "        self.current_shard[key].append(sample)\n",
    "\n",
    "        # Write PNG immediately (for first N samples per class)\n",
    "        label_key = f\"{format_name}/{split}/{label}\"\n",
    "        if self.png_counts[label_key] < CONFIG['save_sample_pngs']:\n",
    "            self._write_png(sample, split, format_name)\n",
    "            self.png_counts[label_key] += 1\n",
    "\n",
    "        # Write shard if full\n",
    "        if len(self.current_shard[key]) >= self.shard_size:\n",
    "            self._write_shard(key)\n",
    "\n",
    "    def _write_shard(self, key):\n",
    "        \"\"\"Write a shard in Parquet format\"\"\"\n",
    "        if not self.current_shard[key]:\n",
    "            return\n",
    "\n",
    "        # Parse label from key\n",
    "        label, format_name, split = key.split('/')\n",
    "        shard_num = self.shard_counts[key]\n",
    "        samples = self.current_shard[key]\n",
    "\n",
    "        # Write Parquet shard - ORGANIZED BY LABEL\n",
    "        parquet_path = f\"{self.base_path}parquet/{format_name}/{label}/{split}/shard_{shard_num:05d}.parquet\"\n",
    "\n",
    "        # Prepare data for Parquet\n",
    "        data = {\n",
    "            'sample_id': [],\n",
    "            'label': [],\n",
    "            'image_format': [],\n",
    "            'image_data': [],\n",
    "            'height': [],\n",
    "            'width': [],\n",
    "            'channels': [],\n",
    "            'payload_bytes': []\n",
    "        }\n",
    "\n",
    "        for sample in samples:\n",
    "            image = sample['image']\n",
    "            data['sample_id'].append(sample['sample_id'])\n",
    "            data['label'].append(sample['label'])\n",
    "            data['image_format'].append(format_name)\n",
    "            data['image_data'].append(image.flatten().tolist())\n",
    "            data['height'].append(image.shape[0])\n",
    "            data['width'].append(image.shape[1])\n",
    "            data['channels'].append(image.shape[2] if len(image.shape) > 2 else 1)\n",
    "            data['payload_bytes'].append(sample['payload_bytes'].tolist())\n",
    "\n",
    "        # Create table and save\n",
    "        table = pa.table(data)\n",
    "        buffer = io.BytesIO()\n",
    "        pq.write_table(table, buffer)\n",
    "        buffer.seek(0)\n",
    "\n",
    "        blob = self.bucket.blob(parquet_path)\n",
    "        blob.upload_from_file(buffer)\n",
    "\n",
    "        self.manifest['parquet'][label][f\"{format_name}/{split}\"].append({\n",
    "            'shard_num': shard_num,\n",
    "            'path': parquet_path,\n",
    "            'num_samples': len(samples)\n",
    "        })\n",
    "\n",
    "        # Clear shard and increment counter\n",
    "        self.current_shard[key] = []\n",
    "        self.shard_counts[key] += 1\n",
    "\n",
    "        print(f\"   ‚úì Wrote shard {shard_num} for {label}/{format_name}/{split}\")\n",
    "\n",
    "    def _write_png(self, sample, split, format_name):\n",
    "        \"\"\"Write individual PNG file\"\"\"\n",
    "        image = sample['image']\n",
    "\n",
    "        # Normalize to 0-255 range\n",
    "        if image.dtype == np.float32 or image.dtype == np.float64:\n",
    "            image_uint8 = (image * 255).astype(np.uint8)\n",
    "        else:\n",
    "            image_uint8 = image.astype(np.uint8)\n",
    "\n",
    "        # Create PIL image\n",
    "        if len(image_uint8.shape) == 2:\n",
    "            pil_image = Image.fromarray(image_uint8, mode='L')\n",
    "        elif image_uint8.shape[2] == 3:\n",
    "            pil_image = Image.fromarray(image_uint8, mode='RGB')\n",
    "        else:\n",
    "            # For 5-channel, save first 3 as RGB\n",
    "            pil_image = Image.fromarray(image_uint8[:, :, :3], mode='RGB')\n",
    "\n",
    "        # Save to buffer\n",
    "        buffer = io.BytesIO()\n",
    "        pil_image.save(buffer, format='PNG')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        # Upload to GCS - ORGANIZED BY LABEL\n",
    "        path = f\"{self.base_path}png/{format_name}/{label}/{split}/{sample['sample_id']}.png\"\n",
    "        blob = self.bucket.blob(path)\n",
    "        blob.upload_from_file(buffer, content_type='image/png')\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"Write remaining shards and save manifests\"\"\"\n",
    "        # Write remaining shards\n",
    "        for key in list(self.current_shard.keys()):\n",
    "            if self.current_shard[key]:\n",
    "                self._write_shard(key)\n",
    "\n",
    "        # Save combined manifest\n",
    "        manifest_data = {\n",
    "            'timestamp': CONFIG['timestamp'],\n",
    "            'shard_size': self.shard_size,\n",
    "            'formats': {\n",
    "                'parquet': dict(self.manifest['parquet']),\n",
    "                'png': dict(self.png_counts)\n",
    "            },\n",
    "            'total_shards': dict(self.shard_counts),\n",
    "            'image_formats': IMAGE_FORMATS,\n",
    "            'labels': list(self.manifest['parquet'].keys())  # List all labels processed\n",
    "        }\n",
    "\n",
    "        manifest_blob = self.bucket.blob(f\"{self.base_path}manifest.json\")\n",
    "        manifest_blob.upload_from_string(\n",
    "            json.dumps(manifest_data, indent=2),\n",
    "            content_type='application/json'\n",
    "        )\n",
    "\n",
    "        return manifest_data\n",
    "\n",
    "print(\"‚úì Multi-format writer ready - NOW ORGANIZED BY LABEL (Parquet + PNG only)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwbOL8MGUvoQ"
   },
   "source": [
    "## Process PCAP Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoR6UHN2UvoQ",
    "outputId": "d58ebd79-67e1-4e5f-c4ec-2ab87a204eab"
   },
   "outputs": [],
   "source": [
    "# Initialize GCS\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(CONFIG['bucket_name'])\n",
    "\n",
    "# List all PCAP files\n",
    "print(\"üîç Discovering PCAP files...\")\n",
    "pcap_files = []\n",
    "\n",
    "# List all blobs in the PCAP directory\n",
    "all_blobs = list(bucket.list_blobs(prefix=CONFIG['input_prefix']))\n",
    "\n",
    "# Extract PCAP files (all in one folder for UNSW)\n",
    "for blob in all_blobs:\n",
    "    if blob.name.endswith('.pcap'):\n",
    "        pcap_files.append({\n",
    "            'path': blob.name,\n",
    "            'size_mb': blob.size / (1024 * 1024)\n",
    "        })\n",
    "\n",
    "print(f\"\\\\nüìä Found {len(pcap_files)} PCAP files\")\n",
    "print(f\"Total size: {sum(f['size_mb'] for f in pcap_files):.1f} MB\")\n",
    "\n",
    "# Show first few files\n",
    "print(\"\\\\nüìÅ Sample files:\")\n",
    "for file_info in pcap_files[:5]:\n",
    "    print(f\"   {file_info['path'].split('/')[-1]}: {file_info['size_mb']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rh-x9gJSUvoQ",
    "outputId": "fd9d7e85-1146-4e29-ce90-35265e6499d4"
   },
   "outputs": [],
   "source": [
    "# Initialize writer\n",
    "output_base = f\"{CONFIG['output_prefix']}{CONFIG['timestamp']}/\"\n",
    "writer = MultiFormatDataWriter(bucket, output_base, CONFIG['shard_size'])\n",
    "\n",
    "print(\"\\\\nüöÄ Processing PCAP files and creating dataset...\")\n",
    "print(f\"Target: {CONFIG['samples_per_class']} samples per label\\\\n\")\n",
    "\n",
    "# Track progress\n",
    "sample_count = 0\n",
    "label_counts = Counter()\n",
    "skipped_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each PCAP file\n",
    "for file_info in pcap_files:\n",
    "    print(f\"\\\\nüì¶ Processing: {file_info['path'].split('/')[-1]} ({file_info['size_mb']:.1f} MB)\")\n",
    "\n",
    "    try:\n",
    "        # Download PCAP file\n",
    "        blob = bucket.blob(file_info['path'])\n",
    "        pcap_data = blob.download_as_bytes()\n",
    "\n",
    "        # Extract packets\n",
    "        packets = read_pcap_packets(pcap_data, max_packets=CONFIG['packets_per_pcap'])\n",
    "        print(f\"   Extracted {len(packets)} packets\")\n",
    "\n",
    "        # Process packets\n",
    "        packets_processed = 0\n",
    "        packets_labeled = 0\n",
    "        \n",
    "        for packet in packets:\n",
    "            # Get label from flow lookup using the raw packet data\n",
    "            label = get_label_from_packet(packet['raw_packet'], packet['timestamp'])\n",
    "            \n",
    "            # Skip packets without labels\n",
    "            if label is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            packets_labeled += 1\n",
    "            \n",
    "            # Skip if we already have enough samples for this label\n",
    "            if label_counts[label] >= CONFIG['samples_per_class']:\n",
    "                continue\n",
    "\n",
    "            # Use the pre-extracted payload\n",
    "            payload = packet['payload']\n",
    "            if isinstance(payload, (bytes, bytearray)):\n",
    "                payload_array = np.frombuffer(payload, dtype=np.uint8)\n",
    "            else:\n",
    "                payload_array = np.array(payload, dtype=np.uint8)\n",
    "\n",
    "            # Pad to 1500 bytes if needed\n",
    "            if len(payload_array) < CONFIG['payload_bytes']:\n",
    "                payload_array = np.pad(payload_array,\n",
    "                                     (0, CONFIG['payload_bytes'] - len(payload_array)),\n",
    "                                     'constant')\n",
    "            else:\n",
    "                payload_array = payload_array[:CONFIG['payload_bytes']]\n",
    "\n",
    "            # Generate sample ID\n",
    "            sample_id = f\"{label}_{label_counts[label]:06d}\"\n",
    "\n",
    "            # Determine split\n",
    "            rand_val = np.random.random()\n",
    "            if rand_val < CONFIG['test_size']:\n",
    "                split = 'test'\n",
    "            elif rand_val < CONFIG['test_size'] + CONFIG['val_size']:\n",
    "                split = 'val'\n",
    "            else:\n",
    "                split = 'train'\n",
    "\n",
    "            # Create images for all formats and save\n",
    "            for format_name, format_config in IMAGE_FORMATS.items():\n",
    "                image = encode_payload_multiformat(payload_array, format_config)\n",
    "\n",
    "                # Add sample (will be saved in Parquet and PNG)\n",
    "                writer.add_sample({\n",
    "                    'image': image,\n",
    "                    'label': label,\n",
    "                    'sample_id': sample_id,\n",
    "                    'payload_bytes': payload_array\n",
    "                }, split, format_name)\n",
    "\n",
    "            label_counts[label] += 1\n",
    "            sample_count += 1\n",
    "            packets_processed += 1\n",
    "\n",
    "            if sample_count % 1000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = sample_count / elapsed\n",
    "                print(f\"   Progress: {sample_count:,} total samples ({rate:.0f} samples/sec)\")\n",
    "                print(f\"   Label distribution: {dict(label_counts)}\")\n",
    "\n",
    "        # Clear memory\n",
    "        del pcap_data\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"   ‚úì Processed {file_info['path'].split('/')[-1]}\")\n",
    "        print(f\"      Packets with labels: {packets_labeled}/{len(packets)} ({packets_labeled/len(packets)*100:.1f}%)\")\n",
    "        print(f\"      Samples created: {packets_processed}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error processing {file_info['path']}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "    # Check if we have enough samples for all labels\n",
    "    all_labels_complete = True\n",
    "    for cat in attack_categories.index:\n",
    "        if label_counts.get(cat, 0) < CONFIG['samples_per_class']:\n",
    "            all_labels_complete = False\n",
    "            break\n",
    "    \n",
    "    if all_labels_complete:\n",
    "        print(f\"\\\\n‚úì Reached target samples for all labels, stopping...\")\n",
    "        break\n",
    "\n",
    "# Finalize\n",
    "print(\"\\\\nüíæ Finalizing all storage formats...\")\n",
    "manifest = writer.finalize()\n",
    "\n",
    "print(f\"\\\\n‚úÖ Dataset creation complete!\")\n",
    "print(f\"üìÅ Location: gs://{CONFIG['bucket_name']}/{output_base}\")\n",
    "print(f\"üìä Total samples: {sample_count:,}\")\n",
    "print(f\"‚è±Ô∏è Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "print(f\"\\\\nüìà Samples per label:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"   {label}: {count:,} samples\")\n",
    "print(f\"\\\\n‚ö†Ô∏è Skipped {skipped_count:,} packets without labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yHOJADuUvoQ"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook processes PCAP files to create a comprehensive multi-format dataset:\n",
    "\n",
    "### What's Created:\n",
    "\n",
    "1. **Parquet Files** (`.parquet`)\n",
    "   - Columnar format for data analysis and ML training\n",
    "   - Easy to load into Pandas/PyTorch/TensorFlow\n",
    "   - Contains: images (flattened), labels, raw payload bytes\n",
    "\n",
    "2. **PNG Files** (`.png`)\n",
    "   - Sample images for visualization\n",
    "   - First 100 samples per class\n",
    "   - Organized by label for easy browsing\n",
    "\n",
    "### Dataset Structure - ORGANIZED BY LABEL:\n",
    "```\n",
    "gs://ai-cyber/datasets/unsw-organized-by-label/[timestamp]/\n",
    "‚îú‚îÄ‚îÄ parquet/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 5channel_32x32/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Normal/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shard_00000.parquet\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Generic/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Exploits/\n",
    "‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ val/\n",
    "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test/\n",
    "‚îú‚îÄ‚îÄ png/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 5channel_32x32/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Normal/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Normal_000001.png\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [other labels...]\n",
    "‚îî‚îÄ‚îÄ manifest.json\n",
    "```\n",
    "\n",
    "### Key Features:\n",
    "- **ORGANIZED BY LABEL** - Each label has its own folder!\n",
    "- Easy to download specific labels without parsing everything\n",
    "- Clear structure: format ‚Üí label ‚Üí split ‚Üí shards\n",
    "- Manifest includes label list for easy discovery\n",
    "- Balanced sampling with up to 12k samples per label\n",
    "- Only 5-channel format for optimal performance"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
