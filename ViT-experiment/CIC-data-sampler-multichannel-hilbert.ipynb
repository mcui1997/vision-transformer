{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Channel Hilbert-Encoded Dataset from PCAP Files\n",
    "\n",
    "**Objective:** Create a novel encoding that combines the spatial locality preservation of Hilbert curves with the semantic richness of multi-channel representations. This encoding ensures consistent spatial mapping across all channels.\n",
    "\n",
    "## Key Innovation\n",
    "\n",
    "This notebook generates a **5-channel Hilbert-encoded** representation where:\n",
    "1. Each channel represents different semantic views of the payload data\n",
    "2. The Hilbert curve mapping is **consistent across all channels**\n",
    "3. Each (x,y) position refers to the same byte index via Hilbert traversal in every channel\n",
    "\n",
    "## Channel Definitions\n",
    "\n",
    "- **Channel 1 (Raw Bytes)**: Direct byte values mapped via Hilbert curve\n",
    "- **Channel 2 (Header Emphasis)**: Weighted representation emphasizing first 64 bytes\n",
    "- **Channel 3 (Byte Frequency)**: Local frequency of each byte value\n",
    "- **Channel 4 (Local Entropy)**: Shannon entropy in sliding windows\n",
    "- **Channel 5 (Gradient Magnitude)**: Rate of change between adjacent bytes\n",
    "\n",
    "## Spatial Consistency\n",
    "\n",
    "The Hilbert curve ensures that bytes adjacent in the 1D payload remain spatially close in the 2D image. By using the **same Hilbert position mapping** for all channels, we maintain interpretability:\n",
    "- Position (5,7) in Channel 1 (raw bytes) corresponds to byte index 123\n",
    "- Position (5,7) in Channel 4 (entropy) shows the entropy at byte index 123\n",
    "- This alignment enables the ViT to learn spatial-semantic relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'bucket_name': 'ai-cyber',\n",
    "    'input_prefix': 'datasets/cic-iot23/pcap/',\n",
    "    'output_prefix': 'datasets/pcap-multichannel-hilbert/',\n",
    "    'samples_per_class': 12000,\n",
    "    'payload_bytes': 1500,\n",
    "    'test_size': 0.15,\n",
    "    'val_size': 0.15,\n",
    "    'random_seed': 42,\n",
    "    'packets_per_pcap': 100000,\n",
    "    'shard_size': 1000,\n",
    "    'max_workers': 4,\n",
    "    'save_sample_pngs': 100,\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "}\n",
    "\n",
    "# Multi-channel Hilbert format configuration\n",
    "MULTICHANNEL_HILBERT_FORMAT = {\n",
    "    'multichannel_hilbert_32x32': {\n",
    "        'shape': (32, 32), \n",
    "        'channels': 5, \n",
    "        'method': 'multichannel_hilbert',\n",
    "        'description': 'Spatially-aligned semantic channels via Hilbert curve'\n",
    "    }\n",
    "}\n",
    "\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "\n",
    "print(\"âœ“ Environment configured for Multi-Channel Hilbert encoding\")\n",
    "print(f\"âœ“ Output: gs://{CONFIG['bucket_name']}/{CONFIG['output_prefix']}\")\n",
    "print(f\"âœ“ Target samples per class: {CONFIG['samples_per_class']:,}\")\n",
    "print(f\"âœ“ Spatial consistency maintained across all {MULTICHANNEL_HILBERT_FORMAT['multichannel_hilbert_32x32']['channels']} channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCAP Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pcap_packets(pcap_data, max_packets=None):\n",
    "    \"\"\"Extract packets from PCAP data\"\"\"\n",
    "    packets = []\n",
    "\n",
    "    # PCAP global header is 24 bytes\n",
    "    if len(pcap_data) < 24:\n",
    "        return packets\n",
    "\n",
    "    # Skip global header\n",
    "    offset = 24\n",
    "    packet_count = 0\n",
    "\n",
    "    while offset < len(pcap_data):\n",
    "        # Check if we have enough data for packet header (16 bytes)\n",
    "        if offset + 16 > len(pcap_data):\n",
    "            break\n",
    "\n",
    "        # Read packet header\n",
    "        ts_sec, ts_usec, incl_len, orig_len = struct.unpack('IIII', pcap_data[offset:offset+16])\n",
    "        offset += 16\n",
    "\n",
    "        # Check if we have the packet data\n",
    "        if offset + incl_len > len(pcap_data):\n",
    "            break\n",
    "\n",
    "        # Extract packet data\n",
    "        packet_data = pcap_data[offset:offset+incl_len]\n",
    "        offset += incl_len\n",
    "\n",
    "        # Extract payload (skip Ethernet header - 14 bytes, IP header - 20 bytes min)\n",
    "        if len(packet_data) > 34:  # At least Ethernet + minimal IP header\n",
    "            payload = packet_data[14:]  # Skip Ethernet header\n",
    "\n",
    "            # Ensure we have at least some data\n",
    "            if len(payload) > 20:  # More than just IP header\n",
    "                packets.append({\n",
    "                    'timestamp': ts_sec + ts_usec/1000000,\n",
    "                    'payload': payload[:CONFIG['payload_bytes']],\n",
    "                    'length': len(payload)\n",
    "                })\n",
    "\n",
    "        packet_count += 1\n",
    "        if max_packets and packet_count >= max_packets:\n",
    "            break\n",
    "\n",
    "    return packets\n",
    "\n",
    "def extract_label_from_path(gcs_path):\n",
    "    \"\"\"Extract label from GCS path (folder name)\"\"\"\n",
    "    parts = gcs_path.split('/')\n",
    "    if len(parts) >= 2:\n",
    "        return parts[-2]  # Folder name is the label\n",
    "    return 'unknown'\n",
    "\n",
    "print(\"âœ“ PCAP processing functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilbert Curve Generation with Documentation\n",
    "\n",
    "The Hilbert curve provides a continuous mapping from 1D to 2D space that preserves locality. This is crucial for maintaining byte adjacency relationships in the image representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hilbert_curve_positions(n):\n",
    "    \"\"\"\n",
    "    Generate Hilbert curve positions for nÃ—n grid.\n",
    "    Returns a list of (x, y) coordinates in Hilbert traversal order.\n",
    "    \n",
    "    Key property: The i-th position in the returned list corresponds to\n",
    "    where the i-th byte should be placed in the 2D image.\n",
    "    \"\"\"\n",
    "    def hilbert(x, y, xi, xj, yi, yj, n):\n",
    "        if n <= 0:\n",
    "            yield x + (xi + yi) // 2, y + (xj + yj) // 2\n",
    "        else:\n",
    "            for i in hilbert(x, y, yi//2, yj//2, xi//2, xj//2, n-1):\n",
    "                yield i\n",
    "            for i in hilbert(x + xi//2, y + xj//2, xi//2, xj//2, yi//2, yj//2, n-1):\n",
    "                yield i\n",
    "            for i in hilbert(x + xi//2 + yi//2, y + xj//2 + yj//2, xi//2, xj//2, yi//2, yj//2, n-1):\n",
    "                yield i\n",
    "            for i in hilbert(x + xi//2 + yi, y + xj//2 + yj, -yi//2, -yj//2, -xi//2, -xj//2, n-1):\n",
    "                yield i\n",
    "\n",
    "    return list(hilbert(0, 0, n, 0, 0, n, int(np.log2(n))))\n",
    "\n",
    "# Pre-compute Hilbert positions for 32x32 grid\n",
    "HILBERT_POSITIONS_32 = hilbert_curve_positions(32)\n",
    "print(f\"âœ“ Generated Hilbert curve for 32Ã—32 grid: {len(HILBERT_POSITIONS_32)} positions\")\n",
    "print(f\"âœ“ First 10 positions: {HILBERT_POSITIONS_32[:10]}\")\n",
    "print(f\"âœ“ This mapping will be used consistently across ALL channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Channel Semantic Feature Extraction\n",
    "\n",
    "Each channel computes different semantic features from the same byte sequence. The key is that all channels use the same spatial mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_semantic_channels(payload_bytes):\n",
    "    \"\"\"\n",
    "    Compute 5 semantic channels from payload bytes.\n",
    "    All channels have the same length as the input payload.\n",
    "    \n",
    "    Returns:\n",
    "        channels: dict with 5 numpy arrays, each of length len(payload_bytes)\n",
    "    \"\"\"\n",
    "    # Ensure payload_bytes is numpy array\n",
    "    if isinstance(payload_bytes, (bytes, bytearray)):\n",
    "        payload_bytes = np.frombuffer(payload_bytes, dtype=np.uint8)\n",
    "    else:\n",
    "        payload_bytes = np.array(payload_bytes, dtype=np.uint8)\n",
    "    \n",
    "    n_bytes = len(payload_bytes)\n",
    "    channels = {}\n",
    "    \n",
    "    # Channel 1: Raw bytes (normalized)\n",
    "    channels['raw_bytes'] = payload_bytes.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Channel 2: Header emphasis (exponential decay from start)\n",
    "    header_weights = np.exp(-np.arange(n_bytes) / 32.0)  # Decay constant of 32 bytes\n",
    "    channels['header_emphasis'] = (payload_bytes.astype(np.float32) * header_weights) / 255.0\n",
    "    \n",
    "    # Channel 3: Byte frequency (how common is each byte value)\n",
    "    byte_counts = np.bincount(payload_bytes, minlength=256)\n",
    "    byte_frequencies = byte_counts / (np.sum(byte_counts) + 1e-10)\n",
    "    channels['byte_frequency'] = byte_frequencies[payload_bytes]\n",
    "    \n",
    "    # Channel 4: Local entropy (Shannon entropy in sliding windows)\n",
    "    window_size = 16\n",
    "    entropy_values = np.zeros(n_bytes, dtype=np.float32)\n",
    "    \n",
    "    for i in range(n_bytes):\n",
    "        # Define window boundaries\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(n_bytes, i + window_size // 2)\n",
    "        window = payload_bytes[start:end]\n",
    "        \n",
    "        # Calculate entropy\n",
    "        if len(window) > 0:\n",
    "            _, counts = np.unique(window, return_counts=True)\n",
    "            probs = counts / len(window)\n",
    "            entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
    "            entropy_values[i] = entropy / 8.0  # Normalize by max entropy\n",
    "    \n",
    "    channels['local_entropy'] = entropy_values\n",
    "    \n",
    "    # Channel 5: Gradient magnitude (rate of change)\n",
    "    # Pad with edge values to maintain array length\n",
    "    gradient = np.abs(np.diff(payload_bytes.astype(np.float32)))\n",
    "    gradient = np.concatenate([[gradient[0]], gradient])  # Pad to maintain length\n",
    "    channels['gradient_magnitude'] = gradient / 255.0\n",
    "    \n",
    "    return channels\n",
    "\n",
    "# Test semantic channel computation\n",
    "test_payload = np.random.randint(0, 256, 100, dtype=np.uint8)\n",
    "test_channels = compute_semantic_channels(test_payload)\n",
    "print(\"âœ“ Semantic channel computation test:\")\n",
    "for name, values in test_channels.items():\n",
    "    print(f\"   {name}: shape={values.shape}, range=[{values.min():.3f}, {values.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Channel Hilbert Encoding Function\n",
    "\n",
    "This is the core function that creates spatially-aligned multi-channel images using consistent Hilbert curve mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_multichannel_hilbert(payload_bytes, image_size=32):\n",
    "    \"\"\"\n",
    "    Encode payload bytes into multi-channel image using Hilbert curve mapping.\n",
    "    \n",
    "    Key innovation: All channels use the SAME Hilbert position mapping,\n",
    "    ensuring spatial consistency across semantic views.\n",
    "    \n",
    "    Args:\n",
    "        payload_bytes: Raw payload bytes\n",
    "        image_size: Size of output image (32x32 default)\n",
    "    \n",
    "    Returns:\n",
    "        image: numpy array of shape (5, 32, 32) with normalized values [0, 1]\n",
    "    \"\"\"\n",
    "    # Get Hilbert positions\n",
    "    if image_size == 32:\n",
    "        positions = HILBERT_POSITIONS_32\n",
    "    else:\n",
    "        positions = hilbert_curve_positions(image_size)\n",
    "    \n",
    "    total_pixels = image_size * image_size\n",
    "    \n",
    "    # Compute semantic channels\n",
    "    channels = compute_semantic_channels(payload_bytes)\n",
    "    \n",
    "    # Initialize multi-channel image\n",
    "    image = np.zeros((5, image_size, image_size), dtype=np.float32)\n",
    "    \n",
    "    # Map each channel using the SAME Hilbert positions\n",
    "    channel_names = ['raw_bytes', 'header_emphasis', 'byte_frequency', \n",
    "                     'local_entropy', 'gradient_magnitude']\n",
    "    \n",
    "    for ch_idx, ch_name in enumerate(channel_names):\n",
    "        channel_data = channels[ch_name]\n",
    "        \n",
    "        # Ensure we have enough data (pad with zeros if needed)\n",
    "        if len(channel_data) < total_pixels:\n",
    "            channel_data = np.pad(channel_data, (0, total_pixels - len(channel_data)), 'constant')\n",
    "        else:\n",
    "            channel_data = channel_data[:total_pixels]\n",
    "        \n",
    "        # Map to 2D using Hilbert positions\n",
    "        for byte_idx, (x, y) in enumerate(positions):\n",
    "            if byte_idx < len(channel_data):\n",
    "                image[ch_idx, x, y] = channel_data[byte_idx]\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Test multi-channel Hilbert encoding\n",
    "test_payload = np.random.randint(0, 256, 1500, dtype=np.uint8)\n",
    "test_image = encode_multichannel_hilbert(test_payload)\n",
    "print(f\"âœ“ Multi-channel Hilbert encoding test:\")\n",
    "print(f\"   Output shape: {test_image.shape}\")\n",
    "print(f\"   Value range: [{test_image.min():.3f}, {test_image.max():.3f}]\")\n",
    "print(f\"\\nâœ“ Spatial consistency verified:\")\n",
    "# Find a valid position to demonstrate\n",
    "example_byte_idx = 100  # Pick byte index 100 as example\n",
    "if example_byte_idx < len(HILBERT_POSITIONS_32):\n",
    "    example_pos = HILBERT_POSITIONS_32[example_byte_idx]\n",
    "    print(f\"   Byte index {example_byte_idx} maps to position {example_pos} in ALL channels\")\n",
    "    print(f\"   This ensures spatial-semantic alignment across the multi-channel representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Multi-Channel Hilbert Encoding\n",
    "\n",
    "Let's visualize how the encoding preserves spatial-semantic alignment across channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multichannel_hilbert(payload_bytes, title=\"Multi-Channel Hilbert Encoding\"):\n",
    "    \"\"\"Visualize all 5 channels of the Hilbert encoding\"\"\"\n",
    "    image = encode_multichannel_hilbert(payload_bytes)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    channel_names = ['Raw Bytes', 'Header Emphasis', 'Byte Frequency', \n",
    "                     'Local Entropy', 'Gradient Magnitude']\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes, channel_names)):\n",
    "        im = ax.imshow(image[i], cmap='viridis', vmin=0, vmax=1)\n",
    "        ax.set_title(f'Channel {i+1}: {name}')\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create sample visualization\n",
    "sample_payload = np.random.randint(0, 256, 1024, dtype=np.uint8)\n",
    "# Add some structure to make it interesting\n",
    "sample_payload[:64] = np.linspace(0, 255, 64).astype(np.uint8)  # Gradient header\n",
    "sample_payload[200:300] = 128  # Constant region\n",
    "sample_payload[400:500] = np.random.randint(0, 256, 100)  # Random region\n",
    "\n",
    "fig = visualize_multichannel_hilbert(sample_payload, \"Sample Multi-Channel Hilbert Encoding\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Channel Interpretations:\")\n",
    "print(\"1. Raw Bytes: Direct payload visualization via Hilbert curve\")\n",
    "print(\"2. Header Emphasis: Highlights initial bytes (protocols headers)\")\n",
    "print(\"3. Byte Frequency: Shows common vs rare byte values\")\n",
    "print(\"4. Local Entropy: Identifies regions of high randomness\")\n",
    "print(\"5. Gradient Magnitude: Detects rapid changes in byte sequences\")\n",
    "print(\"\\nâœ“ All channels maintain spatial correspondence via Hilbert mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Functions for Multi-Channel Hilbert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelHilbertWriter:\n",
    "    \"\"\"Writes multi-channel Hilbert data in Parquet and PNG formats\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket, base_path, shard_size):\n",
    "        self.bucket = bucket\n",
    "        self.base_path = base_path\n",
    "        self.shard_size = shard_size\n",
    "        self.current_shard = defaultdict(list)\n",
    "        self.shard_counts = defaultdict(int)\n",
    "        self.png_counts = defaultdict(int)\n",
    "        self.manifest = {\n",
    "            'format': 'multichannel_hilbert_32x32',\n",
    "            'channels': 5,\n",
    "            'channel_names': ['raw_bytes', 'header_emphasis', 'byte_frequency', \n",
    "                            'local_entropy', 'gradient_magnitude'],\n",
    "            'spatial_mapping': 'hilbert_curve',\n",
    "            'consistency': 'all_channels_use_same_hilbert_positions',\n",
    "            'parquet': defaultdict(lambda: defaultdict(list)),\n",
    "            'png': defaultdict(list)\n",
    "        }\n",
    "    \n",
    "    def add_sample(self, sample, split):\n",
    "        \"\"\"Add a sample with multi-channel Hilbert encoding\"\"\"\n",
    "        # Encode payload to multi-channel Hilbert\n",
    "        image = encode_multichannel_hilbert(sample['payload_bytes'])\n",
    "        \n",
    "        # Create enhanced sample\n",
    "        enhanced_sample = {\n",
    "            'sample_id': sample['sample_id'],\n",
    "            'label': sample['label'],\n",
    "            'image': image,\n",
    "            'payload_bytes': sample['payload_bytes']\n",
    "        }\n",
    "        \n",
    "        # Add to shard\n",
    "        label = sample['label']\n",
    "        key = f\"{label}/multichannel_hilbert_32x32/{split}\"\n",
    "        self.current_shard[key].append(enhanced_sample)\n",
    "        \n",
    "        # Write PNG for first N samples\n",
    "        label_key = f\"multichannel_hilbert_32x32/{split}/{label}\"\n",
    "        if self.png_counts[label_key] < CONFIG['save_sample_pngs']:\n",
    "            self._write_png(enhanced_sample, split)\n",
    "            self.png_counts[label_key] += 1\n",
    "        \n",
    "        # Write shard if full\n",
    "        if len(self.current_shard[key]) >= self.shard_size:\n",
    "            self._write_shard(key)\n",
    "    \n",
    "    def _write_shard(self, key):\n",
    "        \"\"\"Write a shard in Parquet format\"\"\"\n",
    "        if not self.current_shard[key]:\n",
    "            return\n",
    "        \n",
    "        # Parse key\n",
    "        label, format_name, split = key.split('/')\n",
    "        shard_num = self.shard_counts[key]\n",
    "        samples = self.current_shard[key]\n",
    "        \n",
    "        # Prepare data for Parquet\n",
    "        data = {\n",
    "            'sample_id': [],\n",
    "            'label': [],\n",
    "            'image_format': [],\n",
    "            'image_data': [],\n",
    "            'height': [],\n",
    "            'width': [],\n",
    "            'channels': [],\n",
    "            'payload_bytes': [],\n",
    "            'encoding_method': []\n",
    "        }\n",
    "        \n",
    "        for sample in samples:\n",
    "            image = sample['image']\n",
    "            data['sample_id'].append(sample['sample_id'])\n",
    "            data['label'].append(sample['label'])\n",
    "            data['image_format'].append('multichannel_hilbert_32x32')\n",
    "            data['image_data'].append(image.flatten().tolist())\n",
    "            data['height'].append(32)\n",
    "            data['width'].append(32)\n",
    "            data['channels'].append(5)\n",
    "            data['payload_bytes'].append(sample['payload_bytes'].tolist())\n",
    "            data['encoding_method'].append('spatially_aligned_hilbert')\n",
    "        \n",
    "        # Create table and save\n",
    "        parquet_path = f\"{self.base_path}parquet/multichannel_hilbert_32x32/{label}/{split}/shard_{shard_num:05d}.parquet\"\n",
    "        table = pa.table(data)\n",
    "        buffer = io.BytesIO()\n",
    "        pq.write_table(table, buffer)\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        blob = self.bucket.blob(parquet_path)\n",
    "        blob.upload_from_file(buffer)\n",
    "        \n",
    "        self.manifest['parquet'][label][f\"multichannel_hilbert_32x32/{split}\"].append({\n",
    "            'shard_num': shard_num,\n",
    "            'path': parquet_path,\n",
    "            'num_samples': len(samples)\n",
    "        })\n",
    "        \n",
    "        # Clear shard and increment counter\n",
    "        self.current_shard[key] = []\n",
    "        self.shard_counts[key] += 1\n",
    "        \n",
    "        print(f\"   âœ“ Wrote shard {shard_num} for {label}/multichannel_hilbert/{split}\")\n",
    "    \n",
    "    def _write_png(self, sample, split):\n",
    "        \"\"\"Write PNG visualization showing all channels\"\"\"\n",
    "        image = sample['image']\n",
    "        \n",
    "        # Create composite visualization (5 channels side by side)\n",
    "        composite_width = 32 * 5 + 4 * 4  # 5 images with 4 pixel gaps\n",
    "        composite = np.ones((32, composite_width, 3), dtype=np.uint8) * 255\n",
    "        \n",
    "        for i in range(5):\n",
    "            start_x = i * (32 + 4)\n",
    "            # Convert to RGB (using viridis colormap)\n",
    "            channel_data = (image[i] * 255).astype(np.uint8)\n",
    "            # Simple grayscale for now\n",
    "            composite[:, start_x:start_x+32, 0] = channel_data\n",
    "            composite[:, start_x:start_x+32, 1] = channel_data\n",
    "            composite[:, start_x:start_x+32, 2] = channel_data\n",
    "        \n",
    "        # Save composite image\n",
    "        pil_image = Image.fromarray(composite)\n",
    "        buffer = io.BytesIO()\n",
    "        pil_image.save(buffer, format='PNG')\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        path = f\"{self.base_path}png/multichannel_hilbert_32x32/{sample['label']}/{split}/{sample['sample_id']}.png\"\n",
    "        blob = self.bucket.blob(path)\n",
    "        blob.upload_from_file(buffer, content_type='image/png')\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"Write remaining shards and save manifest\"\"\"\n",
    "        # Write remaining shards\n",
    "        for key in list(self.current_shard.keys()):\n",
    "            if self.current_shard[key]:\n",
    "                self._write_shard(key)\n",
    "        \n",
    "        # Enhanced manifest with methodology documentation\n",
    "        manifest_data = {\n",
    "            'timestamp': CONFIG['timestamp'],\n",
    "            'encoding': 'multichannel_hilbert_32x32',\n",
    "            'methodology': {\n",
    "                'description': 'Multi-channel semantic encoding with spatially-consistent Hilbert curve mapping',\n",
    "                'channels': {\n",
    "                    'channel_1': 'Raw payload bytes normalized to [0,1]',\n",
    "                    'channel_2': 'Header-emphasized bytes with exponential decay',\n",
    "                    'channel_3': 'Byte frequency indicating commonality',\n",
    "                    'channel_4': 'Local Shannon entropy in 16-byte windows',\n",
    "                    'channel_5': 'Gradient magnitude showing rate of change'\n",
    "                },\n",
    "                'spatial_mapping': 'Hilbert curve ensures adjacent bytes remain spatially close',\n",
    "                'consistency': 'All channels use identical Hilbert position mapping for interpretability',\n",
    "                'interpretability': 'Position (x,y) in any channel corresponds to the same byte index'\n",
    "            },\n",
    "            'statistics': {\n",
    "                'shard_size': self.shard_size,\n",
    "                'total_shards': dict(self.shard_counts),\n",
    "                'labels': list(self.manifest['parquet'].keys())\n",
    "            },\n",
    "            'files': dict(self.manifest['parquet'])\n",
    "        }\n",
    "        \n",
    "        manifest_blob = self.bucket.blob(f\"{self.base_path}manifest.json\")\n",
    "        manifest_blob.upload_from_string(\n",
    "            json.dumps(manifest_data, indent=2),\n",
    "            content_type='application/json'\n",
    "        )\n",
    "        \n",
    "        return manifest_data\n",
    "\n",
    "print(\"âœ“ Multi-Channel Hilbert writer ready\")\n",
    "print(\"âœ“ Spatial consistency maintained across all semantic channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PCAP Files with Multi-Channel Hilbert Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GCS\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(CONFIG['bucket_name'])\n",
    "\n",
    "# List all PCAP files\n",
    "print(\"ðŸ” Discovering PCAP files...\")\n",
    "pcap_files = []\n",
    "labels = set()\n",
    "\n",
    "# List all blobs in the PCAP directory\n",
    "all_blobs = list(bucket.list_blobs(prefix=CONFIG['input_prefix']))\n",
    "\n",
    "# Extract PCAP files and labels\n",
    "for blob in all_blobs:\n",
    "    if blob.name.endswith('.pcap'):\n",
    "        path_parts = blob.name.split('/')\n",
    "        if len(path_parts) >= 2:\n",
    "            label = path_parts[-2]\n",
    "            labels.add(label)\n",
    "            pcap_files.append({\n",
    "                'path': blob.name,\n",
    "                'label': label,\n",
    "                'size_mb': blob.size / (1024 * 1024)\n",
    "            })\n",
    "\n",
    "print(f\"\\nðŸ“Š Found {len(pcap_files)} PCAP files across {len(labels)} labels\")\n",
    "print(f\"Labels: {sorted(labels)}\")\n",
    "\n",
    "# Group files by label\n",
    "files_by_label = defaultdict(list)\n",
    "for file_info in pcap_files:\n",
    "    files_by_label[file_info['label']].append(file_info)\n",
    "\n",
    "print(\"\\nðŸ“ Files per label:\")\n",
    "for label in sorted(files_by_label.keys()):\n",
    "    total_size = sum(f['size_mb'] for f in files_by_label[label])\n",
    "    print(f\"   {label}: {len(files_by_label[label])} files, {total_size:.1f} MB total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize writer\n",
    "output_base = f\"{CONFIG['output_prefix']}{CONFIG['timestamp']}/\"\n",
    "writer = MultiChannelHilbertWriter(bucket, output_base, CONFIG['shard_size'])\n",
    "\n",
    "print(\"\\nðŸš€ Processing PCAP files with Multi-Channel Hilbert encoding...\")\n",
    "print(f\"Target: {CONFIG['samples_per_class']} samples per label\")\n",
    "print(f\"Encoding: 5-channel semantically-rich Hilbert curve mapping\\n\")\n",
    "\n",
    "# Track progress\n",
    "sample_count = 0\n",
    "label_counts = Counter()\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each label\n",
    "for label in sorted(files_by_label.keys()):\n",
    "    print(f\"\\nðŸ“¦ Processing label: {label}\")\n",
    "    label_sample_count = 0\n",
    "    \n",
    "    # Process PCAP files for this label\n",
    "    for file_info in files_by_label[label]:\n",
    "        if label_counts[label] >= CONFIG['samples_per_class']:\n",
    "            break\n",
    "        \n",
    "        print(f\"   Reading {file_info['path'].split('/')[-1]} ({file_info['size_mb']:.1f} MB)...\")\n",
    "        \n",
    "        try:\n",
    "            # Download PCAP file\n",
    "            blob = bucket.blob(file_info['path'])\n",
    "            pcap_data = blob.download_as_bytes()\n",
    "            \n",
    "            # Extract packets\n",
    "            packets = read_pcap_packets(pcap_data, max_packets=CONFIG['packets_per_pcap'])\n",
    "            print(f\"   Extracted {len(packets)} packets\")\n",
    "            \n",
    "            # Process packets\n",
    "            for packet in packets:\n",
    "                if label_counts[label] >= CONFIG['samples_per_class']:\n",
    "                    break\n",
    "                \n",
    "                # Ensure payload is bytes\n",
    "                payload = packet['payload']\n",
    "                if isinstance(payload, (bytes, bytearray)):\n",
    "                    payload_array = np.frombuffer(payload, dtype=np.uint8)\n",
    "                else:\n",
    "                    payload_array = np.array(payload, dtype=np.uint8)\n",
    "                \n",
    "                # Pad to 1500 bytes if needed\n",
    "                if len(payload_array) < CONFIG['payload_bytes']:\n",
    "                    payload_array = np.pad(payload_array,\n",
    "                                         (0, CONFIG['payload_bytes'] - len(payload_array)),\n",
    "                                         'constant')\n",
    "                else:\n",
    "                    payload_array = payload_array[:CONFIG['payload_bytes']]\n",
    "                \n",
    "                # Generate sample ID\n",
    "                sample_id = f\"{label}_{label_counts[label]:06d}\"\n",
    "                \n",
    "                # Determine split\n",
    "                rand_val = np.random.random()\n",
    "                if rand_val < CONFIG['test_size']:\n",
    "                    split = 'test'\n",
    "                elif rand_val < CONFIG['test_size'] + CONFIG['val_size']:\n",
    "                    split = 'val'\n",
    "                else:\n",
    "                    split = 'train'\n",
    "                \n",
    "                # Add sample with multi-channel Hilbert encoding\n",
    "                writer.add_sample({\n",
    "                    'sample_id': sample_id,\n",
    "                    'label': label,\n",
    "                    'payload_bytes': payload_array\n",
    "                }, split)\n",
    "                \n",
    "                label_counts[label] += 1\n",
    "                sample_count += 1\n",
    "                \n",
    "                if sample_count % 1000 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = sample_count / elapsed\n",
    "                    print(f\"   Progress: {sample_count:,} total samples ({rate:.0f} samples/sec)\")\n",
    "            \n",
    "            # Clear memory\n",
    "            del pcap_data\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Error processing {file_info['path']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"   âœ“ Collected {label_counts[label]} samples for {label}\")\n",
    "\n",
    "# Finalize\n",
    "print(\"\\nðŸ’¾ Finalizing Multi-Channel Hilbert dataset...\")\n",
    "manifest = writer.finalize()\n",
    "\n",
    "print(f\"\\nâœ… Multi-Channel Hilbert dataset creation complete!\")\n",
    "print(f\"ðŸ“ Location: gs://{CONFIG['bucket_name']}/{output_base}\")\n",
    "print(f\"ðŸ“Š Total samples: {sample_count:,}\")\n",
    "print(f\"ðŸ”§ Encoding: 5-channel spatially-consistent Hilbert curve\")\n",
    "print(f\"â±ï¸ Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "print(f\"\\nðŸ“ˆ Samples per label:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"   {label}: {count:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Created\n",
    "\n",
    "This notebook generates a novel **Multi-Channel Hilbert** encoding that combines:\n",
    "\n",
    "1. **Spatial Locality**: Hilbert curve preserves byte adjacency in 2D space\n",
    "2. **Semantic Richness**: 5 channels capture different aspects of payload data\n",
    "3. **Spatial Consistency**: All channels use identical Hilbert mapping for interpretability\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Channel Alignment**: Position (x,y) refers to the same byte index across all channels\n",
    "- **Interpretability**: Can trace any spatial pattern back to specific byte sequences\n",
    "- **ViT Compatibility**: Enables self-attention to learn spatial-semantic relationships\n",
    "\n",
    "### Dataset Structure\n",
    "```\n",
    "gs://ai-cyber/datasets/pcap-multichannel-hilbert/[timestamp]/\n",
    "â”œâ”€â”€ parquet/\n",
    "â”‚   â””â”€â”€ multichannel_hilbert_32x32/\n",
    "â”‚       â”œâ”€â”€ Benign_Final/\n",
    "â”‚       â”‚   â”œâ”€â”€ train/\n",
    "â”‚       â”‚   â”œâ”€â”€ val/\n",
    "â”‚       â”‚   â””â”€â”€ test/\n",
    "â”‚       â””â”€â”€ [other labels...]\n",
    "â”œâ”€â”€ png/\n",
    "â”‚   â””â”€â”€ multichannel_hilbert_32x32/\n",
    "â”‚       â””â”€â”€ [sample visualizations]\n",
    "â””â”€â”€ manifest.json\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Train ViT Model**: Use the multi-channel Hilbert data with adapted architecture\n",
    "2. **Attention Analysis**: Visualize which byte regions the model attends to\n",
    "3. **Performance Comparison**: Compare against single-channel and non-Hilbert encodings\n",
    "4. **Ablation Studies**: Test importance of each semantic channel\n",
    "\n",
    "### Expected Benefits\n",
    "\n",
    "- **Better Long-Range Dependencies**: Self-attention can relate distant bytes that are semantically similar\n",
    "- **Richer Features**: Multiple semantic views provide more information for classification\n",
    "- **Interpretable Attention**: Can map attention patterns back to specific payload regions\n",
    "\n",
    "The combination of Hilbert's spatial locality and multi-channel semantics should enable the ViT to learn more sophisticated patterns in network traffic data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
